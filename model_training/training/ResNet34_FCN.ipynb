{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3eee070",
   "metadata": {},
   "source": [
    "# Training ResNet34 FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37db1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, sys\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: sys.path.insert(0, dir1)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from ummon import *\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from models.resnet_34_fcn import ResNetFCN\n",
    "from utils.water_segmentation import Water\n",
    "from utils import metrics, dataset_statistics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56673d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.segmentation_analyzer import SegmentationAnalyzer\n",
    "from utils.segmentation_trainer import SegmentationTrainer, SegmentationLogger\n",
    "import json\n",
    "\n",
    "def train(data_loader_trn, data_load_val, opt, model, epochs, path, additional_logger):\n",
    "    \n",
    "   \n",
    "    \n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # training state\n",
    "    trs = Trainingstate()\n",
    "\n",
    "    #Additional logger\n",
    "    seg_logger = additional_logger\n",
    "\n",
    "    # optimizer\n",
    "    #opt = optim.Adam(model.parameters(),lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=False)\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with Logger(loglevel=10, logdir='.', log_batch_interval=20) as lg:\n",
    "    \n",
    "        # trainer\n",
    "        trn = SegmentationTrainer(lg, model, loss, opt, trainingstate=trs, model_filename=path, \n",
    "                              combined_training_epochs=0, additional_logger=seg_logger)\n",
    "    \n",
    "        # train\n",
    "        trn.fit(data_loader_trn, epochs=epochs, validation_set=data_loader_val, analyzer=SegmentationAnalyzer)\n",
    "    \n",
    "    \n",
    "        ## evaluate on test set\n",
    "        trs.load_weights_best_validation_(model)\n",
    "        ev = SegmentationAnalyzer.evaluate(model, loss, data_loader_val, lg)\n",
    "        lg.info(\"Performance on validation set: \\nloss={:6.4f} \\niou={:.4f} \\nacc={:.4f} \\nsensitivity={:.4f} \\nspecificity={:.4f} \\nprecision={:.4f} \\nf1={:.4f}\".format(\n",
    "            ev[\"loss\"], ev[\"iou\"],ev[\"accuracy\"], ev[\"sensitivity\"], ev[\"specificity\"], ev[\"precision\"], ev[\"f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb805a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_training(data):\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "    plt.rcParams['axes.xmargin'] = 0\n",
    "    plt.plot(data[\"epoch_trn\"], data[\"loss_trn\"], label=\"training loss\")\n",
    "    plt.plot(data[\"epoch_trn\"], data[\"loss_val\"], label=\"validation loss\")\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('BCE loss')\n",
    "    plt.legend(loc='best')   \n",
    "    plt.show()\n",
    "    print(\"Minimum validation loss: {0:f} at epoch {1}\".format(min(data[\"loss_val\"]), data[\"loss_val\"].index(min(data[\"loss_val\"]))+1))\n",
    "    print(\"Minimum training loss: {0:f} at epoch {1}\".format(min(data[\"loss_trn\"]), data[\"loss_trn\"].index(min(data[\"loss_trn\"]))+1))\n",
    "    \n",
    "    print(\"Maximum validation IoU: {0:f} at epoch {1}\".format(max(data[\"iou_val\"]), data[\"iou_val\"].index(max(data[\"iou_val\"]))+1))\n",
    "    print(\"Maximum training IoU: {0:f} at epoch {1}\".format(max(data[\"iou_trn\"]), data[\"iou_trn\"].index(max(data[\"iou_trn\"]))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc4ddc",
   "metadata": {},
   "source": [
    "# Training on \"Tampere\" Dataset only\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2728cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_FCN_Tamp_RMS_lr_e-4'\n",
    "directory = \"../data/training_states/ResNet34_FCN\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0\n",
    "\n",
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "model = ResNetFCN(resnet34, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550a40f",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d4e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 images in training dataset\n",
      "200 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.TAMP_OPEN_DOCK\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=['open','dock'], data_list_misc=[],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=['channel'], data_list_misc=[],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fbebb",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c360ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "ResNetFCN(\n",
      "  (pretrained_net): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (intermediate): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCNHead(\n",
      "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Trainable params:   22387881\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training               400    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-2.0 max:2.2 mean:-0.5 std:1.1 / Labels:min:0.0 max:1.0 mean:0.2 std:0.4\n",
      "Validation             200    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-2.0 max:2.2 mean:-0.2 std:1.0 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00100 - Loss: 0.10283. [  3 s]\n",
      "Epoch: 1 - 00040/00100 - Loss: 0.08306. [  6 s]\n",
      "Epoch: 1 - 00060/00100 - Loss: 0.07648. [ 10 s]\n",
      "Epoch: 1 - 00080/00100 - Loss: 0.06598. [ 13 s]\n",
      "Epoch: 1 - 00100/00100 - Loss: 0.05661. [ 16 s]\n",
      "Epoch: 1 - loss(trn/val):0.05945/0.07008, acc(val):98.61%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 2 - 00020/00100 - Loss: 0.11249. [  3 s]\n",
      "Epoch: 2 - 00040/00100 - Loss: 0.05100. [  6 s]\n",
      "Epoch: 2 - 00060/00100 - Loss: 0.05666. [  9 s]\n",
      "Epoch: 2 - 00080/00100 - Loss: 0.04294. [ 12 s]\n",
      "Epoch: 2 - 00100/00100 - Loss: 0.05192. [ 15 s]\n",
      "Epoch: 2 - loss(trn/val):0.04533/0.05944, acc(val):98.56%, lr=0.00010 [BEST]. [15s] @25 samples/s \n",
      "Epoch: 3 - 00020/00100 - Loss: 0.04164. [  3 s]\n",
      "Epoch: 3 - 00040/00100 - Loss: 0.04249. [  6 s]\n",
      "Epoch: 3 - 00060/00100 - Loss: 0.04365. [  9 s]\n",
      "Epoch: 3 - 00080/00100 - Loss: 0.03225. [ 12 s]\n",
      "Epoch: 3 - 00100/00100 - Loss: 0.04885. [ 16 s]\n",
      "Epoch: 3 - loss(trn/val):0.03502/0.05462, acc(val):98.44%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 4 - 00020/00100 - Loss: 0.03182. [  3 s]\n",
      "Epoch: 4 - 00040/00100 - Loss: 0.03377. [  6 s]\n",
      "Epoch: 4 - 00060/00100 - Loss: 0.05161. [  9 s]\n",
      "Epoch: 4 - 00080/00100 - Loss: 0.02967. [ 12 s]\n",
      "Epoch: 4 - 00100/00100 - Loss: 0.02802. [ 16 s]\n",
      "Epoch: 4 - loss(trn/val):0.02942/0.04197, acc(val):98.79%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 5 - 00020/00100 - Loss: 0.02710. [  3 s]\n",
      "Epoch: 5 - 00040/00100 - Loss: 0.02599. [  6 s]\n",
      "Epoch: 5 - 00060/00100 - Loss: 0.02554. [  9 s]\n",
      "Epoch: 5 - 00080/00100 - Loss: 0.02933. [ 12 s]\n",
      "Epoch: 5 - 00100/00100 - Loss: 0.02387. [ 16 s]\n",
      "Epoch: 5 - loss(trn/val):0.02719/0.03930, acc(val):98.64%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 6 - 00020/00100 - Loss: 0.02950. [  3 s]\n",
      "Epoch: 6 - 00040/00100 - Loss: 0.02055. [  6 s]\n",
      "Epoch: 6 - 00060/00100 - Loss: 0.02184. [  9 s]\n",
      "Epoch: 6 - 00080/00100 - Loss: 0.02440. [ 13 s]\n",
      "Epoch: 6 - 00100/00100 - Loss: 0.02463. [ 16 s]\n",
      "Epoch: 6 - loss(trn/val):0.02347/0.03573, acc(val):98.89%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 7 - 00020/00100 - Loss: 0.02479. [  3 s]\n",
      "Epoch: 7 - 00040/00100 - Loss: 0.02798. [  6 s]\n",
      "Epoch: 7 - 00060/00100 - Loss: 0.01871. [  9 s]\n",
      "Epoch: 7 - 00080/00100 - Loss: 0.01843. [ 13 s]\n",
      "Epoch: 7 - 00100/00100 - Loss: 0.02219. [ 16 s]\n",
      "Epoch: 7 - loss(trn/val):0.02175/0.03702, acc(val):98.72%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 8 - 00020/00100 - Loss: 0.03674. [  3 s]\n",
      "Epoch: 8 - 00040/00100 - Loss: 0.01529. [  6 s]\n",
      "Epoch: 8 - 00060/00100 - Loss: 0.01583. [  9 s]\n",
      "Epoch: 8 - 00080/00100 - Loss: 0.01578. [ 13 s]\n",
      "Epoch: 8 - 00100/00100 - Loss: 0.03829. [ 16 s]\n",
      "Epoch: 8 - loss(trn/val):0.02057/0.04245, acc(val):98.53%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 9 - 00020/00100 - Loss: 0.01629. [  3 s]\n",
      "Epoch: 9 - 00040/00100 - Loss: 0.01401. [  6 s]\n",
      "Epoch: 9 - 00060/00100 - Loss: 0.02071. [  9 s]\n",
      "Epoch: 9 - 00080/00100 - Loss: 0.01398. [ 13 s]\n",
      "Epoch: 9 - 00100/00100 - Loss: 0.01540. [ 16 s]\n",
      "Epoch: 9 - loss(trn/val):0.01672/0.03598, acc(val):98.78%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 10 - 00020/00100 - Loss: 0.01881. [  3 s]\n",
      "Epoch: 10 - 00040/00100 - Loss: 0.01595. [  6 s]\n",
      "Epoch: 10 - 00060/00100 - Loss: 0.01937. [  9 s]\n",
      "Epoch: 10 - 00080/00100 - Loss: 0.01498. [ 13 s]\n",
      "Epoch: 10 - 00100/00100 - Loss: 0.01715. [ 16 s]\n",
      "Epoch: 10 - loss(trn/val):0.01612/0.02919, acc(val):98.97%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 11 - 00020/00100 - Loss: 0.01324. [  3 s]\n",
      "Epoch: 11 - 00040/00100 - Loss: 0.01611. [  6 s]\n",
      "Epoch: 11 - 00060/00100 - Loss: 0.01322. [  9 s]\n",
      "Epoch: 11 - 00080/00100 - Loss: 0.02452. [ 13 s]\n",
      "Epoch: 11 - 00100/00100 - Loss: 0.01918. [ 16 s]\n",
      "Epoch: 11 - loss(trn/val):0.01487/0.02699, acc(val):99.06%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 12 - 00020/00100 - Loss: 0.01400. [  3 s]\n",
      "Epoch: 12 - 00040/00100 - Loss: 0.01393. [  6 s]\n",
      "Epoch: 12 - 00060/00100 - Loss: 0.01776. [  9 s]\n",
      "Epoch: 12 - 00080/00100 - Loss: 0.01318. [ 13 s]\n",
      "Epoch: 12 - 00100/00100 - Loss: 0.01356. [ 16 s]\n",
      "Epoch: 12 - loss(trn/val):0.01426/0.02818, acc(val):98.97%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 13 - 00020/00100 - Loss: 0.01278. [  3 s]\n",
      "Epoch: 13 - 00040/00100 - Loss: 0.01337. [  6 s]\n",
      "Epoch: 13 - 00060/00100 - Loss: 0.01384. [  9 s]\n",
      "Epoch: 13 - 00080/00100 - Loss: 0.01397. [ 13 s]\n",
      "Epoch: 13 - 00100/00100 - Loss: 0.01475. [ 16 s]\n",
      "Epoch: 13 - loss(trn/val):0.01372/0.02777, acc(val):99.01%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 14 - 00020/00100 - Loss: 0.01126. [  3 s]\n",
      "Epoch: 14 - 00040/00100 - Loss: 0.01345. [  6 s]\n",
      "Epoch: 14 - 00060/00100 - Loss: 0.01149. [  9 s]\n",
      "Epoch: 14 - 00080/00100 - Loss: 0.01654. [ 13 s]\n",
      "Epoch: 14 - 00100/00100 - Loss: 0.01412. [ 16 s]\n",
      "Epoch: 14 - loss(trn/val):0.01295/0.02533, acc(val):99.08%, lr=0.00010 [BEST]. [16s] @24 samples/s \n",
      "Epoch: 15 - 00020/00100 - Loss: 0.01005. [  3 s]\n",
      "Epoch: 15 - 00040/00100 - Loss: 0.01482. [  6 s]\n",
      "Epoch: 15 - 00060/00100 - Loss: 0.01382. [  9 s]\n",
      "Epoch: 15 - 00080/00100 - Loss: 0.01067. [ 13 s]\n",
      "Epoch: 15 - 00100/00100 - Loss: 0.01087. [ 16 s]\n",
      "Epoch: 15 - loss(trn/val):0.01230/0.02747, acc(val):99.03%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 16 - 00020/00100 - Loss: 0.01114. [  3 s]\n",
      "Epoch: 16 - 00040/00100 - Loss: 0.01161. [  6 s]\n",
      "Epoch: 16 - 00060/00100 - Loss: 0.01297. [ 10 s]\n",
      "Epoch: 16 - 00080/00100 - Loss: 0.01245. [ 13 s]\n",
      "Epoch: 16 - 00100/00100 - Loss: 0.01372. [ 16 s]\n",
      "Epoch: 16 - loss(trn/val):0.01313/0.03616, acc(val):98.68%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 17 - 00020/00100 - Loss: 0.01010. [  5 s]\n",
      "Epoch: 17 - 00040/00100 - Loss: 0.01783. [  8 s]\n",
      "Epoch: 17 - 00060/00100 - Loss: 0.00927. [ 11 s]\n",
      "Epoch: 17 - 00080/00100 - Loss: 0.01080. [ 14 s]\n",
      "Epoch: 17 - 00100/00100 - Loss: 0.01208. [ 17 s]\n",
      "Epoch: 17 - loss(trn/val):0.01180/0.04218, acc(val):98.62%, lr=0.00010. [17s] @22 samples/s \n",
      "Epoch: 18 - 00020/00100 - Loss: 0.00936. [  3 s]\n",
      "Epoch: 18 - 00040/00100 - Loss: 0.00952. [  6 s]\n",
      "Epoch: 18 - 00060/00100 - Loss: 0.01190. [ 10 s]\n",
      "Epoch: 18 - 00080/00100 - Loss: 0.01242. [ 13 s]\n",
      "Epoch: 18 - 00100/00100 - Loss: 0.01181. [ 16 s]\n",
      "Epoch: 18 - loss(trn/val):0.01133/0.04227, acc(val):98.65%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 19 - 00020/00100 - Loss: 0.01028. [  3 s]\n",
      "Epoch: 19 - 00040/00100 - Loss: 0.01213. [  6 s]\n",
      "Epoch: 19 - 00060/00100 - Loss: 0.01067. [  9 s]\n",
      "Epoch: 19 - 00080/00100 - Loss: 0.01082. [ 13 s]\n",
      "Epoch: 19 - 00100/00100 - Loss: 0.00823. [ 16 s]\n",
      "Epoch: 19 - loss(trn/val):0.01121/0.04075, acc(val):98.74%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 20 - 00020/00100 - Loss: 0.01699. [  4 s]\n",
      "Epoch: 20 - 00040/00100 - Loss: 0.01655. [  7 s]\n",
      "Epoch: 20 - 00060/00100 - Loss: 0.00926. [ 10 s]\n",
      "Epoch: 20 - 00080/00100 - Loss: 0.01214. [ 13 s]\n",
      "Epoch: 20 - 00100/00100 - Loss: 0.00789. [ 16 s]\n",
      "Epoch: 20 - loss(trn/val):0.01076/0.04210, acc(val):98.69%, lr=0.00010. [16s] @23 samples/s \n",
      "Epoch: 21 - 00020/00100 - Loss: 0.00871. [  3 s]\n",
      "Epoch: 21 - 00040/00100 - Loss: 0.01165. [  6 s]\n",
      "Epoch: 21 - 00060/00100 - Loss: 0.00899. [ 10 s]\n",
      "Epoch: 21 - 00080/00100 - Loss: 0.00848. [ 13 s]\n",
      "Epoch: 21 - 00100/00100 - Loss: 0.01168. [ 16 s]\n",
      "Epoch: 21 - loss(trn/val):0.01136/0.03646, acc(val):98.84%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 22 - 00020/00100 - Loss: 0.00986. [  3 s]\n",
      "Epoch: 22 - 00040/00100 - Loss: 0.00914. [  6 s]\n",
      "Epoch: 22 - 00060/00100 - Loss: 0.01008. [ 10 s]\n",
      "Epoch: 22 - 00080/00100 - Loss: 0.00981. [ 13 s]\n",
      "Epoch: 22 - 00100/00100 - Loss: 0.01384. [ 16 s]\n",
      "Epoch: 22 - loss(trn/val):0.01134/0.04219, acc(val):98.63%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 23 - 00020/00100 - Loss: 0.01162. [  3 s]\n",
      "Epoch: 23 - 00040/00100 - Loss: 0.00853. [  6 s]\n",
      "Epoch: 23 - 00060/00100 - Loss: 0.01195. [  9 s]\n",
      "Epoch: 23 - 00080/00100 - Loss: 0.01282. [ 13 s]\n",
      "Epoch: 23 - 00100/00100 - Loss: 0.01720. [ 16 s]\n",
      "Epoch: 23 - loss(trn/val):0.01215/0.04465, acc(val):98.68%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 24 - 00020/00100 - Loss: 0.02949. [  3 s]\n",
      "Epoch: 24 - 00040/00100 - Loss: 0.01709. [  6 s]\n",
      "Epoch: 24 - 00060/00100 - Loss: 0.01545. [  9 s]\n",
      "Epoch: 24 - 00080/00100 - Loss: 0.01869. [ 13 s]\n",
      "Epoch: 24 - 00100/00100 - Loss: 0.01324. [ 16 s]\n",
      "Epoch: 24 - loss(trn/val):0.01034/0.04306, acc(val):98.63%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 25 - 00020/00100 - Loss: 0.01008. [  3 s]\n",
      "Epoch: 25 - 00040/00100 - Loss: 0.01347. [  6 s]\n",
      "Epoch: 25 - 00060/00100 - Loss: 0.01094. [ 10 s]\n",
      "Epoch: 25 - 00080/00100 - Loss: 0.01031. [ 13 s]\n",
      "Epoch: 25 - 00100/00100 - Loss: 0.01005. [ 16 s]\n",
      "Epoch: 25 - loss(trn/val):0.00985/0.04003, acc(val):98.76%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 26 - 00020/00100 - Loss: 0.01054. [  3 s]\n",
      "Epoch: 26 - 00040/00100 - Loss: 0.01084. [  6 s]\n",
      "Epoch: 26 - 00060/00100 - Loss: 0.01093. [  9 s]\n",
      "Epoch: 26 - 00080/00100 - Loss: 0.00851. [ 13 s]\n",
      "Epoch: 26 - 00100/00100 - Loss: 0.00916. [ 16 s]\n",
      "Epoch: 26 - loss(trn/val):0.00994/0.04229, acc(val):98.72%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 27 - 00020/00100 - Loss: 0.00963. [  3 s]\n",
      "Epoch: 27 - 00040/00100 - Loss: 0.00956. [  6 s]\n",
      "Epoch: 27 - 00060/00100 - Loss: 0.01408. [ 10 s]\n",
      "Epoch: 27 - 00080/00100 - Loss: 0.01010. [ 13 s]\n",
      "Epoch: 27 - 00100/00100 - Loss: 0.00809. [ 16 s]\n",
      "Epoch: 27 - loss(trn/val):0.00957/0.04237, acc(val):98.69%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 28 - 00020/00100 - Loss: 0.00882. [  3 s]\n",
      "Epoch: 28 - 00040/00100 - Loss: 0.00915. [  6 s]\n",
      "Epoch: 28 - 00060/00100 - Loss: 0.01078. [ 10 s]\n",
      "Epoch: 28 - 00080/00100 - Loss: 0.00840. [ 13 s]\n",
      "Epoch: 28 - 00100/00100 - Loss: 0.00876. [ 16 s]\n",
      "Epoch: 28 - loss(trn/val):0.01058/0.04751, acc(val):98.55%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 29 - 00020/00100 - Loss: 0.00923. [  3 s]\n",
      "Epoch: 29 - 00040/00100 - Loss: 0.01128. [  6 s]\n",
      "Epoch: 29 - 00060/00100 - Loss: 0.00889. [  9 s]\n",
      "Epoch: 29 - 00080/00100 - Loss: 0.00753. [ 12 s]\n",
      "Epoch: 29 - 00100/00100 - Loss: 0.01060. [ 15 s]\n",
      "Epoch: 29 - loss(trn/val):0.01019/0.04528, acc(val):98.63%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 30 - 00020/00100 - Loss: 0.00773. [  3 s]\n",
      "Epoch: 30 - 00040/00100 - Loss: 0.00893. [  6 s]\n",
      "Epoch: 30 - 00060/00100 - Loss: 0.00877. [  9 s]\n",
      "Epoch: 30 - 00080/00100 - Loss: 0.00961. [ 12 s]\n",
      "Epoch: 30 - 00100/00100 - Loss: 0.01085. [ 15 s]\n",
      "Epoch: 30 - loss(trn/val):0.00959/0.04613, acc(val):98.66%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 31 - 00020/00100 - Loss: 0.00904. [  3 s]\n",
      "Epoch: 31 - 00040/00100 - Loss: 0.00697. [  6 s]\n",
      "Epoch: 31 - 00060/00100 - Loss: 0.00758. [  9 s]\n",
      "Epoch: 31 - 00080/00100 - Loss: 0.01016. [ 12 s]\n",
      "Epoch: 31 - 00100/00100 - Loss: 0.01213. [ 16 s]\n",
      "Epoch: 31 - loss(trn/val):0.00979/0.05175, acc(val):98.59%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 32 - 00020/00100 - Loss: 0.00821. [  3 s]\n",
      "Epoch: 32 - 00040/00100 - Loss: 0.01267. [  6 s]\n",
      "Epoch: 32 - 00060/00100 - Loss: 0.01195. [  9 s]\n",
      "Epoch: 32 - 00080/00100 - Loss: 0.01000. [ 12 s]\n",
      "Epoch: 32 - 00100/00100 - Loss: 0.01028. [ 15 s]\n",
      "Epoch: 32 - loss(trn/val):0.00949/0.05710, acc(val):98.49%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 33 - 00020/00100 - Loss: 0.00925. [  3 s]\n",
      "Epoch: 33 - 00040/00100 - Loss: 0.00974. [  6 s]\n",
      "Epoch: 33 - 00060/00100 - Loss: 0.00918. [  9 s]\n",
      "Epoch: 33 - 00080/00100 - Loss: 0.00775. [ 12 s]\n",
      "Epoch: 33 - 00100/00100 - Loss: 0.01022. [ 15 s]\n",
      "Epoch: 33 - loss(trn/val):0.00978/0.06082, acc(val):98.35%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 34 - 00020/00100 - Loss: 0.00754. [  3 s]\n",
      "Epoch: 34 - 00040/00100 - Loss: 0.00928. [  6 s]\n",
      "Epoch: 34 - 00060/00100 - Loss: 0.01061. [  9 s]\n",
      "Epoch: 34 - 00080/00100 - Loss: 0.01115. [ 12 s]\n",
      "Epoch: 34 - 00100/00100 - Loss: 0.00703. [ 15 s]\n",
      "Epoch: 34 - loss(trn/val):0.00889/0.06824, acc(val):98.25%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 35 - 00020/00100 - Loss: 0.00907. [  3 s]\n",
      "Epoch: 35 - 00040/00100 - Loss: 0.00682. [  6 s]\n",
      "Epoch: 35 - 00060/00100 - Loss: 0.00750. [  9 s]\n",
      "Epoch: 35 - 00080/00100 - Loss: 0.00690. [ 12 s]\n",
      "Epoch: 35 - 00100/00100 - Loss: 0.00858. [ 15 s]\n",
      "Epoch: 35 - loss(trn/val):0.00868/0.06019, acc(val):98.42%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 36 - 00020/00100 - Loss: 0.00745. [  3 s]\n",
      "Epoch: 36 - 00040/00100 - Loss: 0.00943. [  6 s]\n",
      "Epoch: 36 - 00060/00100 - Loss: 0.01433. [ 10 s]\n",
      "Epoch: 36 - 00080/00100 - Loss: 0.00785. [ 13 s]\n",
      "Epoch: 36 - 00100/00100 - Loss: 0.00920. [ 16 s]\n",
      "Epoch: 36 - loss(trn/val):0.01017/0.06019, acc(val):98.47%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 37 - 00020/00100 - Loss: 0.00996. [  3 s]\n",
      "Epoch: 37 - 00040/00100 - Loss: 0.00983. [  6 s]\n",
      "Epoch: 37 - 00060/00100 - Loss: 0.00689. [  9 s]\n",
      "Epoch: 37 - 00080/00100 - Loss: 0.00998. [ 12 s]\n",
      "Epoch: 37 - 00100/00100 - Loss: 0.00797. [ 15 s]\n",
      "Epoch: 37 - loss(trn/val):0.00879/0.06159, acc(val):98.37%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 38 - 00020/00100 - Loss: 0.01008. [  3 s]\n",
      "Epoch: 38 - 00040/00100 - Loss: 0.01753. [  6 s]\n",
      "Epoch: 38 - 00060/00100 - Loss: 0.00749. [  9 s]\n",
      "Epoch: 38 - 00080/00100 - Loss: 0.00783. [ 12 s]\n",
      "Epoch: 38 - 00100/00100 - Loss: 0.00861. [ 15 s]\n",
      "Epoch: 38 - loss(trn/val):0.00850/0.07494, acc(val):98.19%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 39 - 00020/00100 - Loss: 0.01333. [  3 s]\n",
      "Epoch: 39 - 00040/00100 - Loss: 0.00831. [  6 s]\n",
      "Epoch: 39 - 00060/00100 - Loss: 0.01023. [  9 s]\n",
      "Epoch: 39 - 00080/00100 - Loss: 0.00868. [ 12 s]\n",
      "Epoch: 39 - 00100/00100 - Loss: 0.00923. [ 15 s]\n",
      "Epoch: 39 - loss(trn/val):0.00848/0.05053, acc(val):98.62%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 40 - 00020/00100 - Loss: 0.00700. [  3 s]\n",
      "Epoch: 40 - 00040/00100 - Loss: 0.00673. [  6 s]\n",
      "Epoch: 40 - 00060/00100 - Loss: 0.00869. [  9 s]\n",
      "Epoch: 40 - 00080/00100 - Loss: 0.00761. [ 12 s]\n",
      "Epoch: 40 - 00100/00100 - Loss: 0.00700. [ 15 s]\n",
      "Epoch: 40 - loss(trn/val):0.00857/0.06090, acc(val):98.47%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 41 - 00020/00100 - Loss: 0.00863. [  3 s]\n",
      "Epoch: 41 - 00040/00100 - Loss: 0.01163. [  6 s]\n",
      "Epoch: 41 - 00060/00100 - Loss: 0.00941. [  9 s]\n",
      "Epoch: 41 - 00080/00100 - Loss: 0.00750. [ 12 s]\n",
      "Epoch: 41 - 00100/00100 - Loss: 0.00818. [ 15 s]\n",
      "Epoch: 41 - loss(trn/val):0.00894/0.04446, acc(val):98.76%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 42 - 00020/00100 - Loss: 0.00659. [  3 s]\n",
      "Epoch: 42 - 00040/00100 - Loss: 0.01426. [  6 s]\n",
      "Epoch: 42 - 00060/00100 - Loss: 0.00877. [  9 s]\n",
      "Epoch: 42 - 00080/00100 - Loss: 0.00846. [ 12 s]\n",
      "Epoch: 42 - 00100/00100 - Loss: 0.00639. [ 15 s]\n",
      "Epoch: 42 - loss(trn/val):0.00836/0.10786, acc(val):97.74%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 43 - 00020/00100 - Loss: 0.00673. [  3 s]\n",
      "Epoch: 43 - 00040/00100 - Loss: 0.00832. [  6 s]\n",
      "Epoch: 43 - 00060/00100 - Loss: 0.00903. [ 10 s]\n",
      "Epoch: 43 - 00080/00100 - Loss: 0.02591. [ 13 s]\n",
      "Epoch: 43 - 00100/00100 - Loss: 0.01173. [ 16 s]\n",
      "Epoch: 43 - loss(trn/val):0.01062/0.04997, acc(val):98.42%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 44 - 00020/00100 - Loss: 0.00746. [  3 s]\n",
      "Epoch: 44 - 00040/00100 - Loss: 0.00764. [  7 s]\n",
      "Epoch: 44 - 00060/00100 - Loss: 0.00809. [ 10 s]\n",
      "Epoch: 44 - 00080/00100 - Loss: 0.00622. [ 13 s]\n",
      "Epoch: 44 - 00100/00100 - Loss: 0.00989. [ 16 s]\n",
      "Epoch: 44 - loss(trn/val):0.00799/0.04827, acc(val):98.68%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 45 - 00020/00100 - Loss: 0.01028. [  3 s]\n",
      "Epoch: 45 - 00040/00100 - Loss: 0.00689. [  6 s]\n",
      "Epoch: 45 - 00060/00100 - Loss: 0.00780. [ 10 s]\n",
      "Epoch: 45 - 00080/00100 - Loss: 0.01562. [ 13 s]\n",
      "Epoch: 45 - 00100/00100 - Loss: 0.00658. [ 16 s]\n",
      "Epoch: 45 - loss(trn/val):0.00781/0.05142, acc(val):98.64%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 46 - 00020/00100 - Loss: 0.00647. [  3 s]\n",
      "Epoch: 46 - 00040/00100 - Loss: 0.00937. [  6 s]\n",
      "Epoch: 46 - 00060/00100 - Loss: 0.00664. [ 10 s]\n",
      "Epoch: 46 - 00080/00100 - Loss: 0.00913. [ 13 s]\n",
      "Epoch: 46 - 00100/00100 - Loss: 0.00632. [ 16 s]\n",
      "Epoch: 46 - loss(trn/val):0.00781/0.05287, acc(val):98.60%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 47 - 00020/00100 - Loss: 0.00688. [  3 s]\n",
      "Epoch: 47 - 00040/00100 - Loss: 0.00983. [  7 s]\n",
      "Epoch: 47 - 00060/00100 - Loss: 0.00591. [ 10 s]\n",
      "Epoch: 47 - 00080/00100 - Loss: 0.00618. [ 13 s]\n",
      "Epoch: 47 - 00100/00100 - Loss: 0.00715. [ 16 s]\n",
      "Epoch: 47 - loss(trn/val):0.00768/0.04959, acc(val):98.69%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 48 - 00020/00100 - Loss: 0.00658. [  3 s]\n",
      "Epoch: 48 - 00040/00100 - Loss: 0.00664. [  6 s]\n",
      "Epoch: 48 - 00060/00100 - Loss: 0.01411. [ 10 s]\n",
      "Epoch: 48 - 00080/00100 - Loss: 0.01317. [ 13 s]\n",
      "Epoch: 48 - 00100/00100 - Loss: 0.00610. [ 16 s]\n",
      "Epoch: 48 - loss(trn/val):0.00770/0.06221, acc(val):98.46%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 49 - 00020/00100 - Loss: 0.00820. [  3 s]\n",
      "Epoch: 49 - 00040/00100 - Loss: 0.00835. [  6 s]\n",
      "Epoch: 49 - 00060/00100 - Loss: 0.00868. [ 10 s]\n",
      "Epoch: 49 - 00080/00100 - Loss: 0.01308. [ 13 s]\n",
      "Epoch: 49 - 00100/00100 - Loss: 0.00806. [ 16 s]\n",
      "Epoch: 49 - loss(trn/val):0.00797/0.05315, acc(val):98.61%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 50 - 00020/00100 - Loss: 0.01184. [  3 s]\n",
      "Epoch: 50 - 00040/00100 - Loss: 0.01085. [  7 s]\n",
      "Epoch: 50 - 00060/00100 - Loss: 0.00864. [ 10 s]\n",
      "Epoch: 50 - 00080/00100 - Loss: 0.00661. [ 13 s]\n",
      "Epoch: 50 - 00100/00100 - Loss: 0.00848. [ 16 s]\n",
      "Epoch: 50 - loss(trn/val):0.00792/0.06101, acc(val):98.48%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 51 - 00020/00100 - Loss: 0.00607. [  3 s]\n",
      "Epoch: 51 - 00040/00100 - Loss: 0.00914. [  7 s]\n",
      "Epoch: 51 - 00060/00100 - Loss: 0.01213. [ 10 s]\n",
      "Epoch: 51 - 00080/00100 - Loss: 0.01262. [ 13 s]\n",
      "Epoch: 51 - 00100/00100 - Loss: 0.00814. [ 16 s]\n",
      "Epoch: 51 - loss(trn/val):0.00818/0.05327, acc(val):98.61%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 52 - 00020/00100 - Loss: 0.01095. [  3 s]\n",
      "Epoch: 52 - 00040/00100 - Loss: 0.00795. [  6 s]\n",
      "Epoch: 52 - 00060/00100 - Loss: 0.00711. [ 10 s]\n",
      "Epoch: 52 - 00080/00100 - Loss: 0.00711. [ 13 s]\n",
      "Epoch: 52 - 00100/00100 - Loss: 0.01180. [ 16 s]\n",
      "Epoch: 52 - loss(trn/val):0.00821/0.10515, acc(val):97.85%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 53 - 00020/00100 - Loss: 0.00572. [  3 s]\n",
      "Epoch: 53 - 00040/00100 - Loss: 0.00602. [  6 s]\n",
      "Epoch: 53 - 00060/00100 - Loss: 0.00787. [ 10 s]\n",
      "Epoch: 53 - 00080/00100 - Loss: 0.00657. [ 13 s]\n",
      "Epoch: 53 - 00100/00100 - Loss: 0.01173. [ 16 s]\n",
      "Epoch: 53 - loss(trn/val):0.01320/0.11274, acc(val):97.65%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 54 - 00020/00100 - Loss: 0.00942. [  3 s]\n",
      "Epoch: 54 - 00040/00100 - Loss: 0.00559. [  6 s]\n",
      "Epoch: 54 - 00060/00100 - Loss: 0.00858. [ 10 s]\n",
      "Epoch: 54 - 00080/00100 - Loss: 0.00800. [ 13 s]\n",
      "Epoch: 54 - 00100/00100 - Loss: 0.00607. [ 16 s]\n",
      "Epoch: 54 - loss(trn/val):0.00803/0.08112, acc(val):98.27%, lr=0.00010. [16s] @24 samples/s \n",
      "Epoch: 55 - 00020/00100 - Loss: 0.00747. [  3 s]\n",
      "Epoch: 55 - 00040/00100 - Loss: 0.01146. [  6 s]\n",
      "Epoch: 55 - 00060/00100 - Loss: 0.00553. [  9 s]\n",
      "Epoch: 55 - 00080/00100 - Loss: 0.00613. [ 12 s]\n",
      "Epoch: 55 - 00100/00100 - Loss: 0.00806. [ 15 s]\n",
      "Epoch: 55 - loss(trn/val):0.00768/0.07287, acc(val):98.34%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 56 - 00020/00100 - Loss: 0.00526. [  3 s]\n",
      "Epoch: 56 - 00040/00100 - Loss: 0.00774. [  6 s]\n",
      "Epoch: 56 - 00060/00100 - Loss: 0.00721. [  9 s]\n",
      "Epoch: 56 - 00080/00100 - Loss: 0.00788. [ 12 s]\n",
      "Epoch: 56 - 00100/00100 - Loss: 0.00964. [ 15 s]\n",
      "Epoch: 56 - loss(trn/val):0.00743/0.07190, acc(val):98.36%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 57 - 00020/00100 - Loss: 0.00903. [  4 s]\n",
      "Epoch: 57 - 00040/00100 - Loss: 0.00725. [  7 s]\n",
      "Epoch: 57 - 00060/00100 - Loss: 0.01059. [ 10 s]\n",
      "Epoch: 57 - 00080/00100 - Loss: 0.00663. [ 13 s]\n",
      "Epoch: 57 - 00100/00100 - Loss: 0.01161. [ 16 s]\n",
      "Epoch: 57 - loss(trn/val):0.00738/0.06982, acc(val):98.43%, lr=0.00010. [16s] @23 samples/s \n",
      "Epoch: 58 - 00020/00100 - Loss: 0.00608. [  3 s]\n",
      "Epoch: 58 - 00040/00100 - Loss: 0.01218. [  6 s]\n",
      "Epoch: 58 - 00060/00100 - Loss: 0.00836. [  9 s]\n",
      "Epoch: 58 - 00080/00100 - Loss: 0.00603. [ 12 s]\n",
      "Epoch: 58 - 00100/00100 - Loss: 0.00760. [ 15 s]\n",
      "Epoch: 58 - loss(trn/val):0.00716/0.06812, acc(val):98.41%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 59 - 00020/00100 - Loss: 0.00719. [  3 s]\n",
      "Epoch: 59 - 00040/00100 - Loss: 0.00537. [  6 s]\n",
      "Epoch: 59 - 00060/00100 - Loss: 0.00636. [  9 s]\n",
      "Epoch: 59 - 00080/00100 - Loss: 0.00706. [ 12 s]\n",
      "Epoch: 59 - 00100/00100 - Loss: 0.00760. [ 15 s]\n",
      "Epoch: 59 - loss(trn/val):0.00766/0.06847, acc(val):98.36%, lr=0.00010. [15s] @25 samples/s \n",
      "Epoch: 60 - 00020/00100 - Loss: 0.00836. [  3 s]\n",
      "Epoch: 60 - 00040/00100 - Loss: 0.00798. [  6 s]\n",
      "Epoch: 60 - 00060/00100 - Loss: 0.00585. [  9 s]\n",
      "Epoch: 60 - 00080/00100 - Loss: 0.00681. [ 12 s]\n",
      "Epoch: 60 - 00100/00100 - Loss: 0.00840. [ 15 s]\n",
      "Epoch: 60 - loss(trn/val):0.00808/0.05614, acc(val):98.57%, lr=0.00010. [15s] @25 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.0253 \n",
      "iou=0.9797 \n",
      "acc=0.9908 \n",
      "sensitivity=0.9932 \n",
      "specificity=0.9889 \n",
      "precision=0.9863 \n",
      "f1=0.9896\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, additional_logger = logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea63364",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb752d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.025330 at epoch 14\n",
      "Minimum training loss: 0.007164 at epoch 58\n",
      "Maximum validation IoU: 0.979677 at epoch 14\n",
      "Maximum training IoU: 0.993778 at epoch 58\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_FCN_Tamp_RMS_lr_e-4\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795110d",
   "metadata": {},
   "source": [
    "# Training on \"Misc\" Dataset only\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab4ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_FCN_Misc_RMS_lr_e-4'\n",
    "directory = \"../data/training_states/ResNet34_FCN\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648a6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "model = ResNetFCN(resnet34, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc913c",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cd80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188 images in training dataset\n",
      "212 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.MISC_TRAINING\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=[], data_list_misc=['training'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=[], data_list_misc=['validation'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9f192",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b969c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "ResNetFCN(\n",
      "  (pretrained_net): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (intermediate): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCNHead(\n",
      "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Trainable params:   22387881\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training              2188    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.2 mean:0.0 std:1.0 / Labels:min:0.0 max:1.0 mean:0.3 std:0.5\n",
      "Validation             212    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.2 mean:-0.1 std:0.9 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00547 - Loss: 0.49344. [  3 s]\n",
      "Epoch: 1 - 00040/00547 - Loss: 0.42902. [  6 s]\n",
      "Epoch: 1 - 00060/00547 - Loss: 0.32521. [  9 s]\n",
      "Epoch: 1 - 00080/00547 - Loss: 0.53905. [ 12 s]\n",
      "Epoch: 1 - 00100/00547 - Loss: 0.25179. [ 15 s]\n",
      "Epoch: 1 - 00120/00547 - Loss: 0.37051. [ 18 s]\n",
      "Epoch: 1 - 00140/00547 - Loss: 0.25893. [ 21 s]\n",
      "Epoch: 1 - 00160/00547 - Loss: 0.28441. [ 24 s]\n",
      "Epoch: 1 - 00180/00547 - Loss: 0.38737. [ 28 s]\n",
      "Epoch: 1 - 00200/00547 - Loss: 0.26536. [ 31 s]\n",
      "Epoch: 1 - 00220/00547 - Loss: 0.38704. [ 34 s]\n",
      "Epoch: 1 - 00240/00547 - Loss: 0.25394. [ 37 s]\n",
      "Epoch: 1 - 00260/00547 - Loss: 0.31681. [ 40 s]\n",
      "Epoch: 1 - 00280/00547 - Loss: 0.39631. [ 43 s]\n",
      "Epoch: 1 - 00300/00547 - Loss: 0.33484. [ 46 s]\n",
      "Epoch: 1 - 00320/00547 - Loss: 0.44518. [ 49 s]\n",
      "Epoch: 1 - 00340/00547 - Loss: 0.26954. [ 52 s]\n",
      "Epoch: 1 - 00360/00547 - Loss: 0.36649. [ 55 s]\n",
      "Epoch: 1 - 00380/00547 - Loss: 0.29492. [ 58 s]\n",
      "Epoch: 1 - 00400/00547 - Loss: 0.32622. [ 62 s]\n",
      "Epoch: 1 - 00420/00547 - Loss: 0.25778. [ 65 s]\n",
      "Epoch: 1 - 00440/00547 - Loss: 0.17755. [ 68 s]\n",
      "Epoch: 1 - 00460/00547 - Loss: 0.61593. [ 71 s]\n",
      "Epoch: 1 - 00480/00547 - Loss: 0.32505. [ 74 s]\n",
      "Epoch: 1 - 00500/00547 - Loss: 0.17313. [ 77 s]\n",
      "Epoch: 1 - 00520/00547 - Loss: 0.36216. [ 80 s]\n",
      "Epoch: 1 - 00540/00547 - Loss: 0.28526. [ 83 s]\n",
      "Epoch: 1 - loss(trn/val):0.21561/0.29244, acc(val):88.16%, lr=0.00010 [BEST]. [84s] @25 samples/s \n",
      "Epoch: 2 - 00020/00547 - Loss: 0.14038. [  3 s]\n",
      "Epoch: 2 - 00040/00547 - Loss: 0.46649. [  6 s]\n",
      "Epoch: 2 - 00060/00547 - Loss: 0.26878. [  9 s]\n",
      "Epoch: 2 - 00080/00547 - Loss: 0.21582. [ 12 s]\n",
      "Epoch: 2 - 00100/00547 - Loss: 0.15838. [ 15 s]\n",
      "Epoch: 2 - 00120/00547 - Loss: 0.30726. [ 18 s]\n",
      "Epoch: 2 - 00140/00547 - Loss: 0.42005. [ 21 s]\n",
      "Epoch: 2 - 00160/00547 - Loss: 0.18979. [ 24 s]\n",
      "Epoch: 2 - 00180/00547 - Loss: 0.28241. [ 27 s]\n",
      "Epoch: 2 - 00200/00547 - Loss: 0.25844. [ 30 s]\n",
      "Epoch: 2 - 00220/00547 - Loss: 0.25119. [ 33 s]\n",
      "Epoch: 2 - 00240/00547 - Loss: 0.76104. [ 37 s]\n",
      "Epoch: 2 - 00260/00547 - Loss: 0.28027. [ 40 s]\n",
      "Epoch: 2 - 00280/00547 - Loss: 0.18928. [ 43 s]\n",
      "Epoch: 2 - 00300/00547 - Loss: 0.18740. [ 46 s]\n",
      "Epoch: 2 - 00320/00547 - Loss: 0.43880. [ 49 s]\n",
      "Epoch: 2 - 00340/00547 - Loss: 0.23020. [ 52 s]\n",
      "Epoch: 2 - 00360/00547 - Loss: 0.17858. [ 55 s]\n",
      "Epoch: 2 - 00380/00547 - Loss: 0.23613. [ 58 s]\n",
      "Epoch: 2 - 00400/00547 - Loss: 0.18842. [ 61 s]\n",
      "Epoch: 2 - 00420/00547 - Loss: 0.24723. [ 64 s]\n",
      "Epoch: 2 - 00440/00547 - Loss: 0.28567. [ 68 s]\n",
      "Epoch: 2 - 00460/00547 - Loss: 0.15834. [ 71 s]\n",
      "Epoch: 2 - 00480/00547 - Loss: 0.25058. [ 74 s]\n",
      "Epoch: 2 - 00500/00547 - Loss: 0.22725. [ 77 s]\n",
      "Epoch: 2 - 00520/00547 - Loss: 0.29238. [ 80 s]\n",
      "Epoch: 2 - 00540/00547 - Loss: 0.15895. [ 83 s]\n",
      "Epoch: 2 - loss(trn/val):0.24456/0.36329, acc(val):84.55%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 3 - 00020/00547 - Loss: 0.41507. [  3 s]\n",
      "Epoch: 3 - 00040/00547 - Loss: 0.20246. [  6 s]\n",
      "Epoch: 3 - 00060/00547 - Loss: 0.14561. [  9 s]\n",
      "Epoch: 3 - 00080/00547 - Loss: 0.26557. [ 12 s]\n",
      "Epoch: 3 - 00100/00547 - Loss: 0.21223. [ 15 s]\n",
      "Epoch: 3 - 00120/00547 - Loss: 0.28155. [ 18 s]\n",
      "Epoch: 3 - 00140/00547 - Loss: 0.29388. [ 21 s]\n",
      "Epoch: 3 - 00160/00547 - Loss: 0.15135. [ 24 s]\n",
      "Epoch: 3 - 00180/00547 - Loss: 0.15351. [ 27 s]\n",
      "Epoch: 3 - 00200/00547 - Loss: 0.15898. [ 30 s]\n",
      "Epoch: 3 - 00220/00547 - Loss: 0.33245. [ 34 s]\n",
      "Epoch: 3 - 00240/00547 - Loss: 0.47345. [ 37 s]\n",
      "Epoch: 3 - 00260/00547 - Loss: 0.21376. [ 40 s]\n",
      "Epoch: 3 - 00280/00547 - Loss: 0.11747. [ 43 s]\n",
      "Epoch: 3 - 00300/00547 - Loss: 0.18396. [ 46 s]\n",
      "Epoch: 3 - 00320/00547 - Loss: 0.20170. [ 49 s]\n",
      "Epoch: 3 - 00340/00547 - Loss: 0.20899. [ 52 s]\n",
      "Epoch: 3 - 00360/00547 - Loss: 0.27936. [ 55 s]\n",
      "Epoch: 3 - 00380/00547 - Loss: 0.19843. [ 58 s]\n",
      "Epoch: 3 - 00400/00547 - Loss: 0.19399. [ 61 s]\n",
      "Epoch: 3 - 00420/00547 - Loss: 0.22911. [ 64 s]\n",
      "Epoch: 3 - 00440/00547 - Loss: 0.16908. [ 68 s]\n",
      "Epoch: 3 - 00460/00547 - Loss: 0.20905. [ 71 s]\n",
      "Epoch: 3 - 00480/00547 - Loss: 0.14988. [ 74 s]\n",
      "Epoch: 3 - 00500/00547 - Loss: 0.22700. [ 77 s]\n",
      "Epoch: 3 - 00520/00547 - Loss: 0.06348. [ 80 s]\n",
      "Epoch: 3 - 00540/00547 - Loss: 0.18314. [ 83 s]\n",
      "Epoch: 3 - loss(trn/val):0.17461/0.27457, acc(val):88.51%, lr=0.00010 [BEST]. [84s] @25 samples/s \n",
      "Epoch: 4 - 00020/00547 - Loss: 0.17883. [  3 s]\n",
      "Epoch: 4 - 00040/00547 - Loss: 0.12601. [  6 s]\n",
      "Epoch: 4 - 00060/00547 - Loss: 0.22226. [  9 s]\n",
      "Epoch: 4 - 00080/00547 - Loss: 0.37078. [ 12 s]\n",
      "Epoch: 4 - 00100/00547 - Loss: 0.09995. [ 15 s]\n",
      "Epoch: 4 - 00120/00547 - Loss: 0.09888. [ 19 s]\n",
      "Epoch: 4 - 00140/00547 - Loss: 0.17487. [ 22 s]\n",
      "Epoch: 4 - 00160/00547 - Loss: 0.13155. [ 25 s]\n",
      "Epoch: 4 - 00180/00547 - Loss: 0.15079. [ 28 s]\n",
      "Epoch: 4 - 00200/00547 - Loss: 0.17765. [ 31 s]\n",
      "Epoch: 4 - 00220/00547 - Loss: 0.15823. [ 34 s]\n",
      "Epoch: 4 - 00240/00547 - Loss: 0.08528. [ 37 s]\n",
      "Epoch: 4 - 00260/00547 - Loss: 0.12278. [ 40 s]\n",
      "Epoch: 4 - 00280/00547 - Loss: 0.08850. [ 43 s]\n",
      "Epoch: 4 - 00300/00547 - Loss: 0.17762. [ 47 s]\n",
      "Epoch: 4 - 00320/00547 - Loss: 0.17642. [ 50 s]\n",
      "Epoch: 4 - 00340/00547 - Loss: 0.22578. [ 53 s]\n",
      "Epoch: 4 - 00360/00547 - Loss: 0.12140. [ 56 s]\n",
      "Epoch: 4 - 00380/00547 - Loss: 0.37687. [ 59 s]\n",
      "Epoch: 4 - 00400/00547 - Loss: 0.18834. [ 62 s]\n",
      "Epoch: 4 - 00420/00547 - Loss: 0.20733. [ 65 s]\n",
      "Epoch: 4 - 00440/00547 - Loss: 0.06831. [ 69 s]\n",
      "Epoch: 4 - 00460/00547 - Loss: 0.15009. [ 72 s]\n",
      "Epoch: 4 - 00480/00547 - Loss: 0.13697. [ 75 s]\n",
      "Epoch: 4 - 00500/00547 - Loss: 0.16035. [ 78 s]\n",
      "Epoch: 4 - 00520/00547 - Loss: 0.26777. [ 81 s]\n",
      "Epoch: 4 - 00540/00547 - Loss: 0.23701. [ 84 s]\n",
      "Epoch: 4 - loss(trn/val):0.14866/0.34276, acc(val):85.96%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 5 - 00020/00547 - Loss: 0.06788. [  3 s]\n",
      "Epoch: 5 - 00040/00547 - Loss: 0.28750. [  6 s]\n",
      "Epoch: 5 - 00060/00547 - Loss: 0.13517. [  9 s]\n",
      "Epoch: 5 - 00080/00547 - Loss: 0.11223. [ 12 s]\n",
      "Epoch: 5 - 00100/00547 - Loss: 0.17462. [ 15 s]\n",
      "Epoch: 5 - 00120/00547 - Loss: 0.07372. [ 19 s]\n",
      "Epoch: 5 - 00140/00547 - Loss: 0.09031. [ 22 s]\n",
      "Epoch: 5 - 00160/00547 - Loss: 0.09100. [ 25 s]\n",
      "Epoch: 5 - 00180/00547 - Loss: 0.09902. [ 28 s]\n",
      "Epoch: 5 - 00200/00547 - Loss: 0.12796. [ 31 s]\n",
      "Epoch: 5 - 00220/00547 - Loss: 0.10245. [ 34 s]\n",
      "Epoch: 5 - 00240/00547 - Loss: 0.20338. [ 37 s]\n",
      "Epoch: 5 - 00260/00547 - Loss: 0.11464. [ 40 s]\n",
      "Epoch: 5 - 00280/00547 - Loss: 0.08852. [ 43 s]\n",
      "Epoch: 5 - 00300/00547 - Loss: 0.14728. [ 47 s]\n",
      "Epoch: 5 - 00320/00547 - Loss: 0.18074. [ 50 s]\n",
      "Epoch: 5 - 00340/00547 - Loss: 0.12872. [ 53 s]\n",
      "Epoch: 5 - 00360/00547 - Loss: 0.20493. [ 56 s]\n",
      "Epoch: 5 - 00380/00547 - Loss: 0.12925. [ 59 s]\n",
      "Epoch: 5 - 00400/00547 - Loss: 0.33976. [ 62 s]\n",
      "Epoch: 5 - 00420/00547 - Loss: 0.09050. [ 66 s]\n",
      "Epoch: 5 - 00440/00547 - Loss: 0.09745. [ 69 s]\n",
      "Epoch: 5 - 00460/00547 - Loss: 0.43867. [ 72 s]\n",
      "Epoch: 5 - 00480/00547 - Loss: 0.20898. [ 75 s]\n",
      "Epoch: 5 - 00500/00547 - Loss: 0.17562. [ 78 s]\n",
      "Epoch: 5 - 00520/00547 - Loss: 0.09140. [ 81 s]\n",
      "Epoch: 5 - 00540/00547 - Loss: 0.10696. [ 85 s]\n",
      "Epoch: 5 - loss(trn/val):0.11411/0.22189, acc(val):90.99%, lr=0.00010 [BEST]. [86s] @25 samples/s \n",
      "Epoch: 6 - 00020/00547 - Loss: 0.08075. [  3 s]\n",
      "Epoch: 6 - 00040/00547 - Loss: 0.07792. [  6 s]\n",
      "Epoch: 6 - 00060/00547 - Loss: 0.14489. [  9 s]\n",
      "Epoch: 6 - 00080/00547 - Loss: 0.12995. [ 12 s]\n",
      "Epoch: 6 - 00100/00547 - Loss: 0.14471. [ 15 s]\n",
      "Epoch: 6 - 00120/00547 - Loss: 0.31750. [ 19 s]\n",
      "Epoch: 6 - 00140/00547 - Loss: 0.08853. [ 22 s]\n",
      "Epoch: 6 - 00160/00547 - Loss: 0.07613. [ 25 s]\n",
      "Epoch: 6 - 00180/00547 - Loss: 0.09738. [ 28 s]\n",
      "Epoch: 6 - 00200/00547 - Loss: 0.09670. [ 31 s]\n",
      "Epoch: 6 - 00220/00547 - Loss: 0.10643. [ 34 s]\n",
      "Epoch: 6 - 00240/00547 - Loss: 0.10017. [ 37 s]\n",
      "Epoch: 6 - 00260/00547 - Loss: 0.08239. [ 40 s]\n",
      "Epoch: 6 - 00280/00547 - Loss: 0.13301. [ 43 s]\n",
      "Epoch: 6 - 00300/00547 - Loss: 0.13964. [ 47 s]\n",
      "Epoch: 6 - 00320/00547 - Loss: 0.09997. [ 50 s]\n",
      "Epoch: 6 - 00340/00547 - Loss: 0.13635. [ 53 s]\n",
      "Epoch: 6 - 00360/00547 - Loss: 0.06703. [ 56 s]\n",
      "Epoch: 6 - 00380/00547 - Loss: 0.09762. [ 59 s]\n",
      "Epoch: 6 - 00400/00547 - Loss: 0.08368. [ 63 s]\n",
      "Epoch: 6 - 00420/00547 - Loss: 0.05352. [ 66 s]\n",
      "Epoch: 6 - 00440/00547 - Loss: 0.09430. [ 69 s]\n",
      "Epoch: 6 - 00460/00547 - Loss: 0.06483. [ 72 s]\n",
      "Epoch: 6 - 00480/00547 - Loss: 0.18485. [ 75 s]\n",
      "Epoch: 6 - 00500/00547 - Loss: 0.10432. [ 78 s]\n",
      "Epoch: 6 - 00520/00547 - Loss: 0.24011. [ 82 s]\n",
      "Epoch: 6 - 00540/00547 - Loss: 0.18619. [ 85 s]\n",
      "Epoch: 6 - loss(trn/val):0.10038/0.20988, acc(val):91.79%, lr=0.00010 [BEST]. [86s] @25 samples/s \n",
      "Epoch: 7 - 00020/00547 - Loss: 0.07437. [  3 s]\n",
      "Epoch: 7 - 00040/00547 - Loss: 0.13694. [  6 s]\n",
      "Epoch: 7 - 00060/00547 - Loss: 0.23047. [  9 s]\n",
      "Epoch: 7 - 00080/00547 - Loss: 0.22370. [ 12 s]\n",
      "Epoch: 7 - 00100/00547 - Loss: 0.06079. [ 15 s]\n",
      "Epoch: 7 - 00120/00547 - Loss: 0.24435. [ 19 s]\n",
      "Epoch: 7 - 00140/00547 - Loss: 0.07469. [ 22 s]\n",
      "Epoch: 7 - 00160/00547 - Loss: 0.59809. [ 25 s]\n",
      "Epoch: 7 - 00180/00547 - Loss: 0.07823. [ 28 s]\n",
      "Epoch: 7 - 00200/00547 - Loss: 0.11806. [ 31 s]\n",
      "Epoch: 7 - 00220/00547 - Loss: 0.07953. [ 34 s]\n",
      "Epoch: 7 - 00240/00547 - Loss: 0.13113. [ 37 s]\n",
      "Epoch: 7 - 00260/00547 - Loss: 0.08672. [ 40 s]\n",
      "Epoch: 7 - 00280/00547 - Loss: 0.06824. [ 43 s]\n",
      "Epoch: 7 - 00300/00547 - Loss: 0.09957. [ 47 s]\n",
      "Epoch: 7 - 00320/00547 - Loss: 0.19793. [ 50 s]\n",
      "Epoch: 7 - 00340/00547 - Loss: 0.15222. [ 53 s]\n",
      "Epoch: 7 - 00360/00547 - Loss: 0.41966. [ 56 s]\n",
      "Epoch: 7 - 00380/00547 - Loss: 0.18080. [ 59 s]\n",
      "Epoch: 7 - 00400/00547 - Loss: 0.20971. [ 62 s]\n",
      "Epoch: 7 - 00420/00547 - Loss: 0.09337. [ 65 s]\n",
      "Epoch: 7 - 00440/00547 - Loss: 0.13596. [ 69 s]\n",
      "Epoch: 7 - 00460/00547 - Loss: 0.18068. [ 72 s]\n",
      "Epoch: 7 - 00480/00547 - Loss: 0.06947. [ 75 s]\n",
      "Epoch: 7 - 00500/00547 - Loss: 0.06451. [ 78 s]\n",
      "Epoch: 7 - 00520/00547 - Loss: 0.19192. [ 81 s]\n",
      "Epoch: 7 - 00540/00547 - Loss: 0.09416. [ 84 s]\n",
      "Epoch: 7 - loss(trn/val):0.10881/0.47957, acc(val):88.97%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 8 - 00020/00547 - Loss: 0.08987. [  3 s]\n",
      "Epoch: 8 - 00040/00547 - Loss: 0.09375. [  6 s]\n",
      "Epoch: 8 - 00060/00547 - Loss: 0.12928. [  9 s]\n",
      "Epoch: 8 - 00080/00547 - Loss: 0.07965. [ 12 s]\n",
      "Epoch: 8 - 00100/00547 - Loss: 0.11944. [ 16 s]\n",
      "Epoch: 8 - 00120/00547 - Loss: 0.07887. [ 19 s]\n",
      "Epoch: 8 - 00140/00547 - Loss: 0.12692. [ 22 s]\n",
      "Epoch: 8 - 00160/00547 - Loss: 0.12952. [ 25 s]\n",
      "Epoch: 8 - 00180/00547 - Loss: 0.07624. [ 28 s]\n",
      "Epoch: 8 - 00200/00547 - Loss: 0.07729. [ 31 s]\n",
      "Epoch: 8 - 00220/00547 - Loss: 0.13882. [ 34 s]\n",
      "Epoch: 8 - 00240/00547 - Loss: 0.07515. [ 37 s]\n",
      "Epoch: 8 - 00260/00547 - Loss: 0.10861. [ 40 s]\n",
      "Epoch: 8 - 00280/00547 - Loss: 0.24709. [ 44 s]\n",
      "Epoch: 8 - 00300/00547 - Loss: 0.10262. [ 47 s]\n",
      "Epoch: 8 - 00320/00547 - Loss: 0.09061. [ 50 s]\n",
      "Epoch: 8 - 00340/00547 - Loss: 0.14224. [ 53 s]\n",
      "Epoch: 8 - 00360/00547 - Loss: 0.33550. [ 56 s]\n",
      "Epoch: 8 - 00380/00547 - Loss: 0.09316. [ 60 s]\n",
      "Epoch: 8 - 00400/00547 - Loss: 0.06531. [ 63 s]\n",
      "Epoch: 8 - 00420/00547 - Loss: 0.11912. [ 66 s]\n",
      "Epoch: 8 - 00440/00547 - Loss: 0.07395. [ 69 s]\n",
      "Epoch: 8 - 00460/00547 - Loss: 0.08399. [ 72 s]\n",
      "Epoch: 8 - 00480/00547 - Loss: 0.25579. [ 75 s]\n",
      "Epoch: 8 - 00500/00547 - Loss: 0.12469. [ 78 s]\n",
      "Epoch: 8 - 00520/00547 - Loss: 0.43442. [ 81 s]\n",
      "Epoch: 8 - 00540/00547 - Loss: 0.12116. [ 85 s]\n",
      "Epoch: 8 - loss(trn/val):0.10034/0.24775, acc(val):90.26%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 9 - 00020/00547 - Loss: 0.06704. [  3 s]\n",
      "Epoch: 9 - 00040/00547 - Loss: 0.11896. [  6 s]\n",
      "Epoch: 9 - 00060/00547 - Loss: 0.11405. [  9 s]\n",
      "Epoch: 9 - 00080/00547 - Loss: 0.05107. [ 12 s]\n",
      "Epoch: 9 - 00100/00547 - Loss: 0.07193. [ 15 s]\n",
      "Epoch: 9 - 00120/00547 - Loss: 0.06247. [ 19 s]\n",
      "Epoch: 9 - 00140/00547 - Loss: 0.07795. [ 22 s]\n",
      "Epoch: 9 - 00160/00547 - Loss: 0.25224. [ 25 s]\n",
      "Epoch: 9 - 00180/00547 - Loss: 0.11674. [ 28 s]\n",
      "Epoch: 9 - 00200/00547 - Loss: 0.22362. [ 31 s]\n",
      "Epoch: 9 - 00220/00547 - Loss: 0.06839. [ 34 s]\n",
      "Epoch: 9 - 00240/00547 - Loss: 0.10317. [ 37 s]\n",
      "Epoch: 9 - 00260/00547 - Loss: 0.07984. [ 40 s]\n",
      "Epoch: 9 - 00280/00547 - Loss: 0.11518. [ 43 s]\n",
      "Epoch: 9 - 00300/00547 - Loss: 0.07175. [ 47 s]\n",
      "Epoch: 9 - 00320/00547 - Loss: 0.25925. [ 50 s]\n",
      "Epoch: 9 - 00340/00547 - Loss: 0.18924. [ 53 s]\n",
      "Epoch: 9 - 00360/00547 - Loss: 0.13990. [ 56 s]\n",
      "Epoch: 9 - 00380/00547 - Loss: 0.12711. [ 59 s]\n",
      "Epoch: 9 - 00400/00547 - Loss: 0.06222. [ 63 s]\n",
      "Epoch: 9 - 00420/00547 - Loss: 0.16966. [ 66 s]\n",
      "Epoch: 9 - 00440/00547 - Loss: 0.18378. [ 69 s]\n",
      "Epoch: 9 - 00460/00547 - Loss: 0.09314. [ 72 s]\n",
      "Epoch: 9 - 00480/00547 - Loss: 0.06531. [ 75 s]\n",
      "Epoch: 9 - 00500/00547 - Loss: 0.20572. [ 78 s]\n",
      "Epoch: 9 - 00520/00547 - Loss: 0.10309. [ 82 s]\n",
      "Epoch: 9 - 00540/00547 - Loss: 0.04529. [ 85 s]\n",
      "Epoch: 9 - loss(trn/val):0.08682/0.26193, acc(val):90.06%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 10 - 00020/00547 - Loss: 0.05444. [  3 s]\n",
      "Epoch: 10 - 00040/00547 - Loss: 0.06453. [  6 s]\n",
      "Epoch: 10 - 00060/00547 - Loss: 0.05748. [  9 s]\n",
      "Epoch: 10 - 00080/00547 - Loss: 0.05973. [ 12 s]\n",
      "Epoch: 10 - 00100/00547 - Loss: 0.08913. [ 15 s]\n",
      "Epoch: 10 - 00120/00547 - Loss: 0.07551. [ 18 s]\n",
      "Epoch: 10 - 00140/00547 - Loss: 0.10992. [ 22 s]\n",
      "Epoch: 10 - 00160/00547 - Loss: 0.10574. [ 25 s]\n",
      "Epoch: 10 - 00180/00547 - Loss: 0.11044. [ 28 s]\n",
      "Epoch: 10 - 00200/00547 - Loss: 0.09838. [ 31 s]\n",
      "Epoch: 10 - 00220/00547 - Loss: 0.05765. [ 34 s]\n",
      "Epoch: 10 - 00240/00547 - Loss: 0.05860. [ 37 s]\n",
      "Epoch: 10 - 00260/00547 - Loss: 0.08156. [ 40 s]\n",
      "Epoch: 10 - 00280/00547 - Loss: 0.05702. [ 43 s]\n",
      "Epoch: 10 - 00300/00547 - Loss: 0.06179. [ 47 s]\n",
      "Epoch: 10 - 00320/00547 - Loss: 0.04595. [ 50 s]\n",
      "Epoch: 10 - 00340/00547 - Loss: 0.05942. [ 53 s]\n",
      "Epoch: 10 - 00360/00547 - Loss: 0.10555. [ 56 s]\n",
      "Epoch: 10 - 00380/00547 - Loss: 0.06122. [ 59 s]\n",
      "Epoch: 10 - 00400/00547 - Loss: 0.10235. [ 62 s]\n",
      "Epoch: 10 - 00420/00547 - Loss: 0.05865. [ 66 s]\n",
      "Epoch: 10 - 00440/00547 - Loss: 0.12520. [ 69 s]\n",
      "Epoch: 10 - 00460/00547 - Loss: 0.11064. [ 72 s]\n",
      "Epoch: 10 - 00480/00547 - Loss: 0.11904. [ 75 s]\n",
      "Epoch: 10 - 00500/00547 - Loss: 0.05946. [ 78 s]\n",
      "Epoch: 10 - 00520/00547 - Loss: 0.04858. [ 81 s]\n",
      "Epoch: 10 - 00540/00547 - Loss: 0.10475. [ 84 s]\n",
      "Epoch: 10 - loss(trn/val):0.07942/0.21220, acc(val):92.07%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 11 - 00020/00547 - Loss: 0.07601. [  3 s]\n",
      "Epoch: 11 - 00040/00547 - Loss: 0.11825. [  6 s]\n",
      "Epoch: 11 - 00060/00547 - Loss: 0.12130. [  9 s]\n",
      "Epoch: 11 - 00080/00547 - Loss: 0.03988. [ 12 s]\n",
      "Epoch: 11 - 00100/00547 - Loss: 0.06397. [ 15 s]\n",
      "Epoch: 11 - 00120/00547 - Loss: 0.05067. [ 18 s]\n",
      "Epoch: 11 - 00140/00547 - Loss: 0.06241. [ 22 s]\n",
      "Epoch: 11 - 00160/00547 - Loss: 0.07504. [ 25 s]\n",
      "Epoch: 11 - 00180/00547 - Loss: 0.04117. [ 28 s]\n",
      "Epoch: 11 - 00200/00547 - Loss: 0.08785. [ 31 s]\n",
      "Epoch: 11 - 00220/00547 - Loss: 0.10776. [ 34 s]\n",
      "Epoch: 11 - 00240/00547 - Loss: 0.10680. [ 37 s]\n",
      "Epoch: 11 - 00260/00547 - Loss: 0.14037. [ 40 s]\n",
      "Epoch: 11 - 00280/00547 - Loss: 0.17019. [ 43 s]\n",
      "Epoch: 11 - 00300/00547 - Loss: 0.11471. [ 47 s]\n",
      "Epoch: 11 - 00320/00547 - Loss: 0.07667. [ 50 s]\n",
      "Epoch: 11 - 00340/00547 - Loss: 0.07923. [ 53 s]\n",
      "Epoch: 11 - 00360/00547 - Loss: 0.10211. [ 56 s]\n",
      "Epoch: 11 - 00380/00547 - Loss: 0.09248. [ 59 s]\n",
      "Epoch: 11 - 00400/00547 - Loss: 0.10931. [ 62 s]\n",
      "Epoch: 11 - 00420/00547 - Loss: 0.12935. [ 66 s]\n",
      "Epoch: 11 - 00440/00547 - Loss: 0.06305. [ 69 s]\n",
      "Epoch: 11 - 00460/00547 - Loss: 0.11545. [ 72 s]\n",
      "Epoch: 11 - 00480/00547 - Loss: 0.15024. [ 75 s]\n",
      "Epoch: 11 - 00500/00547 - Loss: 0.07580. [ 78 s]\n",
      "Epoch: 11 - 00520/00547 - Loss: 0.14477. [ 81 s]\n",
      "Epoch: 11 - 00540/00547 - Loss: 0.07369. [ 84 s]\n",
      "Epoch: 11 - loss(trn/val):0.07150/0.25308, acc(val):90.93%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 12 - 00020/00547 - Loss: 0.05332. [  3 s]\n",
      "Epoch: 12 - 00040/00547 - Loss: 0.05831. [  6 s]\n",
      "Epoch: 12 - 00060/00547 - Loss: 0.04950. [  9 s]\n",
      "Epoch: 12 - 00080/00547 - Loss: 0.05116. [ 12 s]\n",
      "Epoch: 12 - 00100/00547 - Loss: 0.07308. [ 15 s]\n",
      "Epoch: 12 - 00120/00547 - Loss: 0.13525. [ 19 s]\n",
      "Epoch: 12 - 00140/00547 - Loss: 0.15844. [ 22 s]\n",
      "Epoch: 12 - 00160/00547 - Loss: 0.04317. [ 25 s]\n",
      "Epoch: 12 - 00180/00547 - Loss: 0.13558. [ 28 s]\n",
      "Epoch: 12 - 00200/00547 - Loss: 0.08609. [ 31 s]\n",
      "Epoch: 12 - 00220/00547 - Loss: 0.05277. [ 34 s]\n",
      "Epoch: 12 - 00240/00547 - Loss: 0.07600. [ 37 s]\n",
      "Epoch: 12 - 00260/00547 - Loss: 0.10803. [ 40 s]\n",
      "Epoch: 12 - 00280/00547 - Loss: 0.08848. [ 43 s]\n",
      "Epoch: 12 - 00300/00547 - Loss: 0.04174. [ 47 s]\n",
      "Epoch: 12 - 00320/00547 - Loss: 0.08934. [ 50 s]\n",
      "Epoch: 12 - 00340/00547 - Loss: 0.06167. [ 53 s]\n",
      "Epoch: 12 - 00360/00547 - Loss: 0.03864. [ 56 s]\n",
      "Epoch: 12 - 00380/00547 - Loss: 0.08820. [ 59 s]\n",
      "Epoch: 12 - 00400/00547 - Loss: 0.06175. [ 63 s]\n",
      "Epoch: 12 - 00420/00547 - Loss: 0.04110. [ 66 s]\n",
      "Epoch: 12 - 00440/00547 - Loss: 0.03577. [ 69 s]\n",
      "Epoch: 12 - 00460/00547 - Loss: 0.06955. [ 72 s]\n",
      "Epoch: 12 - 00480/00547 - Loss: 0.07721. [ 75 s]\n",
      "Epoch: 12 - 00500/00547 - Loss: 0.09822. [ 78 s]\n",
      "Epoch: 12 - 00520/00547 - Loss: 0.08978. [ 81 s]\n",
      "Epoch: 12 - 00540/00547 - Loss: 0.08430. [ 84 s]\n",
      "Epoch: 12 - loss(trn/val):0.06520/0.25550, acc(val):90.89%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 13 - 00020/00547 - Loss: 0.02845. [  3 s]\n",
      "Epoch: 13 - 00040/00547 - Loss: 0.15481. [  6 s]\n",
      "Epoch: 13 - 00060/00547 - Loss: 0.21662. [  9 s]\n",
      "Epoch: 13 - 00080/00547 - Loss: 0.04334. [ 12 s]\n",
      "Epoch: 13 - 00100/00547 - Loss: 0.19555. [ 15 s]\n",
      "Epoch: 13 - 00120/00547 - Loss: 0.14302. [ 18 s]\n",
      "Epoch: 13 - 00140/00547 - Loss: 0.07728. [ 21 s]\n",
      "Epoch: 13 - 00160/00547 - Loss: 0.05405. [ 24 s]\n",
      "Epoch: 13 - 00180/00547 - Loss: 0.04500. [ 27 s]\n",
      "Epoch: 13 - 00200/00547 - Loss: 0.28233. [ 30 s]\n",
      "Epoch: 13 - 00220/00547 - Loss: 0.11867. [ 34 s]\n",
      "Epoch: 13 - 00240/00547 - Loss: 0.03994. [ 37 s]\n",
      "Epoch: 13 - 00260/00547 - Loss: 0.05079. [ 40 s]\n",
      "Epoch: 13 - 00280/00547 - Loss: 0.08838. [ 43 s]\n",
      "Epoch: 13 - 00300/00547 - Loss: 0.10430. [ 46 s]\n",
      "Epoch: 13 - 00320/00547 - Loss: 0.06262. [ 49 s]\n",
      "Epoch: 13 - 00340/00547 - Loss: 0.26589. [ 52 s]\n",
      "Epoch: 13 - 00360/00547 - Loss: 0.12968. [ 55 s]\n",
      "Epoch: 13 - 00380/00547 - Loss: 0.04959. [ 58 s]\n",
      "Epoch: 13 - 00400/00547 - Loss: 0.10794. [ 61 s]\n",
      "Epoch: 13 - 00420/00547 - Loss: 0.14561. [ 64 s]\n",
      "Epoch: 13 - 00440/00547 - Loss: 0.08014. [ 67 s]\n",
      "Epoch: 13 - 00460/00547 - Loss: 0.04134. [ 71 s]\n",
      "Epoch: 13 - 00480/00547 - Loss: 0.06366. [ 74 s]\n",
      "Epoch: 13 - 00500/00547 - Loss: 0.04990. [ 77 s]\n",
      "Epoch: 13 - 00520/00547 - Loss: 0.16186. [ 80 s]\n",
      "Epoch: 13 - 00540/00547 - Loss: 0.04022. [ 83 s]\n",
      "Epoch: 13 - loss(trn/val):0.06290/0.44821, acc(val):86.47%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 14 - 00020/00547 - Loss: 0.08706. [  3 s]\n",
      "Epoch: 14 - 00040/00547 - Loss: 0.04402. [  6 s]\n",
      "Epoch: 14 - 00060/00547 - Loss: 0.09165. [  9 s]\n",
      "Epoch: 14 - 00080/00547 - Loss: 0.04993. [ 12 s]\n",
      "Epoch: 14 - 00100/00547 - Loss: 0.05949. [ 15 s]\n",
      "Epoch: 14 - 00120/00547 - Loss: 0.04104. [ 18 s]\n",
      "Epoch: 14 - 00140/00547 - Loss: 0.05178. [ 21 s]\n",
      "Epoch: 14 - 00160/00547 - Loss: 0.05311. [ 24 s]\n",
      "Epoch: 14 - 00180/00547 - Loss: 0.04602. [ 27 s]\n",
      "Epoch: 14 - 00200/00547 - Loss: 0.05283. [ 31 s]\n",
      "Epoch: 14 - 00220/00547 - Loss: 0.05298. [ 34 s]\n",
      "Epoch: 14 - 00240/00547 - Loss: 0.03175. [ 37 s]\n",
      "Epoch: 14 - 00260/00547 - Loss: 0.05314. [ 40 s]\n",
      "Epoch: 14 - 00280/00547 - Loss: 0.07413. [ 43 s]\n",
      "Epoch: 14 - 00300/00547 - Loss: 0.05308. [ 46 s]\n",
      "Epoch: 14 - 00320/00547 - Loss: 0.04568. [ 49 s]\n",
      "Epoch: 14 - 00340/00547 - Loss: 0.04823. [ 52 s]\n",
      "Epoch: 14 - 00360/00547 - Loss: 0.08734. [ 55 s]\n",
      "Epoch: 14 - 00380/00547 - Loss: 0.02967. [ 58 s]\n",
      "Epoch: 14 - 00400/00547 - Loss: 0.27447. [ 61 s]\n",
      "Epoch: 14 - 00420/00547 - Loss: 0.09006. [ 65 s]\n",
      "Epoch: 14 - 00440/00547 - Loss: 0.07647. [ 68 s]\n",
      "Epoch: 14 - 00460/00547 - Loss: 0.10336. [ 71 s]\n",
      "Epoch: 14 - 00480/00547 - Loss: 0.16083. [ 74 s]\n",
      "Epoch: 14 - 00500/00547 - Loss: 0.03702. [ 77 s]\n",
      "Epoch: 14 - 00520/00547 - Loss: 0.09000. [ 80 s]\n",
      "Epoch: 14 - 00540/00547 - Loss: 0.08723. [ 83 s]\n",
      "Epoch: 14 - loss(trn/val):0.06467/0.36766, acc(val):89.32%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 15 - 00020/00547 - Loss: 0.06446. [  3 s]\n",
      "Epoch: 15 - 00040/00547 - Loss: 0.46924. [  6 s]\n",
      "Epoch: 15 - 00060/00547 - Loss: 0.07835. [  9 s]\n",
      "Epoch: 15 - 00080/00547 - Loss: 0.05386. [ 12 s]\n",
      "Epoch: 15 - 00100/00547 - Loss: 0.09087. [ 15 s]\n",
      "Epoch: 15 - 00120/00547 - Loss: 0.11658. [ 18 s]\n",
      "Epoch: 15 - 00140/00547 - Loss: 0.09251. [ 21 s]\n",
      "Epoch: 15 - 00160/00547 - Loss: 0.06955. [ 24 s]\n",
      "Epoch: 15 - 00180/00547 - Loss: 0.12440. [ 27 s]\n",
      "Epoch: 15 - 00200/00547 - Loss: 0.04736. [ 31 s]\n",
      "Epoch: 15 - 00220/00547 - Loss: 0.10356. [ 34 s]\n",
      "Epoch: 15 - 00240/00547 - Loss: 0.05929. [ 37 s]\n",
      "Epoch: 15 - 00260/00547 - Loss: 0.09754. [ 40 s]\n",
      "Epoch: 15 - 00280/00547 - Loss: 0.04824. [ 43 s]\n",
      "Epoch: 15 - 00300/00547 - Loss: 0.08924. [ 46 s]\n",
      "Epoch: 15 - 00320/00547 - Loss: 0.04096. [ 49 s]\n",
      "Epoch: 15 - 00340/00547 - Loss: 0.06757. [ 52 s]\n",
      "Epoch: 15 - 00360/00547 - Loss: 0.09567. [ 55 s]\n",
      "Epoch: 15 - 00380/00547 - Loss: 0.05707. [ 58 s]\n",
      "Epoch: 15 - 00400/00547 - Loss: 0.06167. [ 61 s]\n",
      "Epoch: 15 - 00420/00547 - Loss: 0.06518. [ 65 s]\n",
      "Epoch: 15 - 00440/00547 - Loss: 0.05022. [ 68 s]\n",
      "Epoch: 15 - 00460/00547 - Loss: 0.07011. [ 71 s]\n",
      "Epoch: 15 - 00480/00547 - Loss: 0.05771. [ 74 s]\n",
      "Epoch: 15 - 00500/00547 - Loss: 0.04650. [ 77 s]\n",
      "Epoch: 15 - 00520/00547 - Loss: 0.08527. [ 80 s]\n",
      "Epoch: 15 - 00540/00547 - Loss: 0.05380. [ 83 s]\n",
      "Epoch: 15 - loss(trn/val):0.05821/0.28147, acc(val):91.42%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 16 - 00020/00547 - Loss: 0.04928. [  3 s]\n",
      "Epoch: 16 - 00040/00547 - Loss: 0.02748. [  6 s]\n",
      "Epoch: 16 - 00060/00547 - Loss: 0.09262. [  9 s]\n",
      "Epoch: 16 - 00080/00547 - Loss: 0.10892. [ 12 s]\n",
      "Epoch: 16 - 00100/00547 - Loss: 0.04006. [ 15 s]\n",
      "Epoch: 16 - 00120/00547 - Loss: 0.03780. [ 18 s]\n",
      "Epoch: 16 - 00140/00547 - Loss: 0.03239. [ 21 s]\n",
      "Epoch: 16 - 00160/00547 - Loss: 0.07237. [ 24 s]\n",
      "Epoch: 16 - 00180/00547 - Loss: 0.06118. [ 27 s]\n",
      "Epoch: 16 - 00200/00547 - Loss: 0.05756. [ 31 s]\n",
      "Epoch: 16 - 00220/00547 - Loss: 0.04883. [ 34 s]\n",
      "Epoch: 16 - 00240/00547 - Loss: 0.03377. [ 37 s]\n",
      "Epoch: 16 - 00260/00547 - Loss: 0.10827. [ 40 s]\n",
      "Epoch: 16 - 00280/00547 - Loss: 0.03823. [ 43 s]\n",
      "Epoch: 16 - 00300/00547 - Loss: 0.03895. [ 46 s]\n",
      "Epoch: 16 - 00320/00547 - Loss: 0.06516. [ 49 s]\n",
      "Epoch: 16 - 00340/00547 - Loss: 0.05044. [ 52 s]\n",
      "Epoch: 16 - 00360/00547 - Loss: 0.18108. [ 55 s]\n",
      "Epoch: 16 - 00380/00547 - Loss: 0.03046. [ 58 s]\n",
      "Epoch: 16 - 00400/00547 - Loss: 0.03667. [ 61 s]\n",
      "Epoch: 16 - 00420/00547 - Loss: 0.06827. [ 64 s]\n",
      "Epoch: 16 - 00440/00547 - Loss: 0.06539. [ 68 s]\n",
      "Epoch: 16 - 00460/00547 - Loss: 0.13168. [ 71 s]\n",
      "Epoch: 16 - 00480/00547 - Loss: 0.05739. [ 74 s]\n",
      "Epoch: 16 - 00500/00547 - Loss: 0.03792. [ 77 s]\n",
      "Epoch: 16 - 00520/00547 - Loss: 0.11414. [ 80 s]\n",
      "Epoch: 16 - 00540/00547 - Loss: 0.12097. [ 83 s]\n",
      "Epoch: 16 - loss(trn/val):0.05463/0.28348, acc(val):91.14%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 17 - 00020/00547 - Loss: 0.06755. [  3 s]\n",
      "Epoch: 17 - 00040/00547 - Loss: 0.07446. [  6 s]\n",
      "Epoch: 17 - 00060/00547 - Loss: 0.04617. [  9 s]\n",
      "Epoch: 17 - 00080/00547 - Loss: 0.05381. [ 12 s]\n",
      "Epoch: 17 - 00100/00547 - Loss: 0.08909. [ 15 s]\n",
      "Epoch: 17 - 00120/00547 - Loss: 0.03309. [ 18 s]\n",
      "Epoch: 17 - 00140/00547 - Loss: 0.04589. [ 21 s]\n",
      "Epoch: 17 - 00160/00547 - Loss: 0.11971. [ 24 s]\n",
      "Epoch: 17 - 00180/00547 - Loss: 0.08732. [ 27 s]\n",
      "Epoch: 17 - 00200/00547 - Loss: 0.04271. [ 31 s]\n",
      "Epoch: 17 - 00220/00547 - Loss: 0.04101. [ 34 s]\n",
      "Epoch: 17 - 00240/00547 - Loss: 0.05544. [ 37 s]\n",
      "Epoch: 17 - 00260/00547 - Loss: 0.02905. [ 40 s]\n",
      "Epoch: 17 - 00280/00547 - Loss: 0.05970. [ 43 s]\n",
      "Epoch: 17 - 00300/00547 - Loss: 0.04091. [ 46 s]\n",
      "Epoch: 17 - 00320/00547 - Loss: 0.04364. [ 49 s]\n",
      "Epoch: 17 - 00340/00547 - Loss: 0.04490. [ 52 s]\n",
      "Epoch: 17 - 00360/00547 - Loss: 0.03148. [ 55 s]\n",
      "Epoch: 17 - 00380/00547 - Loss: 0.08118. [ 58 s]\n",
      "Epoch: 17 - 00400/00547 - Loss: 0.06164. [ 62 s]\n",
      "Epoch: 17 - 00420/00547 - Loss: 0.07984. [ 65 s]\n",
      "Epoch: 17 - 00440/00547 - Loss: 0.06184. [ 68 s]\n",
      "Epoch: 17 - 00460/00547 - Loss: 0.03733. [ 71 s]\n",
      "Epoch: 17 - 00480/00547 - Loss: 0.02991. [ 74 s]\n",
      "Epoch: 17 - 00500/00547 - Loss: 0.09957. [ 77 s]\n",
      "Epoch: 17 - 00520/00547 - Loss: 0.06127. [ 80 s]\n",
      "Epoch: 17 - 00540/00547 - Loss: 0.03739. [ 83 s]\n",
      "Epoch: 17 - loss(trn/val):0.05750/0.35053, acc(val):89.28%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 18 - 00020/00547 - Loss: 0.09535. [  3 s]\n",
      "Epoch: 18 - 00040/00547 - Loss: 0.02339. [  6 s]\n",
      "Epoch: 18 - 00060/00547 - Loss: 0.04239. [  9 s]\n",
      "Epoch: 18 - 00080/00547 - Loss: 0.06399. [ 12 s]\n",
      "Epoch: 18 - 00100/00547 - Loss: 0.02852. [ 15 s]\n",
      "Epoch: 18 - 00120/00547 - Loss: 0.03067. [ 18 s]\n",
      "Epoch: 18 - 00140/00547 - Loss: 0.11633. [ 21 s]\n",
      "Epoch: 18 - 00160/00547 - Loss: 0.03100. [ 24 s]\n",
      "Epoch: 18 - 00180/00547 - Loss: 0.02977. [ 28 s]\n",
      "Epoch: 18 - 00200/00547 - Loss: 0.04488. [ 31 s]\n",
      "Epoch: 18 - 00220/00547 - Loss: 0.08631. [ 34 s]\n",
      "Epoch: 18 - 00240/00547 - Loss: 0.07393. [ 37 s]\n",
      "Epoch: 18 - 00260/00547 - Loss: 0.07317. [ 40 s]\n",
      "Epoch: 18 - 00280/00547 - Loss: 0.07320. [ 43 s]\n",
      "Epoch: 18 - 00300/00547 - Loss: 0.07508. [ 46 s]\n",
      "Epoch: 18 - 00320/00547 - Loss: 0.03392. [ 49 s]\n",
      "Epoch: 18 - 00340/00547 - Loss: 0.05341. [ 52 s]\n",
      "Epoch: 18 - 00360/00547 - Loss: 0.04576. [ 55 s]\n",
      "Epoch: 18 - 00380/00547 - Loss: 0.06703. [ 59 s]\n",
      "Epoch: 18 - 00400/00547 - Loss: 0.04608. [ 62 s]\n",
      "Epoch: 18 - 00420/00547 - Loss: 0.07505. [ 65 s]\n",
      "Epoch: 18 - 00440/00547 - Loss: 0.05343. [ 68 s]\n",
      "Epoch: 18 - 00460/00547 - Loss: 0.05893. [ 71 s]\n",
      "Epoch: 18 - 00480/00547 - Loss: 0.05756. [ 74 s]\n",
      "Epoch: 18 - 00500/00547 - Loss: 0.09395. [ 77 s]\n",
      "Epoch: 18 - 00520/00547 - Loss: 0.06445. [ 80 s]\n",
      "Epoch: 18 - 00540/00547 - Loss: 0.61503. [ 83 s]\n",
      "Epoch: 18 - loss(trn/val):0.09360/0.48669, acc(val):87.45%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 19 - 00020/00547 - Loss: 0.07309. [  3 s]\n",
      "Epoch: 19 - 00040/00547 - Loss: 0.10376. [  6 s]\n",
      "Epoch: 19 - 00060/00547 - Loss: 0.06225. [  9 s]\n",
      "Epoch: 19 - 00080/00547 - Loss: 0.05305. [ 12 s]\n",
      "Epoch: 19 - 00100/00547 - Loss: 0.04544. [ 15 s]\n",
      "Epoch: 19 - 00120/00547 - Loss: 0.04750. [ 18 s]\n",
      "Epoch: 19 - 00140/00547 - Loss: 0.07480. [ 21 s]\n",
      "Epoch: 19 - 00160/00547 - Loss: 0.03383. [ 24 s]\n",
      "Epoch: 19 - 00180/00547 - Loss: 0.02904. [ 28 s]\n",
      "Epoch: 19 - 00200/00547 - Loss: 0.07072. [ 31 s]\n",
      "Epoch: 19 - 00220/00547 - Loss: 0.03146. [ 34 s]\n",
      "Epoch: 19 - 00240/00547 - Loss: 0.05272. [ 37 s]\n",
      "Epoch: 19 - 00260/00547 - Loss: 0.05468. [ 40 s]\n",
      "Epoch: 19 - 00280/00547 - Loss: 0.02539. [ 43 s]\n",
      "Epoch: 19 - 00300/00547 - Loss: 0.07673. [ 46 s]\n",
      "Epoch: 19 - 00320/00547 - Loss: 0.06943. [ 49 s]\n",
      "Epoch: 19 - 00340/00547 - Loss: 0.04821. [ 52 s]\n",
      "Epoch: 19 - 00360/00547 - Loss: 0.03768. [ 55 s]\n",
      "Epoch: 19 - 00380/00547 - Loss: 0.03139. [ 58 s]\n",
      "Epoch: 19 - 00400/00547 - Loss: 0.03484. [ 62 s]\n",
      "Epoch: 19 - 00420/00547 - Loss: 0.03451. [ 65 s]\n",
      "Epoch: 19 - 00440/00547 - Loss: 0.03950. [ 68 s]\n",
      "Epoch: 19 - 00460/00547 - Loss: 0.03284. [ 71 s]\n",
      "Epoch: 19 - 00480/00547 - Loss: 0.04631. [ 74 s]\n",
      "Epoch: 19 - 00500/00547 - Loss: 0.03578. [ 77 s]\n",
      "Epoch: 19 - 00520/00547 - Loss: 0.04959. [ 80 s]\n",
      "Epoch: 19 - 00540/00547 - Loss: 0.05591. [ 83 s]\n",
      "Epoch: 19 - loss(trn/val):0.04766/0.34458, acc(val):90.02%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 20 - 00020/00547 - Loss: 0.03695. [  3 s]\n",
      "Epoch: 20 - 00040/00547 - Loss: 0.09745. [  6 s]\n",
      "Epoch: 20 - 00060/00547 - Loss: 0.03482. [  9 s]\n",
      "Epoch: 20 - 00080/00547 - Loss: 0.05259. [ 12 s]\n",
      "Epoch: 20 - 00100/00547 - Loss: 0.03831. [ 15 s]\n",
      "Epoch: 20 - 00120/00547 - Loss: 0.03860. [ 18 s]\n",
      "Epoch: 20 - 00140/00547 - Loss: 0.03747. [ 21 s]\n",
      "Epoch: 20 - 00160/00547 - Loss: 0.03179. [ 24 s]\n",
      "Epoch: 20 - 00180/00547 - Loss: 0.03572. [ 28 s]\n",
      "Epoch: 20 - 00200/00547 - Loss: 0.04195. [ 31 s]\n",
      "Epoch: 20 - 00220/00547 - Loss: 0.04146. [ 34 s]\n",
      "Epoch: 20 - 00240/00547 - Loss: 0.04388. [ 37 s]\n",
      "Epoch: 20 - 00260/00547 - Loss: 0.03299. [ 40 s]\n",
      "Epoch: 20 - 00280/00547 - Loss: 0.12004. [ 43 s]\n",
      "Epoch: 20 - 00300/00547 - Loss: 0.05444. [ 46 s]\n",
      "Epoch: 20 - 00320/00547 - Loss: 0.05241. [ 49 s]\n",
      "Epoch: 20 - 00340/00547 - Loss: 0.05915. [ 52 s]\n",
      "Epoch: 20 - 00360/00547 - Loss: 0.06602. [ 55 s]\n",
      "Epoch: 20 - 00380/00547 - Loss: 0.06674. [ 58 s]\n",
      "Epoch: 20 - 00400/00547 - Loss: 0.03557. [ 62 s]\n",
      "Epoch: 20 - 00420/00547 - Loss: 0.03122. [ 65 s]\n",
      "Epoch: 20 - 00440/00547 - Loss: 0.07751. [ 68 s]\n",
      "Epoch: 20 - 00460/00547 - Loss: 0.03587. [ 71 s]\n",
      "Epoch: 20 - 00480/00547 - Loss: 0.09898. [ 74 s]\n",
      "Epoch: 20 - 00500/00547 - Loss: 0.04590. [ 77 s]\n",
      "Epoch: 20 - 00520/00547 - Loss: 0.06416. [ 80 s]\n",
      "Epoch: 20 - 00540/00547 - Loss: 0.06940. [ 83 s]\n",
      "Epoch: 20 - loss(trn/val):0.04695/0.38386, acc(val):89.19%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 21 - 00020/00547 - Loss: 0.04372. [  3 s]\n",
      "Epoch: 21 - 00040/00547 - Loss: 0.02960. [  6 s]\n",
      "Epoch: 21 - 00060/00547 - Loss: 0.05801. [  9 s]\n",
      "Epoch: 21 - 00080/00547 - Loss: 0.04718. [ 12 s]\n",
      "Epoch: 21 - 00100/00547 - Loss: 0.04243. [ 15 s]\n",
      "Epoch: 21 - 00120/00547 - Loss: 0.05147. [ 18 s]\n",
      "Epoch: 21 - 00140/00547 - Loss: 0.03706. [ 21 s]\n",
      "Epoch: 21 - 00160/00547 - Loss: 0.03920. [ 24 s]\n",
      "Epoch: 21 - 00180/00547 - Loss: 0.03674. [ 28 s]\n",
      "Epoch: 21 - 00200/00547 - Loss: 0.09512. [ 31 s]\n",
      "Epoch: 21 - 00220/00547 - Loss: 0.04634. [ 34 s]\n",
      "Epoch: 21 - 00240/00547 - Loss: 0.16251. [ 37 s]\n",
      "Epoch: 21 - 00260/00547 - Loss: 0.04019. [ 40 s]\n",
      "Epoch: 21 - 00280/00547 - Loss: 0.03958. [ 43 s]\n",
      "Epoch: 21 - 00300/00547 - Loss: 0.17001. [ 46 s]\n",
      "Epoch: 21 - 00320/00547 - Loss: 0.04831. [ 49 s]\n",
      "Epoch: 21 - 00340/00547 - Loss: 0.04430. [ 52 s]\n",
      "Epoch: 21 - 00360/00547 - Loss: 0.09553. [ 56 s]\n",
      "Epoch: 21 - 00380/00547 - Loss: 0.07321. [ 59 s]\n",
      "Epoch: 21 - 00400/00547 - Loss: 0.05050. [ 62 s]\n",
      "Epoch: 21 - 00420/00547 - Loss: 0.02847. [ 65 s]\n",
      "Epoch: 21 - 00440/00547 - Loss: 0.02361. [ 68 s]\n",
      "Epoch: 21 - 00460/00547 - Loss: 0.06400. [ 71 s]\n",
      "Epoch: 21 - 00480/00547 - Loss: 0.04808. [ 74 s]\n",
      "Epoch: 21 - 00500/00547 - Loss: 0.03434. [ 77 s]\n",
      "Epoch: 21 - 00520/00547 - Loss: 0.06682. [ 80 s]\n",
      "Epoch: 21 - 00540/00547 - Loss: 0.05864. [ 84 s]\n",
      "Epoch: 21 - loss(trn/val):0.06999/0.39152, acc(val):88.96%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 22 - 00020/00547 - Loss: 0.10820. [  3 s]\n",
      "Epoch: 22 - 00040/00547 - Loss: 0.04254. [  6 s]\n",
      "Epoch: 22 - 00060/00547 - Loss: 0.04416. [  9 s]\n",
      "Epoch: 22 - 00080/00547 - Loss: 0.06056. [ 12 s]\n",
      "Epoch: 22 - 00100/00547 - Loss: 0.05323. [ 15 s]\n",
      "Epoch: 22 - 00120/00547 - Loss: 0.04176. [ 18 s]\n",
      "Epoch: 22 - 00140/00547 - Loss: 0.03578. [ 21 s]\n",
      "Epoch: 22 - 00160/00547 - Loss: 0.07124. [ 24 s]\n",
      "Epoch: 22 - 00180/00547 - Loss: 0.03833. [ 28 s]\n",
      "Epoch: 22 - 00200/00547 - Loss: 0.02833. [ 31 s]\n",
      "Epoch: 22 - 00220/00547 - Loss: 0.05783. [ 34 s]\n",
      "Epoch: 22 - 00240/00547 - Loss: 0.04874. [ 37 s]\n",
      "Epoch: 22 - 00260/00547 - Loss: 0.04290. [ 40 s]\n",
      "Epoch: 22 - 00280/00547 - Loss: 0.05851. [ 43 s]\n",
      "Epoch: 22 - 00300/00547 - Loss: 0.05683. [ 46 s]\n",
      "Epoch: 22 - 00320/00547 - Loss: 0.04836. [ 49 s]\n",
      "Epoch: 22 - 00340/00547 - Loss: 0.05518. [ 52 s]\n",
      "Epoch: 22 - 00360/00547 - Loss: 0.04416. [ 55 s]\n",
      "Epoch: 22 - 00380/00547 - Loss: 0.05350. [ 59 s]\n",
      "Epoch: 22 - 00400/00547 - Loss: 0.02193. [ 62 s]\n",
      "Epoch: 22 - 00420/00547 - Loss: 0.03038. [ 65 s]\n",
      "Epoch: 22 - 00440/00547 - Loss: 0.04000. [ 68 s]\n",
      "Epoch: 22 - 00460/00547 - Loss: 0.04437. [ 71 s]\n",
      "Epoch: 22 - 00480/00547 - Loss: 0.03978. [ 74 s]\n",
      "Epoch: 22 - 00500/00547 - Loss: 0.02647. [ 77 s]\n",
      "Epoch: 22 - 00520/00547 - Loss: 0.06197. [ 80 s]\n",
      "Epoch: 22 - 00540/00547 - Loss: 0.06875. [ 83 s]\n",
      "Epoch: 22 - loss(trn/val):0.05924/0.28291, acc(val):91.06%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 23 - 00020/00547 - Loss: 0.05625. [  3 s]\n",
      "Epoch: 23 - 00040/00547 - Loss: 0.03900. [  6 s]\n",
      "Epoch: 23 - 00060/00547 - Loss: 0.04312. [  9 s]\n",
      "Epoch: 23 - 00080/00547 - Loss: 0.04011. [ 12 s]\n",
      "Epoch: 23 - 00100/00547 - Loss: 0.05288. [ 15 s]\n",
      "Epoch: 23 - 00120/00547 - Loss: 0.03232. [ 18 s]\n",
      "Epoch: 23 - 00140/00547 - Loss: 0.04171. [ 21 s]\n",
      "Epoch: 23 - 00160/00547 - Loss: 0.05063. [ 25 s]\n",
      "Epoch: 23 - 00180/00547 - Loss: 0.14322. [ 28 s]\n",
      "Epoch: 23 - 00200/00547 - Loss: 0.03110. [ 31 s]\n",
      "Epoch: 23 - 00220/00547 - Loss: 0.36383. [ 34 s]\n",
      "Epoch: 23 - 00240/00547 - Loss: 0.03805. [ 37 s]\n",
      "Epoch: 23 - 00260/00547 - Loss: 0.03509. [ 40 s]\n",
      "Epoch: 23 - 00280/00547 - Loss: 0.01944. [ 43 s]\n",
      "Epoch: 23 - 00300/00547 - Loss: 0.05216. [ 46 s]\n",
      "Epoch: 23 - 00320/00547 - Loss: 0.03979. [ 49 s]\n",
      "Epoch: 23 - 00340/00547 - Loss: 0.05650. [ 52 s]\n",
      "Epoch: 23 - 00360/00547 - Loss: 0.04951. [ 56 s]\n",
      "Epoch: 23 - 00380/00547 - Loss: 0.11588. [ 59 s]\n",
      "Epoch: 23 - 00400/00547 - Loss: 0.04368. [ 62 s]\n",
      "Epoch: 23 - 00420/00547 - Loss: 0.03188. [ 65 s]\n",
      "Epoch: 23 - 00440/00547 - Loss: 0.04378. [ 68 s]\n",
      "Epoch: 23 - 00460/00547 - Loss: 0.04405. [ 71 s]\n",
      "Epoch: 23 - 00480/00547 - Loss: 0.03578. [ 74 s]\n",
      "Epoch: 23 - 00500/00547 - Loss: 0.04298. [ 77 s]\n",
      "Epoch: 23 - 00520/00547 - Loss: 0.12495. [ 80 s]\n",
      "Epoch: 23 - 00540/00547 - Loss: 0.05306. [ 83 s]\n",
      "Epoch: 23 - loss(trn/val):0.04873/0.30268, acc(val):91.40%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 24 - 00020/00547 - Loss: 0.04126. [  3 s]\n",
      "Epoch: 24 - 00040/00547 - Loss: 0.03360. [  6 s]\n",
      "Epoch: 24 - 00060/00547 - Loss: 0.06351. [  9 s]\n",
      "Epoch: 24 - 00080/00547 - Loss: 0.02480. [ 12 s]\n",
      "Epoch: 24 - 00100/00547 - Loss: 0.05432. [ 15 s]\n",
      "Epoch: 24 - 00120/00547 - Loss: 0.03684. [ 18 s]\n",
      "Epoch: 24 - 00140/00547 - Loss: 0.07655. [ 21 s]\n",
      "Epoch: 24 - 00160/00547 - Loss: 0.06630. [ 24 s]\n",
      "Epoch: 24 - 00180/00547 - Loss: 0.03675. [ 28 s]\n",
      "Epoch: 24 - 00200/00547 - Loss: 0.04404. [ 31 s]\n",
      "Epoch: 24 - 00220/00547 - Loss: 0.06200. [ 34 s]\n",
      "Epoch: 24 - 00240/00547 - Loss: 0.03819. [ 37 s]\n",
      "Epoch: 24 - 00260/00547 - Loss: 0.04045. [ 40 s]\n",
      "Epoch: 24 - 00280/00547 - Loss: 0.06872. [ 43 s]\n",
      "Epoch: 24 - 00300/00547 - Loss: 0.03709. [ 46 s]\n",
      "Epoch: 24 - 00320/00547 - Loss: 0.03544. [ 49 s]\n",
      "Epoch: 24 - 00340/00547 - Loss: 0.06038. [ 52 s]\n",
      "Epoch: 24 - 00360/00547 - Loss: 0.06377. [ 55 s]\n",
      "Epoch: 24 - 00380/00547 - Loss: 0.06593. [ 59 s]\n",
      "Epoch: 24 - 00400/00547 - Loss: 0.02159. [ 62 s]\n",
      "Epoch: 24 - 00420/00547 - Loss: 0.07541. [ 65 s]\n",
      "Epoch: 24 - 00440/00547 - Loss: 0.03681. [ 68 s]\n",
      "Epoch: 24 - 00460/00547 - Loss: 0.06494. [ 71 s]\n",
      "Epoch: 24 - 00480/00547 - Loss: 0.04207. [ 74 s]\n",
      "Epoch: 24 - 00500/00547 - Loss: 0.04955. [ 77 s]\n",
      "Epoch: 24 - 00520/00547 - Loss: 0.03913. [ 80 s]\n",
      "Epoch: 24 - 00540/00547 - Loss: 0.08092. [ 83 s]\n",
      "Epoch: 24 - loss(trn/val):0.04904/0.27816, acc(val):91.47%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 25 - 00020/00547 - Loss: 0.03695. [  3 s]\n",
      "Epoch: 25 - 00040/00547 - Loss: 0.03403. [  6 s]\n",
      "Epoch: 25 - 00060/00547 - Loss: 0.04818. [  9 s]\n",
      "Epoch: 25 - 00080/00547 - Loss: 0.04441. [ 12 s]\n",
      "Epoch: 25 - 00100/00547 - Loss: 0.01800. [ 15 s]\n",
      "Epoch: 25 - 00120/00547 - Loss: 0.03906. [ 18 s]\n",
      "Epoch: 25 - 00140/00547 - Loss: 0.08193. [ 21 s]\n",
      "Epoch: 25 - 00160/00547 - Loss: 0.05387. [ 25 s]\n",
      "Epoch: 25 - 00180/00547 - Loss: 0.03982. [ 28 s]\n",
      "Epoch: 25 - 00200/00547 - Loss: 0.04796. [ 31 s]\n",
      "Epoch: 25 - 00220/00547 - Loss: 0.03544. [ 34 s]\n",
      "Epoch: 25 - 00240/00547 - Loss: 0.02490. [ 37 s]\n",
      "Epoch: 25 - 00260/00547 - Loss: 0.03168. [ 40 s]\n",
      "Epoch: 25 - 00280/00547 - Loss: 0.05142. [ 43 s]\n",
      "Epoch: 25 - 00300/00547 - Loss: 0.06075. [ 46 s]\n",
      "Epoch: 25 - 00320/00547 - Loss: 0.10965. [ 49 s]\n",
      "Epoch: 25 - 00340/00547 - Loss: 0.03142. [ 52 s]\n",
      "Epoch: 25 - 00360/00547 - Loss: 0.04777. [ 55 s]\n",
      "Epoch: 25 - 00380/00547 - Loss: 0.03919. [ 58 s]\n",
      "Epoch: 25 - 00400/00547 - Loss: 0.03645. [ 62 s]\n",
      "Epoch: 25 - 00420/00547 - Loss: 0.03308. [ 65 s]\n",
      "Epoch: 25 - 00440/00547 - Loss: 0.06207. [ 68 s]\n",
      "Epoch: 25 - 00460/00547 - Loss: 0.07145. [ 71 s]\n",
      "Epoch: 25 - 00480/00547 - Loss: 0.07067. [ 74 s]\n",
      "Epoch: 25 - 00500/00547 - Loss: 0.03739. [ 77 s]\n",
      "Epoch: 25 - 00520/00547 - Loss: 0.04859. [ 80 s]\n",
      "Epoch: 25 - 00540/00547 - Loss: 0.02564. [ 83 s]\n",
      "Epoch: 25 - loss(trn/val):0.05882/0.27010, acc(val):90.25%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 26 - 00020/00547 - Loss: 0.06104. [  3 s]\n",
      "Epoch: 26 - 00040/00547 - Loss: 0.02363. [  6 s]\n",
      "Epoch: 26 - 00060/00547 - Loss: 0.02911. [  9 s]\n",
      "Epoch: 26 - 00080/00547 - Loss: 0.05537. [ 12 s]\n",
      "Epoch: 26 - 00100/00547 - Loss: 0.03076. [ 15 s]\n",
      "Epoch: 26 - 00120/00547 - Loss: 0.02641. [ 18 s]\n",
      "Epoch: 26 - 00140/00547 - Loss: 0.07604. [ 21 s]\n",
      "Epoch: 26 - 00160/00547 - Loss: 0.03261. [ 24 s]\n",
      "Epoch: 26 - 00180/00547 - Loss: 0.04482. [ 28 s]\n",
      "Epoch: 26 - 00200/00547 - Loss: 0.02675. [ 31 s]\n",
      "Epoch: 26 - 00220/00547 - Loss: 0.11815. [ 34 s]\n",
      "Epoch: 26 - 00240/00547 - Loss: 0.03288. [ 37 s]\n",
      "Epoch: 26 - 00260/00547 - Loss: 0.03577. [ 40 s]\n",
      "Epoch: 26 - 00280/00547 - Loss: 0.05772. [ 43 s]\n",
      "Epoch: 26 - 00300/00547 - Loss: 0.03744. [ 46 s]\n",
      "Epoch: 26 - 00320/00547 - Loss: 0.04424. [ 49 s]\n",
      "Epoch: 26 - 00340/00547 - Loss: 0.14286. [ 52 s]\n",
      "Epoch: 26 - 00360/00547 - Loss: 0.04109. [ 56 s]\n",
      "Epoch: 26 - 00380/00547 - Loss: 0.14257. [ 59 s]\n",
      "Epoch: 26 - 00400/00547 - Loss: 0.05177. [ 62 s]\n",
      "Epoch: 26 - 00420/00547 - Loss: 0.05754. [ 65 s]\n",
      "Epoch: 26 - 00440/00547 - Loss: 0.05731. [ 68 s]\n",
      "Epoch: 26 - 00460/00547 - Loss: 0.03916. [ 71 s]\n",
      "Epoch: 26 - 00480/00547 - Loss: 0.03100. [ 74 s]\n",
      "Epoch: 26 - 00500/00547 - Loss: 0.02651. [ 77 s]\n",
      "Epoch: 26 - 00520/00547 - Loss: 0.02818. [ 80 s]\n",
      "Epoch: 26 - 00540/00547 - Loss: 0.04048. [ 83 s]\n",
      "Epoch: 26 - loss(trn/val):0.03932/0.28347, acc(val):92.36%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 27 - 00020/00547 - Loss: 0.08009. [  3 s]\n",
      "Epoch: 27 - 00040/00547 - Loss: 0.03335. [  6 s]\n",
      "Epoch: 27 - 00060/00547 - Loss: 0.05390. [  9 s]\n",
      "Epoch: 27 - 00080/00547 - Loss: 0.05315. [ 12 s]\n",
      "Epoch: 27 - 00100/00547 - Loss: 0.02373. [ 15 s]\n",
      "Epoch: 27 - 00120/00547 - Loss: 0.14286. [ 18 s]\n",
      "Epoch: 27 - 00140/00547 - Loss: 0.04082. [ 21 s]\n",
      "Epoch: 27 - 00160/00547 - Loss: 0.05777. [ 24 s]\n",
      "Epoch: 27 - 00180/00547 - Loss: 0.07646. [ 28 s]\n",
      "Epoch: 27 - 00200/00547 - Loss: 0.02121. [ 31 s]\n",
      "Epoch: 27 - 00220/00547 - Loss: 0.04358. [ 34 s]\n",
      "Epoch: 27 - 00240/00547 - Loss: 0.05549. [ 37 s]\n",
      "Epoch: 27 - 00260/00547 - Loss: 0.07909. [ 40 s]\n",
      "Epoch: 27 - 00280/00547 - Loss: 0.06169. [ 43 s]\n",
      "Epoch: 27 - 00300/00547 - Loss: 0.04709. [ 46 s]\n",
      "Epoch: 27 - 00320/00547 - Loss: 0.03216. [ 49 s]\n",
      "Epoch: 27 - 00340/00547 - Loss: 0.03320. [ 52 s]\n",
      "Epoch: 27 - 00360/00547 - Loss: 0.03178. [ 55 s]\n",
      "Epoch: 27 - 00380/00547 - Loss: 0.05846. [ 58 s]\n",
      "Epoch: 27 - 00400/00547 - Loss: 0.02325. [ 62 s]\n",
      "Epoch: 27 - 00420/00547 - Loss: 0.02501. [ 65 s]\n",
      "Epoch: 27 - 00440/00547 - Loss: 0.08189. [ 68 s]\n",
      "Epoch: 27 - 00460/00547 - Loss: 0.02526. [ 71 s]\n",
      "Epoch: 27 - 00480/00547 - Loss: 0.03654. [ 74 s]\n",
      "Epoch: 27 - 00500/00547 - Loss: 0.04725. [ 77 s]\n",
      "Epoch: 27 - 00520/00547 - Loss: 0.03527. [ 80 s]\n",
      "Epoch: 27 - 00540/00547 - Loss: 0.02728. [ 83 s]\n",
      "Epoch: 27 - loss(trn/val):0.18696/0.51052, acc(val):87.43%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 28 - 00020/00547 - Loss: 0.08291. [  3 s]\n",
      "Epoch: 28 - 00040/00547 - Loss: 0.06508. [  6 s]\n",
      "Epoch: 28 - 00060/00547 - Loss: 0.05222. [  9 s]\n",
      "Epoch: 28 - 00080/00547 - Loss: 0.03588. [ 12 s]\n",
      "Epoch: 28 - 00100/00547 - Loss: 0.03517. [ 15 s]\n",
      "Epoch: 28 - 00120/00547 - Loss: 0.03171. [ 18 s]\n",
      "Epoch: 28 - 00140/00547 - Loss: 0.09761. [ 21 s]\n",
      "Epoch: 28 - 00160/00547 - Loss: 0.02512. [ 24 s]\n",
      "Epoch: 28 - 00180/00547 - Loss: 0.03216. [ 28 s]\n",
      "Epoch: 28 - 00200/00547 - Loss: 0.04696. [ 31 s]\n",
      "Epoch: 28 - 00220/00547 - Loss: 0.01711. [ 34 s]\n",
      "Epoch: 28 - 00240/00547 - Loss: 0.04598. [ 37 s]\n",
      "Epoch: 28 - 00260/00547 - Loss: 0.06699. [ 40 s]\n",
      "Epoch: 28 - 00280/00547 - Loss: 0.05287. [ 43 s]\n",
      "Epoch: 28 - 00300/00547 - Loss: 0.03188. [ 46 s]\n",
      "Epoch: 28 - 00320/00547 - Loss: 0.04335. [ 49 s]\n",
      "Epoch: 28 - 00340/00547 - Loss: 0.03392. [ 53 s]\n",
      "Epoch: 28 - 00360/00547 - Loss: 0.03874. [ 56 s]\n",
      "Epoch: 28 - 00380/00547 - Loss: 0.04084. [ 59 s]\n",
      "Epoch: 28 - 00400/00547 - Loss: 0.07121. [ 62 s]\n",
      "Epoch: 28 - 00420/00547 - Loss: 0.04219. [ 65 s]\n",
      "Epoch: 28 - 00440/00547 - Loss: 0.10390. [ 68 s]\n",
      "Epoch: 28 - 00460/00547 - Loss: 0.05382. [ 71 s]\n",
      "Epoch: 28 - 00480/00547 - Loss: 0.07436. [ 74 s]\n",
      "Epoch: 28 - 00500/00547 - Loss: 0.03058. [ 77 s]\n",
      "Epoch: 28 - 00520/00547 - Loss: 0.04379. [ 80 s]\n",
      "Epoch: 28 - 00540/00547 - Loss: 0.03261. [ 83 s]\n",
      "Epoch: 28 - loss(trn/val):0.04216/0.31909, acc(val):91.15%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 29 - 00020/00547 - Loss: 0.03723. [  3 s]\n",
      "Epoch: 29 - 00040/00547 - Loss: 0.03196. [  6 s]\n",
      "Epoch: 29 - 00060/00547 - Loss: 0.04075. [  9 s]\n",
      "Epoch: 29 - 00080/00547 - Loss: 0.03537. [ 12 s]\n",
      "Epoch: 29 - 00100/00547 - Loss: 0.05082. [ 15 s]\n",
      "Epoch: 29 - 00120/00547 - Loss: 0.03370. [ 18 s]\n",
      "Epoch: 29 - 00140/00547 - Loss: 0.10491. [ 21 s]\n",
      "Epoch: 29 - 00160/00547 - Loss: 0.06301. [ 24 s]\n",
      "Epoch: 29 - 00180/00547 - Loss: 0.05653. [ 27 s]\n",
      "Epoch: 29 - 00200/00547 - Loss: 0.02994. [ 31 s]\n",
      "Epoch: 29 - 00220/00547 - Loss: 0.03375. [ 34 s]\n",
      "Epoch: 29 - 00240/00547 - Loss: 0.04595. [ 37 s]\n",
      "Epoch: 29 - 00260/00547 - Loss: 0.05635. [ 40 s]\n",
      "Epoch: 29 - 00280/00547 - Loss: 0.03104. [ 43 s]\n",
      "Epoch: 29 - 00300/00547 - Loss: 0.04614. [ 46 s]\n",
      "Epoch: 29 - 00320/00547 - Loss: 0.04815. [ 49 s]\n",
      "Epoch: 29 - 00340/00547 - Loss: 0.04162. [ 52 s]\n",
      "Epoch: 29 - 00360/00547 - Loss: 0.02325. [ 55 s]\n",
      "Epoch: 29 - 00380/00547 - Loss: 0.05292. [ 58 s]\n",
      "Epoch: 29 - 00400/00547 - Loss: 0.04176. [ 61 s]\n",
      "Epoch: 29 - 00420/00547 - Loss: 0.04124. [ 64 s]\n",
      "Epoch: 29 - 00440/00547 - Loss: 0.04872. [ 68 s]\n",
      "Epoch: 29 - 00460/00547 - Loss: 0.08406. [ 71 s]\n",
      "Epoch: 29 - 00480/00547 - Loss: 0.03210. [ 74 s]\n",
      "Epoch: 29 - 00500/00547 - Loss: 0.03385. [ 77 s]\n",
      "Epoch: 29 - 00520/00547 - Loss: 0.03528. [ 80 s]\n",
      "Epoch: 29 - 00540/00547 - Loss: 0.03228. [ 83 s]\n",
      "Epoch: 29 - loss(trn/val):0.03751/0.34969, acc(val):89.95%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 30 - 00020/00547 - Loss: 0.04964. [  3 s]\n",
      "Epoch: 30 - 00040/00547 - Loss: 0.03202. [  6 s]\n",
      "Epoch: 30 - 00060/00547 - Loss: 0.03386. [  9 s]\n",
      "Epoch: 30 - 00080/00547 - Loss: 0.05599. [ 12 s]\n",
      "Epoch: 30 - 00100/00547 - Loss: 0.02996. [ 15 s]\n",
      "Epoch: 30 - 00120/00547 - Loss: 0.05151. [ 18 s]\n",
      "Epoch: 30 - 00140/00547 - Loss: 0.03175. [ 21 s]\n",
      "Epoch: 30 - 00160/00547 - Loss: 0.02801. [ 24 s]\n",
      "Epoch: 30 - 00180/00547 - Loss: 0.03399. [ 28 s]\n",
      "Epoch: 30 - 00200/00547 - Loss: 0.03746. [ 31 s]\n",
      "Epoch: 30 - 00220/00547 - Loss: 0.03701. [ 34 s]\n",
      "Epoch: 30 - 00240/00547 - Loss: 0.03951. [ 37 s]\n",
      "Epoch: 30 - 00260/00547 - Loss: 0.04084. [ 40 s]\n",
      "Epoch: 30 - 00280/00547 - Loss: 0.05828. [ 43 s]\n",
      "Epoch: 30 - 00300/00547 - Loss: 0.05693. [ 46 s]\n",
      "Epoch: 30 - 00320/00547 - Loss: 0.06487. [ 49 s]\n",
      "Epoch: 30 - 00340/00547 - Loss: 0.06999. [ 52 s]\n",
      "Epoch: 30 - 00360/00547 - Loss: 0.04697. [ 56 s]\n",
      "Epoch: 30 - 00380/00547 - Loss: 0.05128. [ 59 s]\n",
      "Epoch: 30 - 00400/00547 - Loss: 0.03415. [ 62 s]\n",
      "Epoch: 30 - 00420/00547 - Loss: 0.04134. [ 65 s]\n",
      "Epoch: 30 - 00440/00547 - Loss: 0.10741. [ 68 s]\n",
      "Epoch: 30 - 00460/00547 - Loss: 0.04788. [ 71 s]\n",
      "Epoch: 30 - 00480/00547 - Loss: 0.04078. [ 74 s]\n",
      "Epoch: 30 - 00500/00547 - Loss: 0.04063. [ 77 s]\n",
      "Epoch: 30 - 00520/00547 - Loss: 0.02965. [ 80 s]\n",
      "Epoch: 30 - 00540/00547 - Loss: 0.02200. [ 83 s]\n",
      "Epoch: 30 - loss(trn/val):0.05208/0.44141, acc(val):90.33%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 31 - 00020/00547 - Loss: 0.05320. [  4 s]\n",
      "Epoch: 31 - 00040/00547 - Loss: 0.02510. [  7 s]\n",
      "Epoch: 31 - 00060/00547 - Loss: 0.02964. [ 10 s]\n",
      "Epoch: 31 - 00080/00547 - Loss: 0.03992. [ 13 s]\n",
      "Epoch: 31 - 00100/00547 - Loss: 0.06344. [ 17 s]\n",
      "Epoch: 31 - 00120/00547 - Loss: 0.07829. [ 20 s]\n",
      "Epoch: 31 - 00140/00547 - Loss: 0.13697. [ 23 s]\n",
      "Epoch: 31 - 00160/00547 - Loss: 0.04438. [ 26 s]\n",
      "Epoch: 31 - 00180/00547 - Loss: 0.02867. [ 29 s]\n",
      "Epoch: 31 - 00200/00547 - Loss: 0.03530. [ 32 s]\n",
      "Epoch: 31 - 00220/00547 - Loss: 0.01586. [ 35 s]\n",
      "Epoch: 31 - 00240/00547 - Loss: 0.07014. [ 38 s]\n",
      "Epoch: 31 - 00260/00547 - Loss: 0.04241. [ 41 s]\n",
      "Epoch: 31 - 00280/00547 - Loss: 0.04792. [ 44 s]\n",
      "Epoch: 31 - 00300/00547 - Loss: 0.03244. [ 47 s]\n",
      "Epoch: 31 - 00320/00547 - Loss: 0.04389. [ 50 s]\n",
      "Epoch: 31 - 00340/00547 - Loss: 0.04681. [ 54 s]\n",
      "Epoch: 31 - 00360/00547 - Loss: 0.03080. [ 57 s]\n",
      "Epoch: 31 - 00380/00547 - Loss: 0.03354. [ 60 s]\n",
      "Epoch: 31 - 00400/00547 - Loss: 0.03151. [ 63 s]\n",
      "Epoch: 31 - 00420/00547 - Loss: 0.05431. [ 66 s]\n",
      "Epoch: 31 - 00440/00547 - Loss: 0.03942. [ 69 s]\n",
      "Epoch: 31 - 00460/00547 - Loss: 0.02039. [ 73 s]\n",
      "Epoch: 31 - 00480/00547 - Loss: 0.03724. [ 76 s]\n",
      "Epoch: 31 - 00500/00547 - Loss: 0.03873. [ 79 s]\n",
      "Epoch: 31 - 00520/00547 - Loss: 0.02489. [ 82 s]\n",
      "Epoch: 31 - 00540/00547 - Loss: 0.03927. [ 85 s]\n",
      "Epoch: 31 - loss(trn/val):0.03425/0.33103, acc(val):91.18%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 32 - 00020/00547 - Loss: 0.02256. [  3 s]\n",
      "Epoch: 32 - 00040/00547 - Loss: 0.02580. [  6 s]\n",
      "Epoch: 32 - 00060/00547 - Loss: 0.08215. [  9 s]\n",
      "Epoch: 32 - 00080/00547 - Loss: 0.03443. [ 12 s]\n",
      "Epoch: 32 - 00100/00547 - Loss: 0.03802. [ 15 s]\n",
      "Epoch: 32 - 00120/00547 - Loss: 0.02103. [ 19 s]\n",
      "Epoch: 32 - 00140/00547 - Loss: 0.05617. [ 22 s]\n",
      "Epoch: 32 - 00160/00547 - Loss: 0.01619. [ 25 s]\n",
      "Epoch: 32 - 00180/00547 - Loss: 0.01742. [ 28 s]\n",
      "Epoch: 32 - 00200/00547 - Loss: 0.03270. [ 31 s]\n",
      "Epoch: 32 - 00220/00547 - Loss: 0.07444. [ 34 s]\n",
      "Epoch: 32 - 00240/00547 - Loss: 0.04814. [ 37 s]\n",
      "Epoch: 32 - 00260/00547 - Loss: 0.02957. [ 40 s]\n",
      "Epoch: 32 - 00280/00547 - Loss: 0.03219. [ 43 s]\n",
      "Epoch: 32 - 00300/00547 - Loss: 0.03775. [ 47 s]\n",
      "Epoch: 32 - 00320/00547 - Loss: 0.05304. [ 50 s]\n",
      "Epoch: 32 - 00340/00547 - Loss: 0.05529. [ 53 s]\n",
      "Epoch: 32 - 00360/00547 - Loss: 0.11296. [ 56 s]\n",
      "Epoch: 32 - 00380/00547 - Loss: 0.04800. [ 59 s]\n",
      "Epoch: 32 - 00400/00547 - Loss: 0.04489. [ 62 s]\n",
      "Epoch: 32 - 00420/00547 - Loss: 0.01261. [ 65 s]\n",
      "Epoch: 32 - 00440/00547 - Loss: 0.04587. [ 69 s]\n",
      "Epoch: 32 - 00460/00547 - Loss: 0.03584. [ 72 s]\n",
      "Epoch: 32 - 00480/00547 - Loss: 0.02524. [ 75 s]\n",
      "Epoch: 32 - 00500/00547 - Loss: 0.05149. [ 78 s]\n",
      "Epoch: 32 - 00520/00547 - Loss: 0.05029. [ 81 s]\n",
      "Epoch: 32 - 00540/00547 - Loss: 0.04018. [ 84 s]\n",
      "Epoch: 32 - loss(trn/val):0.03651/0.36341, acc(val):90.43%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 33 - 00020/00547 - Loss: 0.02743. [  3 s]\n",
      "Epoch: 33 - 00040/00547 - Loss: 0.03147. [  6 s]\n",
      "Epoch: 33 - 00060/00547 - Loss: 0.03435. [  9 s]\n",
      "Epoch: 33 - 00080/00547 - Loss: 0.04392. [ 12 s]\n",
      "Epoch: 33 - 00100/00547 - Loss: 0.02640. [ 15 s]\n",
      "Epoch: 33 - 00120/00547 - Loss: 0.03587. [ 19 s]\n",
      "Epoch: 33 - 00140/00547 - Loss: 0.02788. [ 22 s]\n",
      "Epoch: 33 - 00160/00547 - Loss: 0.05630. [ 25 s]\n",
      "Epoch: 33 - 00180/00547 - Loss: 0.01769. [ 28 s]\n",
      "Epoch: 33 - 00200/00547 - Loss: 0.03015. [ 31 s]\n",
      "Epoch: 33 - 00220/00547 - Loss: 0.02090. [ 34 s]\n",
      "Epoch: 33 - 00240/00547 - Loss: 0.01988. [ 37 s]\n",
      "Epoch: 33 - 00260/00547 - Loss: 0.03870. [ 40 s]\n",
      "Epoch: 33 - 00280/00547 - Loss: 0.04211. [ 43 s]\n",
      "Epoch: 33 - 00300/00547 - Loss: 0.03393. [ 47 s]\n",
      "Epoch: 33 - 00320/00547 - Loss: 0.45381. [ 50 s]\n",
      "Epoch: 33 - 00340/00547 - Loss: 0.03233. [ 53 s]\n",
      "Epoch: 33 - 00360/00547 - Loss: 0.04427. [ 56 s]\n",
      "Epoch: 33 - 00380/00547 - Loss: 0.09522. [ 59 s]\n",
      "Epoch: 33 - 00400/00547 - Loss: 0.03112. [ 63 s]\n",
      "Epoch: 33 - 00420/00547 - Loss: 0.05673. [ 66 s]\n",
      "Epoch: 33 - 00440/00547 - Loss: 0.04430. [ 69 s]\n",
      "Epoch: 33 - 00460/00547 - Loss: 0.01472. [ 72 s]\n",
      "Epoch: 33 - 00480/00547 - Loss: 0.03846. [ 75 s]\n",
      "Epoch: 33 - 00500/00547 - Loss: 0.04362. [ 78 s]\n",
      "Epoch: 33 - 00520/00547 - Loss: 0.05294. [ 81 s]\n",
      "Epoch: 33 - 00540/00547 - Loss: 0.04999. [ 85 s]\n",
      "Epoch: 33 - loss(trn/val):0.03796/0.30756, acc(val):91.60%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 34 - 00020/00547 - Loss: 0.01929. [  3 s]\n",
      "Epoch: 34 - 00040/00547 - Loss: 0.04622. [  6 s]\n",
      "Epoch: 34 - 00060/00547 - Loss: 0.02718. [  9 s]\n",
      "Epoch: 34 - 00080/00547 - Loss: 0.06140. [ 12 s]\n",
      "Epoch: 34 - 00100/00547 - Loss: 0.10437. [ 15 s]\n",
      "Epoch: 34 - 00120/00547 - Loss: 0.03064. [ 18 s]\n",
      "Epoch: 34 - 00140/00547 - Loss: 0.03870. [ 22 s]\n",
      "Epoch: 34 - 00160/00547 - Loss: 0.03531. [ 25 s]\n",
      "Epoch: 34 - 00180/00547 - Loss: 0.03602. [ 28 s]\n",
      "Epoch: 34 - 00200/00547 - Loss: 0.03988. [ 31 s]\n",
      "Epoch: 34 - 00220/00547 - Loss: 0.04082. [ 34 s]\n",
      "Epoch: 34 - 00240/00547 - Loss: 0.03286. [ 37 s]\n",
      "Epoch: 34 - 00260/00547 - Loss: 0.02477. [ 40 s]\n",
      "Epoch: 34 - 00280/00547 - Loss: 0.03094. [ 43 s]\n",
      "Epoch: 34 - 00300/00547 - Loss: 0.03337. [ 46 s]\n",
      "Epoch: 34 - 00320/00547 - Loss: 0.02662. [ 50 s]\n",
      "Epoch: 34 - 00340/00547 - Loss: 0.05311. [ 53 s]\n",
      "Epoch: 34 - 00360/00547 - Loss: 0.02071. [ 56 s]\n",
      "Epoch: 34 - 00380/00547 - Loss: 0.02633. [ 59 s]\n",
      "Epoch: 34 - 00400/00547 - Loss: 0.03762. [ 62 s]\n",
      "Epoch: 34 - 00420/00547 - Loss: 0.07423. [ 65 s]\n",
      "Epoch: 34 - 00440/00547 - Loss: 0.04098. [ 69 s]\n",
      "Epoch: 34 - 00460/00547 - Loss: 0.02761. [ 72 s]\n",
      "Epoch: 34 - 00480/00547 - Loss: 0.04764. [ 75 s]\n",
      "Epoch: 34 - 00500/00547 - Loss: 0.09394. [ 78 s]\n",
      "Epoch: 34 - 00520/00547 - Loss: 0.04532. [ 81 s]\n",
      "Epoch: 34 - 00540/00547 - Loss: 0.04911. [ 84 s]\n",
      "Epoch: 34 - loss(trn/val):0.04178/0.29105, acc(val):90.98%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 35 - 00020/00547 - Loss: 0.04675. [  3 s]\n",
      "Epoch: 35 - 00040/00547 - Loss: 0.02844. [  6 s]\n",
      "Epoch: 35 - 00060/00547 - Loss: 0.02874. [  9 s]\n",
      "Epoch: 35 - 00080/00547 - Loss: 0.03180. [ 12 s]\n",
      "Epoch: 35 - 00100/00547 - Loss: 0.02526. [ 15 s]\n",
      "Epoch: 35 - 00120/00547 - Loss: 0.02228. [ 18 s]\n",
      "Epoch: 35 - 00140/00547 - Loss: 0.02595. [ 22 s]\n",
      "Epoch: 35 - 00160/00547 - Loss: 0.04409. [ 25 s]\n",
      "Epoch: 35 - 00180/00547 - Loss: 0.02357. [ 28 s]\n",
      "Epoch: 35 - 00200/00547 - Loss: 0.01862. [ 31 s]\n",
      "Epoch: 35 - 00220/00547 - Loss: 0.03182. [ 34 s]\n",
      "Epoch: 35 - 00240/00547 - Loss: 0.03233. [ 37 s]\n",
      "Epoch: 35 - 00260/00547 - Loss: 0.02427. [ 40 s]\n",
      "Epoch: 35 - 00280/00547 - Loss: 0.04167. [ 43 s]\n",
      "Epoch: 35 - 00300/00547 - Loss: 0.04162. [ 47 s]\n",
      "Epoch: 35 - 00320/00547 - Loss: 0.02798. [ 50 s]\n",
      "Epoch: 35 - 00340/00547 - Loss: 0.03061. [ 53 s]\n",
      "Epoch: 35 - 00360/00547 - Loss: 0.02344. [ 56 s]\n",
      "Epoch: 35 - 00380/00547 - Loss: 0.04535. [ 59 s]\n",
      "Epoch: 35 - 00400/00547 - Loss: 0.02320. [ 63 s]\n",
      "Epoch: 35 - 00420/00547 - Loss: 0.02650. [ 66 s]\n",
      "Epoch: 35 - 00440/00547 - Loss: 0.04912. [ 69 s]\n",
      "Epoch: 35 - 00460/00547 - Loss: 0.02851. [ 72 s]\n",
      "Epoch: 35 - 00480/00547 - Loss: 0.04830. [ 75 s]\n",
      "Epoch: 35 - 00500/00547 - Loss: 0.03713. [ 78 s]\n",
      "Epoch: 35 - 00520/00547 - Loss: 0.04255. [ 81 s]\n",
      "Epoch: 35 - 00540/00547 - Loss: 0.05820. [ 84 s]\n",
      "Epoch: 35 - loss(trn/val):0.03713/0.28736, acc(val):92.05%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 36 - 00020/00547 - Loss: 0.02651. [  3 s]\n",
      "Epoch: 36 - 00040/00547 - Loss: 0.04891. [  6 s]\n",
      "Epoch: 36 - 00060/00547 - Loss: 0.03851. [  9 s]\n",
      "Epoch: 36 - 00080/00547 - Loss: 0.03924. [ 12 s]\n",
      "Epoch: 36 - 00100/00547 - Loss: 0.02983. [ 15 s]\n",
      "Epoch: 36 - 00120/00547 - Loss: 0.04336. [ 18 s]\n",
      "Epoch: 36 - 00140/00547 - Loss: 0.05512. [ 22 s]\n",
      "Epoch: 36 - 00160/00547 - Loss: 0.04266. [ 25 s]\n",
      "Epoch: 36 - 00180/00547 - Loss: 0.04694. [ 28 s]\n",
      "Epoch: 36 - 00200/00547 - Loss: 0.04626. [ 31 s]\n",
      "Epoch: 36 - 00220/00547 - Loss: 0.03305. [ 34 s]\n",
      "Epoch: 36 - 00240/00547 - Loss: 0.04967. [ 37 s]\n",
      "Epoch: 36 - 00260/00547 - Loss: 0.02820. [ 40 s]\n",
      "Epoch: 36 - 00280/00547 - Loss: 0.01899. [ 43 s]\n",
      "Epoch: 36 - 00300/00547 - Loss: 0.01764. [ 46 s]\n",
      "Epoch: 36 - 00320/00547 - Loss: 0.02713. [ 49 s]\n",
      "Epoch: 36 - 00340/00547 - Loss: 0.03321. [ 52 s]\n",
      "Epoch: 36 - 00360/00547 - Loss: 0.04439. [ 56 s]\n",
      "Epoch: 36 - 00380/00547 - Loss: 0.05521. [ 59 s]\n",
      "Epoch: 36 - 00400/00547 - Loss: 0.04878. [ 62 s]\n",
      "Epoch: 36 - 00420/00547 - Loss: 0.01849. [ 65 s]\n",
      "Epoch: 36 - 00440/00547 - Loss: 0.02779. [ 68 s]\n",
      "Epoch: 36 - 00460/00547 - Loss: 0.05452. [ 71 s]\n",
      "Epoch: 36 - 00480/00547 - Loss: 0.03442. [ 74 s]\n",
      "Epoch: 36 - 00500/00547 - Loss: 0.03917. [ 77 s]\n",
      "Epoch: 36 - 00520/00547 - Loss: 0.06532. [ 81 s]\n",
      "Epoch: 36 - 00540/00547 - Loss: 0.09168. [ 84 s]\n",
      "Epoch: 36 - loss(trn/val):0.03789/0.23795, acc(val):92.85%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 37 - 00020/00547 - Loss: 0.02917. [  3 s]\n",
      "Epoch: 37 - 00040/00547 - Loss: 0.04351. [  6 s]\n",
      "Epoch: 37 - 00060/00547 - Loss: 0.02520. [  9 s]\n",
      "Epoch: 37 - 00080/00547 - Loss: 0.03260. [ 12 s]\n",
      "Epoch: 37 - 00100/00547 - Loss: 0.02419. [ 15 s]\n",
      "Epoch: 37 - 00120/00547 - Loss: 0.03262. [ 18 s]\n",
      "Epoch: 37 - 00140/00547 - Loss: 0.04542. [ 21 s]\n",
      "Epoch: 37 - 00160/00547 - Loss: 0.05680. [ 25 s]\n",
      "Epoch: 37 - 00180/00547 - Loss: 0.02458. [ 28 s]\n",
      "Epoch: 37 - 00200/00547 - Loss: 0.05589. [ 31 s]\n",
      "Epoch: 37 - 00220/00547 - Loss: 0.33361. [ 34 s]\n",
      "Epoch: 37 - 00240/00547 - Loss: 0.05871. [ 37 s]\n",
      "Epoch: 37 - 00260/00547 - Loss: 0.04521. [ 40 s]\n",
      "Epoch: 37 - 00280/00547 - Loss: 0.02810. [ 43 s]\n",
      "Epoch: 37 - 00300/00547 - Loss: 0.04148. [ 46 s]\n",
      "Epoch: 37 - 00320/00547 - Loss: 0.05064. [ 49 s]\n",
      "Epoch: 37 - 00340/00547 - Loss: 0.03499. [ 52 s]\n",
      "Epoch: 37 - 00360/00547 - Loss: 0.03589. [ 55 s]\n",
      "Epoch: 37 - 00380/00547 - Loss: 0.04201. [ 59 s]\n",
      "Epoch: 37 - 00400/00547 - Loss: 0.03593. [ 62 s]\n",
      "Epoch: 37 - 00420/00547 - Loss: 0.03729. [ 65 s]\n",
      "Epoch: 37 - 00440/00547 - Loss: 0.08526. [ 68 s]\n",
      "Epoch: 37 - 00460/00547 - Loss: 0.03409. [ 71 s]\n",
      "Epoch: 37 - 00480/00547 - Loss: 0.02741. [ 74 s]\n",
      "Epoch: 37 - 00500/00547 - Loss: 0.02936. [ 77 s]\n",
      "Epoch: 37 - 00520/00547 - Loss: 0.06719. [ 80 s]\n",
      "Epoch: 37 - 00540/00547 - Loss: 0.04937. [ 83 s]\n",
      "Epoch: 37 - loss(trn/val):0.03349/0.38556, acc(val):90.48%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 38 - 00020/00547 - Loss: 0.02678. [  3 s]\n",
      "Epoch: 38 - 00040/00547 - Loss: 0.03735. [  6 s]\n",
      "Epoch: 38 - 00060/00547 - Loss: 0.03535. [  9 s]\n",
      "Epoch: 38 - 00080/00547 - Loss: 0.04002. [ 12 s]\n",
      "Epoch: 38 - 00100/00547 - Loss: 0.02355. [ 15 s]\n",
      "Epoch: 38 - 00120/00547 - Loss: 0.02159. [ 18 s]\n",
      "Epoch: 38 - 00140/00547 - Loss: 0.02104. [ 21 s]\n",
      "Epoch: 38 - 00160/00547 - Loss: 0.04636. [ 24 s]\n",
      "Epoch: 38 - 00180/00547 - Loss: 0.02525. [ 28 s]\n",
      "Epoch: 38 - 00200/00547 - Loss: 0.10389. [ 31 s]\n",
      "Epoch: 38 - 00220/00547 - Loss: 0.04282. [ 34 s]\n",
      "Epoch: 38 - 00240/00547 - Loss: 0.01705. [ 37 s]\n",
      "Epoch: 38 - 00260/00547 - Loss: 0.05891. [ 40 s]\n",
      "Epoch: 38 - 00280/00547 - Loss: 0.04726. [ 43 s]\n",
      "Epoch: 38 - 00300/00547 - Loss: 0.02108. [ 46 s]\n",
      "Epoch: 38 - 00320/00547 - Loss: 0.05157. [ 49 s]\n",
      "Epoch: 38 - 00340/00547 - Loss: 0.04055. [ 52 s]\n",
      "Epoch: 38 - 00360/00547 - Loss: 0.02259. [ 55 s]\n",
      "Epoch: 38 - 00380/00547 - Loss: 0.02300. [ 59 s]\n",
      "Epoch: 38 - 00400/00547 - Loss: 0.02429. [ 62 s]\n",
      "Epoch: 38 - 00420/00547 - Loss: 0.01884. [ 65 s]\n",
      "Epoch: 38 - 00440/00547 - Loss: 0.04645. [ 68 s]\n",
      "Epoch: 38 - 00460/00547 - Loss: 0.05005. [ 71 s]\n",
      "Epoch: 38 - 00480/00547 - Loss: 0.01756. [ 74 s]\n",
      "Epoch: 38 - 00500/00547 - Loss: 0.02763. [ 77 s]\n",
      "Epoch: 38 - 00520/00547 - Loss: 0.04011. [ 80 s]\n",
      "Epoch: 38 - 00540/00547 - Loss: 0.03156. [ 83 s]\n",
      "Epoch: 38 - loss(trn/val):0.03188/0.38064, acc(val):90.61%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 39 - 00020/00547 - Loss: 0.03134. [  3 s]\n",
      "Epoch: 39 - 00040/00547 - Loss: 0.01677. [  6 s]\n",
      "Epoch: 39 - 00060/00547 - Loss: 0.02850. [  9 s]\n",
      "Epoch: 39 - 00080/00547 - Loss: 0.02993. [ 12 s]\n",
      "Epoch: 39 - 00100/00547 - Loss: 0.02746. [ 15 s]\n",
      "Epoch: 39 - 00120/00547 - Loss: 0.01405. [ 18 s]\n",
      "Epoch: 39 - 00140/00547 - Loss: 0.03203. [ 21 s]\n",
      "Epoch: 39 - 00160/00547 - Loss: 0.02768. [ 24 s]\n",
      "Epoch: 39 - 00180/00547 - Loss: 0.03626. [ 27 s]\n",
      "Epoch: 39 - 00200/00547 - Loss: 0.04419. [ 31 s]\n",
      "Epoch: 39 - 00220/00547 - Loss: 0.04853. [ 34 s]\n",
      "Epoch: 39 - 00240/00547 - Loss: 0.06409. [ 37 s]\n",
      "Epoch: 39 - 00260/00547 - Loss: 0.11701. [ 40 s]\n",
      "Epoch: 39 - 00280/00547 - Loss: 0.05162. [ 43 s]\n",
      "Epoch: 39 - 00300/00547 - Loss: 0.02847. [ 46 s]\n",
      "Epoch: 39 - 00320/00547 - Loss: 0.04436. [ 49 s]\n",
      "Epoch: 39 - 00340/00547 - Loss: 0.02320. [ 52 s]\n",
      "Epoch: 39 - 00360/00547 - Loss: 0.02084. [ 55 s]\n",
      "Epoch: 39 - 00380/00547 - Loss: 0.03134. [ 58 s]\n",
      "Epoch: 39 - 00400/00547 - Loss: 0.02689. [ 61 s]\n",
      "Epoch: 39 - 00420/00547 - Loss: 0.04896. [ 65 s]\n",
      "Epoch: 39 - 00440/00547 - Loss: 0.04344. [ 68 s]\n",
      "Epoch: 39 - 00460/00547 - Loss: 0.03820. [ 71 s]\n",
      "Epoch: 39 - 00480/00547 - Loss: 0.02285. [ 74 s]\n",
      "Epoch: 39 - 00500/00547 - Loss: 0.04641. [ 77 s]\n",
      "Epoch: 39 - 00520/00547 - Loss: 0.05122. [ 80 s]\n",
      "Epoch: 39 - 00540/00547 - Loss: 0.04549. [ 83 s]\n",
      "Epoch: 39 - loss(trn/val):0.03668/0.43715, acc(val):89.78%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 40 - 00020/00547 - Loss: 0.03839. [  3 s]\n",
      "Epoch: 40 - 00040/00547 - Loss: 0.03866. [  6 s]\n",
      "Epoch: 40 - 00060/00547 - Loss: 0.02812. [  9 s]\n",
      "Epoch: 40 - 00080/00547 - Loss: 0.07320. [ 12 s]\n",
      "Epoch: 40 - 00100/00547 - Loss: 0.04921. [ 15 s]\n",
      "Epoch: 40 - 00120/00547 - Loss: 0.03017. [ 18 s]\n",
      "Epoch: 40 - 00140/00547 - Loss: 0.03343. [ 21 s]\n",
      "Epoch: 40 - 00160/00547 - Loss: 0.04394. [ 24 s]\n",
      "Epoch: 40 - 00180/00547 - Loss: 0.02191. [ 28 s]\n",
      "Epoch: 40 - 00200/00547 - Loss: 0.03746. [ 31 s]\n",
      "Epoch: 40 - 00220/00547 - Loss: 0.02200. [ 34 s]\n",
      "Epoch: 40 - 00240/00547 - Loss: 0.03158. [ 37 s]\n",
      "Epoch: 40 - 00260/00547 - Loss: 0.02637. [ 40 s]\n",
      "Epoch: 40 - 00280/00547 - Loss: 0.01834. [ 43 s]\n",
      "Epoch: 40 - 00300/00547 - Loss: 0.02412. [ 46 s]\n",
      "Epoch: 40 - 00320/00547 - Loss: 0.02560. [ 50 s]\n",
      "Epoch: 40 - 00340/00547 - Loss: 0.02238. [ 53 s]\n",
      "Epoch: 40 - 00360/00547 - Loss: 0.03609. [ 56 s]\n",
      "Epoch: 40 - 00380/00547 - Loss: 0.02667. [ 59 s]\n",
      "Epoch: 40 - 00400/00547 - Loss: 0.02736. [ 62 s]\n",
      "Epoch: 40 - 00420/00547 - Loss: 0.02407. [ 65 s]\n",
      "Epoch: 40 - 00440/00547 - Loss: 0.03143. [ 68 s]\n",
      "Epoch: 40 - 00460/00547 - Loss: 0.09805. [ 71 s]\n",
      "Epoch: 40 - 00480/00547 - Loss: 0.03195. [ 75 s]\n",
      "Epoch: 40 - 00500/00547 - Loss: 0.09548. [ 78 s]\n",
      "Epoch: 40 - 00520/00547 - Loss: 0.05312. [ 81 s]\n",
      "Epoch: 40 - 00540/00547 - Loss: 0.06925. [ 84 s]\n",
      "Epoch: 40 - loss(trn/val):0.05262/0.25540, acc(val):92.60%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 41 - 00020/00547 - Loss: 0.05853. [  3 s]\n",
      "Epoch: 41 - 00040/00547 - Loss: 0.05171. [  6 s]\n",
      "Epoch: 41 - 00060/00547 - Loss: 0.02352. [  9 s]\n",
      "Epoch: 41 - 00080/00547 - Loss: 0.04066. [ 12 s]\n",
      "Epoch: 41 - 00100/00547 - Loss: 0.02778. [ 15 s]\n",
      "Epoch: 41 - 00120/00547 - Loss: 0.02541. [ 19 s]\n",
      "Epoch: 41 - 00140/00547 - Loss: 0.02740. [ 22 s]\n",
      "Epoch: 41 - 00160/00547 - Loss: 0.02190. [ 25 s]\n",
      "Epoch: 41 - 00180/00547 - Loss: 0.03175. [ 28 s]\n",
      "Epoch: 41 - 00200/00547 - Loss: 0.02830. [ 31 s]\n",
      "Epoch: 41 - 00220/00547 - Loss: 0.03080. [ 34 s]\n",
      "Epoch: 41 - 00240/00547 - Loss: 0.03313. [ 37 s]\n",
      "Epoch: 41 - 00260/00547 - Loss: 0.03730. [ 40 s]\n",
      "Epoch: 41 - 00280/00547 - Loss: 0.02175. [ 43 s]\n",
      "Epoch: 41 - 00300/00547 - Loss: 0.03517. [ 46 s]\n",
      "Epoch: 41 - 00320/00547 - Loss: 0.02340. [ 49 s]\n",
      "Epoch: 41 - 00340/00547 - Loss: 0.02601. [ 52 s]\n",
      "Epoch: 41 - 00360/00547 - Loss: 0.06699. [ 55 s]\n",
      "Epoch: 41 - 00380/00547 - Loss: 0.02861. [ 58 s]\n",
      "Epoch: 41 - 00400/00547 - Loss: 0.02655. [ 61 s]\n",
      "Epoch: 41 - 00420/00547 - Loss: 0.06204. [ 65 s]\n",
      "Epoch: 41 - 00440/00547 - Loss: 0.02350. [ 68 s]\n",
      "Epoch: 41 - 00460/00547 - Loss: 0.02428. [ 71 s]\n",
      "Epoch: 41 - 00480/00547 - Loss: 0.04889. [ 74 s]\n",
      "Epoch: 41 - 00500/00547 - Loss: 0.03120. [ 77 s]\n",
      "Epoch: 41 - 00520/00547 - Loss: 0.04022. [ 80 s]\n",
      "Epoch: 41 - 00540/00547 - Loss: 0.03056. [ 83 s]\n",
      "Epoch: 41 - loss(trn/val):0.03291/0.32728, acc(val):91.45%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 42 - 00020/00547 - Loss: 0.02664. [  3 s]\n",
      "Epoch: 42 - 00040/00547 - Loss: 0.02978. [  6 s]\n",
      "Epoch: 42 - 00060/00547 - Loss: 0.02674. [  9 s]\n",
      "Epoch: 42 - 00080/00547 - Loss: 0.03665. [ 12 s]\n",
      "Epoch: 42 - 00100/00547 - Loss: 0.03775. [ 15 s]\n",
      "Epoch: 42 - 00120/00547 - Loss: 0.03701. [ 18 s]\n",
      "Epoch: 42 - 00140/00547 - Loss: 0.03642. [ 21 s]\n",
      "Epoch: 42 - 00160/00547 - Loss: 0.03523. [ 25 s]\n",
      "Epoch: 42 - 00180/00547 - Loss: 0.05697. [ 28 s]\n",
      "Epoch: 42 - 00200/00547 - Loss: 0.03369. [ 31 s]\n",
      "Epoch: 42 - 00220/00547 - Loss: 0.11528. [ 34 s]\n",
      "Epoch: 42 - 00240/00547 - Loss: 0.05397. [ 37 s]\n",
      "Epoch: 42 - 00260/00547 - Loss: 0.03237. [ 40 s]\n",
      "Epoch: 42 - 00280/00547 - Loss: 0.03427. [ 43 s]\n",
      "Epoch: 42 - 00300/00547 - Loss: 0.02381. [ 46 s]\n",
      "Epoch: 42 - 00320/00547 - Loss: 0.02506. [ 49 s]\n",
      "Epoch: 42 - 00340/00547 - Loss: 0.02676. [ 53 s]\n",
      "Epoch: 42 - 00360/00547 - Loss: 0.04037. [ 56 s]\n",
      "Epoch: 42 - 00380/00547 - Loss: 0.03227. [ 59 s]\n",
      "Epoch: 42 - 00400/00547 - Loss: 0.04148. [ 62 s]\n",
      "Epoch: 42 - 00420/00547 - Loss: 0.03325. [ 65 s]\n",
      "Epoch: 42 - 00440/00547 - Loss: 0.02018. [ 68 s]\n",
      "Epoch: 42 - 00460/00547 - Loss: 0.02521. [ 71 s]\n",
      "Epoch: 42 - 00480/00547 - Loss: 0.02180. [ 74 s]\n",
      "Epoch: 42 - 00500/00547 - Loss: 0.02519. [ 77 s]\n",
      "Epoch: 42 - 00520/00547 - Loss: 0.03630. [ 80 s]\n",
      "Epoch: 42 - 00540/00547 - Loss: 0.03302. [ 83 s]\n",
      "Epoch: 42 - loss(trn/val):0.04313/0.39713, acc(val):90.25%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 43 - 00020/00547 - Loss: 0.02874. [  3 s]\n",
      "Epoch: 43 - 00040/00547 - Loss: 0.04079. [  6 s]\n",
      "Epoch: 43 - 00060/00547 - Loss: 0.03538. [  9 s]\n",
      "Epoch: 43 - 00080/00547 - Loss: 0.02890. [ 12 s]\n",
      "Epoch: 43 - 00100/00547 - Loss: 0.03813. [ 15 s]\n",
      "Epoch: 43 - 00120/00547 - Loss: 0.04058. [ 18 s]\n",
      "Epoch: 43 - 00140/00547 - Loss: 0.03328. [ 21 s]\n",
      "Epoch: 43 - 00160/00547 - Loss: 0.01878. [ 24 s]\n",
      "Epoch: 43 - 00180/00547 - Loss: 0.03089. [ 28 s]\n",
      "Epoch: 43 - 00200/00547 - Loss: 0.01858. [ 31 s]\n",
      "Epoch: 43 - 00220/00547 - Loss: 0.02438. [ 34 s]\n",
      "Epoch: 43 - 00240/00547 - Loss: 0.03453. [ 37 s]\n",
      "Epoch: 43 - 00260/00547 - Loss: 0.03222. [ 40 s]\n",
      "Epoch: 43 - 00280/00547 - Loss: 0.04755. [ 43 s]\n",
      "Epoch: 43 - 00300/00547 - Loss: 0.02485. [ 46 s]\n",
      "Epoch: 43 - 00320/00547 - Loss: 0.02453. [ 49 s]\n",
      "Epoch: 43 - 00340/00547 - Loss: 0.02575. [ 52 s]\n",
      "Epoch: 43 - 00360/00547 - Loss: 0.03493. [ 55 s]\n",
      "Epoch: 43 - 00380/00547 - Loss: 0.03218. [ 58 s]\n",
      "Epoch: 43 - 00400/00547 - Loss: 0.02945. [ 61 s]\n",
      "Epoch: 43 - 00420/00547 - Loss: 0.03364. [ 65 s]\n",
      "Epoch: 43 - 00440/00547 - Loss: 0.03732. [ 68 s]\n",
      "Epoch: 43 - 00460/00547 - Loss: 0.03627. [ 71 s]\n",
      "Epoch: 43 - 00480/00547 - Loss: 0.02434. [ 74 s]\n",
      "Epoch: 43 - 00500/00547 - Loss: 0.05338. [ 77 s]\n",
      "Epoch: 43 - 00520/00547 - Loss: 0.02189. [ 80 s]\n",
      "Epoch: 43 - 00540/00547 - Loss: 0.01768. [ 83 s]\n",
      "Epoch: 43 - loss(trn/val):0.02916/0.40037, acc(val):91.16%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 44 - 00020/00547 - Loss: 0.04802. [  3 s]\n",
      "Epoch: 44 - 00040/00547 - Loss: 0.19056. [  6 s]\n",
      "Epoch: 44 - 00060/00547 - Loss: 0.04361. [  9 s]\n",
      "Epoch: 44 - 00080/00547 - Loss: 0.03895. [ 12 s]\n",
      "Epoch: 44 - 00100/00547 - Loss: 0.03504. [ 15 s]\n",
      "Epoch: 44 - 00120/00547 - Loss: 0.02709. [ 18 s]\n",
      "Epoch: 44 - 00140/00547 - Loss: 0.03414. [ 21 s]\n",
      "Epoch: 44 - 00160/00547 - Loss: 0.04452. [ 24 s]\n",
      "Epoch: 44 - 00180/00547 - Loss: 0.03511. [ 27 s]\n",
      "Epoch: 44 - 00200/00547 - Loss: 0.03465. [ 30 s]\n",
      "Epoch: 44 - 00220/00547 - Loss: 0.03225. [ 34 s]\n",
      "Epoch: 44 - 00240/00547 - Loss: 0.07049. [ 37 s]\n",
      "Epoch: 44 - 00260/00547 - Loss: 0.02311. [ 40 s]\n",
      "Epoch: 44 - 00280/00547 - Loss: 0.03941. [ 43 s]\n",
      "Epoch: 44 - 00300/00547 - Loss: 0.03311. [ 46 s]\n",
      "Epoch: 44 - 00320/00547 - Loss: 0.02299. [ 49 s]\n",
      "Epoch: 44 - 00340/00547 - Loss: 0.04245. [ 52 s]\n",
      "Epoch: 44 - 00360/00547 - Loss: 0.03554. [ 55 s]\n",
      "Epoch: 44 - 00380/00547 - Loss: 0.04607. [ 58 s]\n",
      "Epoch: 44 - 00400/00547 - Loss: 0.03455. [ 62 s]\n",
      "Epoch: 44 - 00420/00547 - Loss: 0.06248. [ 65 s]\n",
      "Epoch: 44 - 00440/00547 - Loss: 0.03178. [ 68 s]\n",
      "Epoch: 44 - 00460/00547 - Loss: 0.02546. [ 71 s]\n",
      "Epoch: 44 - 00480/00547 - Loss: 0.03049. [ 74 s]\n",
      "Epoch: 44 - 00500/00547 - Loss: 0.02109. [ 77 s]\n",
      "Epoch: 44 - 00520/00547 - Loss: 0.03611. [ 80 s]\n",
      "Epoch: 44 - 00540/00547 - Loss: 0.01875. [ 83 s]\n",
      "Epoch: 44 - loss(trn/val):0.03024/0.39187, acc(val):91.01%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 45 - 00020/00547 - Loss: 0.02357. [  3 s]\n",
      "Epoch: 45 - 00040/00547 - Loss: 0.02465. [  6 s]\n",
      "Epoch: 45 - 00060/00547 - Loss: 0.02473. [  9 s]\n",
      "Epoch: 45 - 00080/00547 - Loss: 0.01648. [ 12 s]\n",
      "Epoch: 45 - 00100/00547 - Loss: 0.03252. [ 15 s]\n",
      "Epoch: 45 - 00120/00547 - Loss: 0.04089. [ 18 s]\n",
      "Epoch: 45 - 00140/00547 - Loss: 0.03931. [ 21 s]\n",
      "Epoch: 45 - 00160/00547 - Loss: 0.03021. [ 24 s]\n",
      "Epoch: 45 - 00180/00547 - Loss: 0.02036. [ 28 s]\n",
      "Epoch: 45 - 00200/00547 - Loss: 0.16204. [ 31 s]\n",
      "Epoch: 45 - 00220/00547 - Loss: 0.14277. [ 34 s]\n",
      "Epoch: 45 - 00240/00547 - Loss: 0.03494. [ 37 s]\n",
      "Epoch: 45 - 00260/00547 - Loss: 0.03289. [ 40 s]\n",
      "Epoch: 45 - 00280/00547 - Loss: 0.02621. [ 43 s]\n",
      "Epoch: 45 - 00300/00547 - Loss: 0.02529. [ 46 s]\n",
      "Epoch: 45 - 00320/00547 - Loss: 0.04129. [ 49 s]\n",
      "Epoch: 45 - 00340/00547 - Loss: 0.05918. [ 53 s]\n",
      "Epoch: 45 - 00360/00547 - Loss: 0.03567. [ 56 s]\n",
      "Epoch: 45 - 00380/00547 - Loss: 0.02369. [ 59 s]\n",
      "Epoch: 45 - 00400/00547 - Loss: 0.01855. [ 62 s]\n",
      "Epoch: 45 - 00420/00547 - Loss: 0.03032. [ 65 s]\n",
      "Epoch: 45 - 00440/00547 - Loss: 0.02155. [ 68 s]\n",
      "Epoch: 45 - 00460/00547 - Loss: 0.01911. [ 71 s]\n",
      "Epoch: 45 - 00480/00547 - Loss: 0.01570. [ 74 s]\n",
      "Epoch: 45 - 00500/00547 - Loss: 0.01683. [ 77 s]\n",
      "Epoch: 45 - 00520/00547 - Loss: 0.02324. [ 81 s]\n",
      "Epoch: 45 - 00540/00547 - Loss: 0.03204. [ 84 s]\n",
      "Epoch: 45 - loss(trn/val):0.02743/0.38413, acc(val):91.65%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 46 - 00020/00547 - Loss: 0.03132. [  3 s]\n",
      "Epoch: 46 - 00040/00547 - Loss: 0.01410. [  6 s]\n",
      "Epoch: 46 - 00060/00547 - Loss: 0.01729. [  9 s]\n",
      "Epoch: 46 - 00080/00547 - Loss: 0.02918. [ 12 s]\n",
      "Epoch: 46 - 00100/00547 - Loss: 0.02587. [ 15 s]\n",
      "Epoch: 46 - 00120/00547 - Loss: 0.03311. [ 18 s]\n",
      "Epoch: 46 - 00140/00547 - Loss: 0.03210. [ 22 s]\n",
      "Epoch: 46 - 00160/00547 - Loss: 0.02158. [ 25 s]\n",
      "Epoch: 46 - 00180/00547 - Loss: 0.01928. [ 28 s]\n",
      "Epoch: 46 - 00200/00547 - Loss: 0.04409. [ 31 s]\n",
      "Epoch: 46 - 00220/00547 - Loss: 0.04821. [ 34 s]\n",
      "Epoch: 46 - 00240/00547 - Loss: 0.02623. [ 37 s]\n",
      "Epoch: 46 - 00260/00547 - Loss: 0.02569. [ 40 s]\n",
      "Epoch: 46 - 00280/00547 - Loss: 0.03720. [ 43 s]\n",
      "Epoch: 46 - 00300/00547 - Loss: 0.01093. [ 46 s]\n",
      "Epoch: 46 - 00320/00547 - Loss: 0.03853. [ 49 s]\n",
      "Epoch: 46 - 00340/00547 - Loss: 0.03027. [ 53 s]\n",
      "Epoch: 46 - 00360/00547 - Loss: 0.01598. [ 56 s]\n",
      "Epoch: 46 - 00380/00547 - Loss: 0.02022. [ 59 s]\n",
      "Epoch: 46 - 00400/00547 - Loss: 0.02749. [ 62 s]\n",
      "Epoch: 46 - 00420/00547 - Loss: 0.02200. [ 65 s]\n",
      "Epoch: 46 - 00440/00547 - Loss: 0.03312. [ 68 s]\n",
      "Epoch: 46 - 00460/00547 - Loss: 0.03673. [ 71 s]\n",
      "Epoch: 46 - 00480/00547 - Loss: 0.02052. [ 74 s]\n",
      "Epoch: 46 - 00500/00547 - Loss: 0.05306. [ 77 s]\n",
      "Epoch: 46 - 00520/00547 - Loss: 0.03814. [ 80 s]\n",
      "Epoch: 46 - 00540/00547 - Loss: 0.02088. [ 84 s]\n",
      "Epoch: 46 - loss(trn/val):0.03380/0.37496, acc(val):91.29%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 47 - 00020/00547 - Loss: 0.06105. [  3 s]\n",
      "Epoch: 47 - 00040/00547 - Loss: 0.11137. [  6 s]\n",
      "Epoch: 47 - 00060/00547 - Loss: 0.04573. [  9 s]\n",
      "Epoch: 47 - 00080/00547 - Loss: 0.05336. [ 12 s]\n",
      "Epoch: 47 - 00100/00547 - Loss: 0.08534. [ 15 s]\n",
      "Epoch: 47 - 00120/00547 - Loss: 0.03591. [ 18 s]\n",
      "Epoch: 47 - 00140/00547 - Loss: 0.02993. [ 21 s]\n",
      "Epoch: 47 - 00160/00547 - Loss: 0.01886. [ 25 s]\n",
      "Epoch: 47 - 00180/00547 - Loss: 0.01694. [ 28 s]\n",
      "Epoch: 47 - 00200/00547 - Loss: 0.03482. [ 31 s]\n",
      "Epoch: 47 - 00220/00547 - Loss: 0.02852. [ 34 s]\n",
      "Epoch: 47 - 00240/00547 - Loss: 0.03145. [ 37 s]\n",
      "Epoch: 47 - 00260/00547 - Loss: 0.02665. [ 40 s]\n",
      "Epoch: 47 - 00280/00547 - Loss: 0.03075. [ 43 s]\n",
      "Epoch: 47 - 00300/00547 - Loss: 0.08400. [ 46 s]\n",
      "Epoch: 47 - 00320/00547 - Loss: 0.01351. [ 49 s]\n",
      "Epoch: 47 - 00340/00547 - Loss: 0.02665. [ 53 s]\n",
      "Epoch: 47 - 00360/00547 - Loss: 0.01918. [ 56 s]\n",
      "Epoch: 47 - 00380/00547 - Loss: 0.03989. [ 59 s]\n",
      "Epoch: 47 - 00400/00547 - Loss: 0.03426. [ 62 s]\n",
      "Epoch: 47 - 00420/00547 - Loss: 0.03644. [ 65 s]\n",
      "Epoch: 47 - 00440/00547 - Loss: 0.02616. [ 68 s]\n",
      "Epoch: 47 - 00460/00547 - Loss: 0.02041. [ 71 s]\n",
      "Epoch: 47 - 00480/00547 - Loss: 0.04360. [ 74 s]\n",
      "Epoch: 47 - 00500/00547 - Loss: 0.02150. [ 77 s]\n",
      "Epoch: 47 - 00520/00547 - Loss: 0.06035. [ 81 s]\n",
      "Epoch: 47 - 00540/00547 - Loss: 0.01618. [ 84 s]\n",
      "Epoch: 47 - loss(trn/val):0.02764/0.32824, acc(val):92.14%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 48 - 00020/00547 - Loss: 0.01725. [  3 s]\n",
      "Epoch: 48 - 00040/00547 - Loss: 0.03177. [  6 s]\n",
      "Epoch: 48 - 00060/00547 - Loss: 0.01499. [  9 s]\n",
      "Epoch: 48 - 00080/00547 - Loss: 0.03219. [ 12 s]\n",
      "Epoch: 48 - 00100/00547 - Loss: 0.02633. [ 15 s]\n",
      "Epoch: 48 - 00120/00547 - Loss: 0.01743. [ 18 s]\n",
      "Epoch: 48 - 00140/00547 - Loss: 0.02325. [ 21 s]\n",
      "Epoch: 48 - 00160/00547 - Loss: 0.03663. [ 24 s]\n",
      "Epoch: 48 - 00180/00547 - Loss: 0.01263. [ 28 s]\n",
      "Epoch: 48 - 00200/00547 - Loss: 0.03852. [ 31 s]\n",
      "Epoch: 48 - 00220/00547 - Loss: 0.03569. [ 34 s]\n",
      "Epoch: 48 - 00240/00547 - Loss: 0.04305. [ 37 s]\n",
      "Epoch: 48 - 00260/00547 - Loss: 0.03293. [ 40 s]\n",
      "Epoch: 48 - 00280/00547 - Loss: 0.05814. [ 43 s]\n",
      "Epoch: 48 - 00300/00547 - Loss: 0.05330. [ 46 s]\n",
      "Epoch: 48 - 00320/00547 - Loss: 0.03328. [ 49 s]\n",
      "Epoch: 48 - 00340/00547 - Loss: 0.04299. [ 52 s]\n",
      "Epoch: 48 - 00360/00547 - Loss: 0.03178. [ 56 s]\n",
      "Epoch: 48 - 00380/00547 - Loss: 0.01540. [ 59 s]\n",
      "Epoch: 48 - 00400/00547 - Loss: 0.03602. [ 62 s]\n",
      "Epoch: 48 - 00420/00547 - Loss: 0.04333. [ 65 s]\n",
      "Epoch: 48 - 00440/00547 - Loss: 0.03645. [ 68 s]\n",
      "Epoch: 48 - 00460/00547 - Loss: 0.02756. [ 71 s]\n",
      "Epoch: 48 - 00480/00547 - Loss: 0.02539. [ 74 s]\n",
      "Epoch: 48 - 00500/00547 - Loss: 0.02170. [ 77 s]\n",
      "Epoch: 48 - 00520/00547 - Loss: 0.03842. [ 80 s]\n",
      "Epoch: 48 - 00540/00547 - Loss: 0.03172. [ 84 s]\n",
      "Epoch: 48 - loss(trn/val):0.03263/0.28520, acc(val):92.41%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 49 - 00020/00547 - Loss: 0.02170. [  3 s]\n",
      "Epoch: 49 - 00040/00547 - Loss: 0.03103. [  6 s]\n",
      "Epoch: 49 - 00060/00547 - Loss: 0.11069. [  9 s]\n",
      "Epoch: 49 - 00080/00547 - Loss: 0.03175. [ 12 s]\n",
      "Epoch: 49 - 00100/00547 - Loss: 0.06766. [ 15 s]\n",
      "Epoch: 49 - 00120/00547 - Loss: 0.04000. [ 18 s]\n",
      "Epoch: 49 - 00140/00547 - Loss: 0.02626. [ 21 s]\n",
      "Epoch: 49 - 00160/00547 - Loss: 0.03315. [ 25 s]\n",
      "Epoch: 49 - 00180/00547 - Loss: 0.02254. [ 28 s]\n",
      "Epoch: 49 - 00200/00547 - Loss: 0.01895. [ 31 s]\n",
      "Epoch: 49 - 00220/00547 - Loss: 0.02513. [ 34 s]\n",
      "Epoch: 49 - 00240/00547 - Loss: 0.03116. [ 37 s]\n",
      "Epoch: 49 - 00260/00547 - Loss: 0.01275. [ 40 s]\n",
      "Epoch: 49 - 00280/00547 - Loss: 0.02433. [ 43 s]\n",
      "Epoch: 49 - 00300/00547 - Loss: 0.05609. [ 46 s]\n",
      "Epoch: 49 - 00320/00547 - Loss: 0.03171. [ 49 s]\n",
      "Epoch: 49 - 00340/00547 - Loss: 0.02721. [ 52 s]\n",
      "Epoch: 49 - 00360/00547 - Loss: 0.02368. [ 55 s]\n",
      "Epoch: 49 - 00380/00547 - Loss: 0.03139. [ 59 s]\n",
      "Epoch: 49 - 00400/00547 - Loss: 0.01717. [ 62 s]\n",
      "Epoch: 49 - 00420/00547 - Loss: 0.02524. [ 65 s]\n",
      "Epoch: 49 - 00440/00547 - Loss: 0.04060. [ 68 s]\n",
      "Epoch: 49 - 00460/00547 - Loss: 0.02494. [ 71 s]\n",
      "Epoch: 49 - 00480/00547 - Loss: 0.04416. [ 74 s]\n",
      "Epoch: 49 - 00500/00547 - Loss: 0.04014. [ 77 s]\n",
      "Epoch: 49 - 00520/00547 - Loss: 0.02952. [ 80 s]\n",
      "Epoch: 49 - 00540/00547 - Loss: 0.02626. [ 83 s]\n",
      "Epoch: 49 - loss(trn/val):0.02785/0.30840, acc(val):92.46%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 50 - 00020/00547 - Loss: 0.02927. [  3 s]\n",
      "Epoch: 50 - 00040/00547 - Loss: 0.01653. [  6 s]\n",
      "Epoch: 50 - 00060/00547 - Loss: 0.02869. [  9 s]\n",
      "Epoch: 50 - 00080/00547 - Loss: 0.04044. [ 12 s]\n",
      "Epoch: 50 - 00100/00547 - Loss: 0.02401. [ 16 s]\n",
      "Epoch: 50 - 00120/00547 - Loss: 0.03013. [ 19 s]\n",
      "Epoch: 50 - 00140/00547 - Loss: 0.01654. [ 22 s]\n",
      "Epoch: 50 - 00160/00547 - Loss: 0.02148. [ 25 s]\n",
      "Epoch: 50 - 00180/00547 - Loss: 0.03348. [ 28 s]\n",
      "Epoch: 50 - 00200/00547 - Loss: 0.02554. [ 31 s]\n",
      "Epoch: 50 - 00220/00547 - Loss: 0.03414. [ 34 s]\n",
      "Epoch: 50 - 00240/00547 - Loss: 0.05129. [ 37 s]\n",
      "Epoch: 50 - 00260/00547 - Loss: 0.02101. [ 41 s]\n",
      "Epoch: 50 - 00280/00547 - Loss: 0.06592. [ 44 s]\n",
      "Epoch: 50 - 00300/00547 - Loss: 0.03970. [ 47 s]\n",
      "Epoch: 50 - 00320/00547 - Loss: 0.03116. [ 50 s]\n",
      "Epoch: 50 - 00340/00547 - Loss: 0.02430. [ 53 s]\n",
      "Epoch: 50 - 00360/00547 - Loss: 0.03126. [ 56 s]\n",
      "Epoch: 50 - 00380/00547 - Loss: 0.05304. [ 60 s]\n",
      "Epoch: 50 - 00400/00547 - Loss: 0.03102. [ 63 s]\n",
      "Epoch: 50 - 00420/00547 - Loss: 0.04272. [ 66 s]\n",
      "Epoch: 50 - 00440/00547 - Loss: 0.03810. [ 69 s]\n",
      "Epoch: 50 - 00460/00547 - Loss: 0.03819. [ 72 s]\n",
      "Epoch: 50 - 00480/00547 - Loss: 0.04003. [ 75 s]\n",
      "Epoch: 50 - 00500/00547 - Loss: 0.06849. [ 79 s]\n",
      "Epoch: 50 - 00520/00547 - Loss: 0.03201. [ 82 s]\n",
      "Epoch: 50 - 00540/00547 - Loss: 0.04487. [ 85 s]\n",
      "Epoch: 50 - loss(trn/val):0.03274/0.48145, acc(val):89.34%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 51 - 00020/00547 - Loss: 0.04033. [  3 s]\n",
      "Epoch: 51 - 00040/00547 - Loss: 0.01610. [  6 s]\n",
      "Epoch: 51 - 00060/00547 - Loss: 0.03312. [  9 s]\n",
      "Epoch: 51 - 00080/00547 - Loss: 0.03507. [ 12 s]\n",
      "Epoch: 51 - 00100/00547 - Loss: 0.04008. [ 16 s]\n",
      "Epoch: 51 - 00120/00547 - Loss: 0.03766. [ 19 s]\n",
      "Epoch: 51 - 00140/00547 - Loss: 0.02773. [ 22 s]\n",
      "Epoch: 51 - 00160/00547 - Loss: 0.05644. [ 25 s]\n",
      "Epoch: 51 - 00180/00547 - Loss: 0.02706. [ 28 s]\n",
      "Epoch: 51 - 00200/00547 - Loss: 0.04406. [ 31 s]\n",
      "Epoch: 51 - 00220/00547 - Loss: 0.03117. [ 34 s]\n",
      "Epoch: 51 - 00240/00547 - Loss: 0.02845. [ 37 s]\n",
      "Epoch: 51 - 00260/00547 - Loss: 0.01854. [ 41 s]\n",
      "Epoch: 51 - 00280/00547 - Loss: 0.04404. [ 44 s]\n",
      "Epoch: 51 - 00300/00547 - Loss: 0.05599. [ 47 s]\n",
      "Epoch: 51 - 00320/00547 - Loss: 0.01718. [ 50 s]\n",
      "Epoch: 51 - 00340/00547 - Loss: 0.01656. [ 53 s]\n",
      "Epoch: 51 - 00360/00547 - Loss: 0.02156. [ 57 s]\n",
      "Epoch: 51 - 00380/00547 - Loss: 0.02576. [ 60 s]\n",
      "Epoch: 51 - 00400/00547 - Loss: 0.02154. [ 63 s]\n",
      "Epoch: 51 - 00420/00547 - Loss: 0.01805. [ 66 s]\n",
      "Epoch: 51 - 00440/00547 - Loss: 0.05722. [ 69 s]\n",
      "Epoch: 51 - 00460/00547 - Loss: 0.01733. [ 72 s]\n",
      "Epoch: 51 - 00480/00547 - Loss: 0.01560. [ 76 s]\n",
      "Epoch: 51 - 00500/00547 - Loss: 0.01756. [ 79 s]\n",
      "Epoch: 51 - 00520/00547 - Loss: 0.07312. [ 82 s]\n",
      "Epoch: 51 - 00540/00547 - Loss: 0.08098. [ 85 s]\n",
      "Epoch: 51 - loss(trn/val):0.03284/0.33861, acc(val):90.71%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 52 - 00020/00547 - Loss: 0.03349. [  3 s]\n",
      "Epoch: 52 - 00040/00547 - Loss: 0.02975. [  6 s]\n",
      "Epoch: 52 - 00060/00547 - Loss: 0.02890. [  9 s]\n",
      "Epoch: 52 - 00080/00547 - Loss: 0.02923. [ 12 s]\n",
      "Epoch: 52 - 00100/00547 - Loss: 0.08422. [ 15 s]\n",
      "Epoch: 52 - 00120/00547 - Loss: 0.01921. [ 19 s]\n",
      "Epoch: 52 - 00140/00547 - Loss: 0.04376. [ 22 s]\n",
      "Epoch: 52 - 00160/00547 - Loss: 0.03794. [ 25 s]\n",
      "Epoch: 52 - 00180/00547 - Loss: 0.03072. [ 28 s]\n",
      "Epoch: 52 - 00200/00547 - Loss: 0.01825. [ 31 s]\n",
      "Epoch: 52 - 00220/00547 - Loss: 0.02511. [ 34 s]\n",
      "Epoch: 52 - 00240/00547 - Loss: 0.02985. [ 37 s]\n",
      "Epoch: 52 - 00260/00547 - Loss: 0.01255. [ 40 s]\n",
      "Epoch: 52 - 00280/00547 - Loss: 0.02299. [ 44 s]\n",
      "Epoch: 52 - 00300/00547 - Loss: 0.02789. [ 47 s]\n",
      "Epoch: 52 - 00320/00547 - Loss: 0.04442. [ 50 s]\n",
      "Epoch: 52 - 00340/00547 - Loss: 0.03246. [ 53 s]\n",
      "Epoch: 52 - 00360/00547 - Loss: 0.04940. [ 56 s]\n",
      "Epoch: 52 - 00380/00547 - Loss: 0.02204. [ 59 s]\n",
      "Epoch: 52 - 00400/00547 - Loss: 0.03869. [ 62 s]\n",
      "Epoch: 52 - 00420/00547 - Loss: 0.02517. [ 66 s]\n",
      "Epoch: 52 - 00440/00547 - Loss: 0.03577. [ 69 s]\n",
      "Epoch: 52 - 00460/00547 - Loss: 0.02048. [ 72 s]\n",
      "Epoch: 52 - 00480/00547 - Loss: 0.03070. [ 75 s]\n",
      "Epoch: 52 - 00500/00547 - Loss: 0.01954. [ 78 s]\n",
      "Epoch: 52 - 00520/00547 - Loss: 0.01324. [ 81 s]\n",
      "Epoch: 52 - 00540/00547 - Loss: 0.03102. [ 85 s]\n",
      "Epoch: 52 - loss(trn/val):0.02673/0.44702, acc(val):90.84%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 53 - 00020/00547 - Loss: 0.01307. [  3 s]\n",
      "Epoch: 53 - 00040/00547 - Loss: 0.04083. [  6 s]\n",
      "Epoch: 53 - 00060/00547 - Loss: 0.02075. [  9 s]\n",
      "Epoch: 53 - 00080/00547 - Loss: 0.03746. [ 12 s]\n",
      "Epoch: 53 - 00100/00547 - Loss: 0.02407. [ 16 s]\n",
      "Epoch: 53 - 00120/00547 - Loss: 0.02048. [ 19 s]\n",
      "Epoch: 53 - 00140/00547 - Loss: 0.02792. [ 22 s]\n",
      "Epoch: 53 - 00160/00547 - Loss: 0.04230. [ 25 s]\n",
      "Epoch: 53 - 00180/00547 - Loss: 0.01652. [ 28 s]\n",
      "Epoch: 53 - 00200/00547 - Loss: 0.02387. [ 31 s]\n",
      "Epoch: 53 - 00220/00547 - Loss: 0.04602. [ 34 s]\n",
      "Epoch: 53 - 00240/00547 - Loss: 0.02280. [ 37 s]\n",
      "Epoch: 53 - 00260/00547 - Loss: 0.02252. [ 40 s]\n",
      "Epoch: 53 - 00280/00547 - Loss: 0.03198. [ 44 s]\n",
      "Epoch: 53 - 00300/00547 - Loss: 0.02434. [ 47 s]\n",
      "Epoch: 53 - 00320/00547 - Loss: 0.02913. [ 50 s]\n",
      "Epoch: 53 - 00340/00547 - Loss: 0.05383. [ 53 s]\n",
      "Epoch: 53 - 00360/00547 - Loss: 0.05390. [ 56 s]\n",
      "Epoch: 53 - 00380/00547 - Loss: 0.05980. [ 60 s]\n",
      "Epoch: 53 - 00400/00547 - Loss: 0.08094. [ 63 s]\n",
      "Epoch: 53 - 00420/00547 - Loss: 0.02250. [ 66 s]\n",
      "Epoch: 53 - 00440/00547 - Loss: 0.04165. [ 69 s]\n",
      "Epoch: 53 - 00460/00547 - Loss: 0.01827. [ 72 s]\n",
      "Epoch: 53 - 00480/00547 - Loss: 0.01670. [ 76 s]\n",
      "Epoch: 53 - 00500/00547 - Loss: 0.04722. [ 79 s]\n",
      "Epoch: 53 - 00520/00547 - Loss: 0.03304. [ 82 s]\n",
      "Epoch: 53 - 00540/00547 - Loss: 0.01375. [ 85 s]\n",
      "Epoch: 53 - loss(trn/val):0.03078/0.36820, acc(val):91.04%, lr=0.00010. [86s] @25 samples/s \n",
      "Epoch: 54 - 00020/00547 - Loss: 0.01414. [  3 s]\n",
      "Epoch: 54 - 00040/00547 - Loss: 0.03341. [  6 s]\n",
      "Epoch: 54 - 00060/00547 - Loss: 0.02602. [  9 s]\n",
      "Epoch: 54 - 00080/00547 - Loss: 0.02296. [ 12 s]\n",
      "Epoch: 54 - 00100/00547 - Loss: 0.02211. [ 15 s]\n",
      "Epoch: 54 - 00120/00547 - Loss: 0.01647. [ 19 s]\n",
      "Epoch: 54 - 00140/00547 - Loss: 0.04278. [ 22 s]\n",
      "Epoch: 54 - 00160/00547 - Loss: 0.03726. [ 25 s]\n",
      "Epoch: 54 - 00180/00547 - Loss: 0.02546. [ 28 s]\n",
      "Epoch: 54 - 00200/00547 - Loss: 0.03011. [ 31 s]\n",
      "Epoch: 54 - 00220/00547 - Loss: 0.03078. [ 34 s]\n",
      "Epoch: 54 - 00240/00547 - Loss: 0.03257. [ 37 s]\n",
      "Epoch: 54 - 00260/00547 - Loss: 0.02747. [ 40 s]\n",
      "Epoch: 54 - 00280/00547 - Loss: 0.05594. [ 44 s]\n",
      "Epoch: 54 - 00300/00547 - Loss: 0.03857. [ 47 s]\n",
      "Epoch: 54 - 00320/00547 - Loss: 0.06374. [ 50 s]\n",
      "Epoch: 54 - 00340/00547 - Loss: 0.04341. [ 53 s]\n",
      "Epoch: 54 - 00360/00547 - Loss: 0.04311. [ 56 s]\n",
      "Epoch: 54 - 00380/00547 - Loss: 0.03079. [ 60 s]\n",
      "Epoch: 54 - 00400/00547 - Loss: 0.04589. [ 63 s]\n",
      "Epoch: 54 - 00420/00547 - Loss: 0.05764. [ 66 s]\n",
      "Epoch: 54 - 00440/00547 - Loss: 0.03542. [ 69 s]\n",
      "Epoch: 54 - 00460/00547 - Loss: 0.02491. [ 72 s]\n",
      "Epoch: 54 - 00480/00547 - Loss: 0.03152. [ 75 s]\n",
      "Epoch: 54 - 00500/00547 - Loss: 0.02290. [ 78 s]\n",
      "Epoch: 54 - 00520/00547 - Loss: 0.03649. [ 81 s]\n",
      "Epoch: 54 - 00540/00547 - Loss: 0.02312. [ 84 s]\n",
      "Epoch: 54 - loss(trn/val):0.02908/0.34670, acc(val):91.76%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 55 - 00020/00547 - Loss: 0.02391. [  3 s]\n",
      "Epoch: 55 - 00040/00547 - Loss: 0.01328. [  6 s]\n",
      "Epoch: 55 - 00060/00547 - Loss: 0.01648. [  9 s]\n",
      "Epoch: 55 - 00080/00547 - Loss: 0.03060. [ 12 s]\n",
      "Epoch: 55 - 00100/00547 - Loss: 0.03152. [ 15 s]\n",
      "Epoch: 55 - 00120/00547 - Loss: 0.01357. [ 18 s]\n",
      "Epoch: 55 - 00140/00547 - Loss: 0.04826. [ 21 s]\n",
      "Epoch: 55 - 00160/00547 - Loss: 0.01538. [ 24 s]\n",
      "Epoch: 55 - 00180/00547 - Loss: 0.02009. [ 27 s]\n",
      "Epoch: 55 - 00200/00547 - Loss: 0.02851. [ 31 s]\n",
      "Epoch: 55 - 00220/00547 - Loss: 0.02337. [ 34 s]\n",
      "Epoch: 55 - 00240/00547 - Loss: 0.02774. [ 37 s]\n",
      "Epoch: 55 - 00260/00547 - Loss: 0.02886. [ 40 s]\n",
      "Epoch: 55 - 00280/00547 - Loss: 0.02494. [ 43 s]\n",
      "Epoch: 55 - 00300/00547 - Loss: 0.02091. [ 46 s]\n",
      "Epoch: 55 - 00320/00547 - Loss: 0.03578. [ 49 s]\n",
      "Epoch: 55 - 00340/00547 - Loss: 0.02847. [ 52 s]\n",
      "Epoch: 55 - 00360/00547 - Loss: 0.05600. [ 55 s]\n",
      "Epoch: 55 - 00380/00547 - Loss: 0.04744. [ 59 s]\n",
      "Epoch: 55 - 00400/00547 - Loss: 0.07076. [ 62 s]\n",
      "Epoch: 55 - 00420/00547 - Loss: 0.04413. [ 65 s]\n",
      "Epoch: 55 - 00440/00547 - Loss: 0.15755. [ 68 s]\n",
      "Epoch: 55 - 00460/00547 - Loss: 0.03858. [ 71 s]\n",
      "Epoch: 55 - 00480/00547 - Loss: 0.03307. [ 74 s]\n",
      "Epoch: 55 - 00500/00547 - Loss: 0.03127. [ 77 s]\n",
      "Epoch: 55 - 00520/00547 - Loss: 0.03834. [ 80 s]\n",
      "Epoch: 55 - 00540/00547 - Loss: 0.02952. [ 83 s]\n",
      "Epoch: 55 - loss(trn/val):0.03058/0.33097, acc(val):91.92%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 56 - 00020/00547 - Loss: 0.03296. [  3 s]\n",
      "Epoch: 56 - 00040/00547 - Loss: 0.02793. [  6 s]\n",
      "Epoch: 56 - 00060/00547 - Loss: 0.01791. [  9 s]\n",
      "Epoch: 56 - 00080/00547 - Loss: 0.02893. [ 12 s]\n",
      "Epoch: 56 - 00100/00547 - Loss: 0.15205. [ 15 s]\n",
      "Epoch: 56 - 00120/00547 - Loss: 0.05584. [ 18 s]\n",
      "Epoch: 56 - 00140/00547 - Loss: 0.03262. [ 21 s]\n",
      "Epoch: 56 - 00160/00547 - Loss: 0.04189. [ 24 s]\n",
      "Epoch: 56 - 00180/00547 - Loss: 0.02245. [ 28 s]\n",
      "Epoch: 56 - 00200/00547 - Loss: 0.12294. [ 31 s]\n",
      "Epoch: 56 - 00220/00547 - Loss: 0.01804. [ 34 s]\n",
      "Epoch: 56 - 00240/00547 - Loss: 0.04147. [ 37 s]\n",
      "Epoch: 56 - 00260/00547 - Loss: 0.02761. [ 40 s]\n",
      "Epoch: 56 - 00280/00547 - Loss: 0.01885. [ 43 s]\n",
      "Epoch: 56 - 00300/00547 - Loss: 0.03242. [ 46 s]\n",
      "Epoch: 56 - 00320/00547 - Loss: 0.01902. [ 49 s]\n",
      "Epoch: 56 - 00340/00547 - Loss: 0.03733. [ 52 s]\n",
      "Epoch: 56 - 00360/00547 - Loss: 0.02890. [ 55 s]\n",
      "Epoch: 56 - 00380/00547 - Loss: 0.05350. [ 59 s]\n",
      "Epoch: 56 - 00400/00547 - Loss: 0.02778. [ 62 s]\n",
      "Epoch: 56 - 00420/00547 - Loss: 0.03269. [ 65 s]\n",
      "Epoch: 56 - 00440/00547 - Loss: 0.02395. [ 68 s]\n",
      "Epoch: 56 - 00460/00547 - Loss: 0.01246. [ 71 s]\n",
      "Epoch: 56 - 00480/00547 - Loss: 0.03736. [ 74 s]\n",
      "Epoch: 56 - 00500/00547 - Loss: 0.04123. [ 77 s]\n",
      "Epoch: 56 - 00520/00547 - Loss: 0.04592. [ 80 s]\n",
      "Epoch: 56 - 00540/00547 - Loss: 0.02902. [ 83 s]\n",
      "Epoch: 56 - loss(trn/val):0.02586/0.33881, acc(val):92.50%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 57 - 00020/00547 - Loss: 0.02689. [  3 s]\n",
      "Epoch: 57 - 00040/00547 - Loss: 0.02189. [  6 s]\n",
      "Epoch: 57 - 00060/00547 - Loss: 0.03153. [  9 s]\n",
      "Epoch: 57 - 00080/00547 - Loss: 0.01879. [ 12 s]\n",
      "Epoch: 57 - 00100/00547 - Loss: 0.03287. [ 15 s]\n",
      "Epoch: 57 - 00120/00547 - Loss: 0.02169. [ 18 s]\n",
      "Epoch: 57 - 00140/00547 - Loss: 0.04582. [ 21 s]\n",
      "Epoch: 57 - 00160/00547 - Loss: 0.03429. [ 25 s]\n",
      "Epoch: 57 - 00180/00547 - Loss: 0.02448. [ 28 s]\n",
      "Epoch: 57 - 00200/00547 - Loss: 0.03060. [ 31 s]\n",
      "Epoch: 57 - 00220/00547 - Loss: 0.02497. [ 34 s]\n",
      "Epoch: 57 - 00240/00547 - Loss: 0.06778. [ 37 s]\n",
      "Epoch: 57 - 00260/00547 - Loss: 0.02884. [ 40 s]\n",
      "Epoch: 57 - 00280/00547 - Loss: 0.03838. [ 43 s]\n",
      "Epoch: 57 - 00300/00547 - Loss: 0.05804. [ 46 s]\n",
      "Epoch: 57 - 00320/00547 - Loss: 0.04928. [ 49 s]\n",
      "Epoch: 57 - 00340/00547 - Loss: 0.03883. [ 52 s]\n",
      "Epoch: 57 - 00360/00547 - Loss: 0.02294. [ 55 s]\n",
      "Epoch: 57 - 00380/00547 - Loss: 0.01648. [ 58 s]\n",
      "Epoch: 57 - 00400/00547 - Loss: 0.03167. [ 62 s]\n",
      "Epoch: 57 - 00420/00547 - Loss: 0.02374. [ 65 s]\n",
      "Epoch: 57 - 00440/00547 - Loss: 0.06045. [ 68 s]\n",
      "Epoch: 57 - 00460/00547 - Loss: 0.01938. [ 71 s]\n",
      "Epoch: 57 - 00480/00547 - Loss: 0.05634. [ 74 s]\n",
      "Epoch: 57 - 00500/00547 - Loss: 0.03048. [ 77 s]\n",
      "Epoch: 57 - 00520/00547 - Loss: 0.02764. [ 80 s]\n",
      "Epoch: 57 - 00540/00547 - Loss: 0.02191. [ 83 s]\n",
      "Epoch: 57 - loss(trn/val):0.02638/0.40319, acc(val):90.97%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 58 - 00020/00547 - Loss: 0.02767. [  3 s]\n",
      "Epoch: 58 - 00040/00547 - Loss: 0.02065. [  6 s]\n",
      "Epoch: 58 - 00060/00547 - Loss: 0.01751. [  9 s]\n",
      "Epoch: 58 - 00080/00547 - Loss: 0.02800. [ 12 s]\n",
      "Epoch: 58 - 00100/00547 - Loss: 0.02006. [ 15 s]\n",
      "Epoch: 58 - 00120/00547 - Loss: 0.02019. [ 18 s]\n",
      "Epoch: 58 - 00140/00547 - Loss: 0.02154. [ 21 s]\n",
      "Epoch: 58 - 00160/00547 - Loss: 0.02190. [ 24 s]\n",
      "Epoch: 58 - 00180/00547 - Loss: 0.04590. [ 27 s]\n",
      "Epoch: 58 - 00200/00547 - Loss: 0.03522. [ 31 s]\n",
      "Epoch: 58 - 00220/00547 - Loss: 0.01897. [ 34 s]\n",
      "Epoch: 58 - 00240/00547 - Loss: 0.03722. [ 37 s]\n",
      "Epoch: 58 - 00260/00547 - Loss: 0.04232. [ 40 s]\n",
      "Epoch: 58 - 00280/00547 - Loss: 0.04064. [ 43 s]\n",
      "Epoch: 58 - 00300/00547 - Loss: 0.03365. [ 46 s]\n",
      "Epoch: 58 - 00320/00547 - Loss: 0.02169. [ 49 s]\n",
      "Epoch: 58 - 00340/00547 - Loss: 0.02686. [ 52 s]\n",
      "Epoch: 58 - 00360/00547 - Loss: 0.05903. [ 55 s]\n",
      "Epoch: 58 - 00380/00547 - Loss: 0.03771. [ 58 s]\n",
      "Epoch: 58 - 00400/00547 - Loss: 0.02081. [ 62 s]\n",
      "Epoch: 58 - 00420/00547 - Loss: 0.06155. [ 65 s]\n",
      "Epoch: 58 - 00440/00547 - Loss: 0.07733. [ 68 s]\n",
      "Epoch: 58 - 00460/00547 - Loss: 0.03893. [ 71 s]\n",
      "Epoch: 58 - 00480/00547 - Loss: 0.03087. [ 74 s]\n",
      "Epoch: 58 - 00500/00547 - Loss: 0.03425. [ 77 s]\n",
      "Epoch: 58 - 00520/00547 - Loss: 0.03593. [ 80 s]\n",
      "Epoch: 58 - 00540/00547 - Loss: 0.03170. [ 83 s]\n",
      "Epoch: 58 - loss(trn/val):0.02732/0.41589, acc(val):90.81%, lr=0.00010. [84s] @25 samples/s \n",
      "Epoch: 59 - 00020/00547 - Loss: 0.02196. [  3 s]\n",
      "Epoch: 59 - 00040/00547 - Loss: 0.01262. [  6 s]\n",
      "Epoch: 59 - 00060/00547 - Loss: 0.02955. [  9 s]\n",
      "Epoch: 59 - 00080/00547 - Loss: 0.02022. [ 12 s]\n",
      "Epoch: 59 - 00100/00547 - Loss: 0.03616. [ 15 s]\n",
      "Epoch: 59 - 00120/00547 - Loss: 0.03059. [ 18 s]\n",
      "Epoch: 59 - 00140/00547 - Loss: 0.04887. [ 21 s]\n",
      "Epoch: 59 - 00160/00547 - Loss: 0.02167. [ 24 s]\n",
      "Epoch: 59 - 00180/00547 - Loss: 0.02979. [ 27 s]\n",
      "Epoch: 59 - 00200/00547 - Loss: 0.02880. [ 31 s]\n",
      "Epoch: 59 - 00220/00547 - Loss: 0.03348. [ 34 s]\n",
      "Epoch: 59 - 00240/00547 - Loss: 0.02746. [ 37 s]\n",
      "Epoch: 59 - 00260/00547 - Loss: 0.02642. [ 40 s]\n",
      "Epoch: 59 - 00280/00547 - Loss: 0.01063. [ 43 s]\n",
      "Epoch: 59 - 00300/00547 - Loss: 0.02042. [ 46 s]\n",
      "Epoch: 59 - 00320/00547 - Loss: 0.03600. [ 49 s]\n",
      "Epoch: 59 - 00340/00547 - Loss: 0.02718. [ 52 s]\n",
      "Epoch: 59 - 00360/00547 - Loss: 0.03269. [ 56 s]\n",
      "Epoch: 59 - 00380/00547 - Loss: 0.01869. [ 59 s]\n",
      "Epoch: 59 - 00400/00547 - Loss: 0.02297. [ 62 s]\n",
      "Epoch: 59 - 00420/00547 - Loss: 0.02799. [ 65 s]\n",
      "Epoch: 59 - 00440/00547 - Loss: 0.02126. [ 68 s]\n",
      "Epoch: 59 - 00460/00547 - Loss: 0.01992. [ 71 s]\n",
      "Epoch: 59 - 00480/00547 - Loss: 0.07292. [ 74 s]\n",
      "Epoch: 59 - 00500/00547 - Loss: 0.03949. [ 77 s]\n",
      "Epoch: 59 - 00520/00547 - Loss: 0.11807. [ 80 s]\n",
      "Epoch: 59 - 00540/00547 - Loss: 0.05353. [ 84 s]\n",
      "Epoch: 59 - loss(trn/val):0.03575/0.34250, acc(val):90.28%, lr=0.00010. [85s] @25 samples/s \n",
      "Epoch: 60 - 00020/00547 - Loss: 0.02741. [  3 s]\n",
      "Epoch: 60 - 00040/00547 - Loss: 0.05845. [  6 s]\n",
      "Epoch: 60 - 00060/00547 - Loss: 0.02461. [  9 s]\n",
      "Epoch: 60 - 00080/00547 - Loss: 0.03455. [ 12 s]\n",
      "Epoch: 60 - 00100/00547 - Loss: 0.02647. [ 15 s]\n",
      "Epoch: 60 - 00120/00547 - Loss: 0.04522. [ 18 s]\n",
      "Epoch: 60 - 00140/00547 - Loss: 0.04205. [ 21 s]\n",
      "Epoch: 60 - 00160/00547 - Loss: 0.02665. [ 25 s]\n",
      "Epoch: 60 - 00180/00547 - Loss: 0.01552. [ 28 s]\n",
      "Epoch: 60 - 00200/00547 - Loss: 0.05977. [ 31 s]\n",
      "Epoch: 60 - 00220/00547 - Loss: 0.05552. [ 34 s]\n",
      "Epoch: 60 - 00240/00547 - Loss: 0.02673. [ 37 s]\n",
      "Epoch: 60 - 00260/00547 - Loss: 0.03081. [ 40 s]\n",
      "Epoch: 60 - 00280/00547 - Loss: 0.03095. [ 43 s]\n",
      "Epoch: 60 - 00300/00547 - Loss: 0.02142. [ 46 s]\n",
      "Epoch: 60 - 00320/00547 - Loss: 0.01880. [ 49 s]\n",
      "Epoch: 60 - 00340/00547 - Loss: 0.02386. [ 52 s]\n",
      "Epoch: 60 - 00360/00547 - Loss: 0.01861. [ 55 s]\n",
      "Epoch: 60 - 00380/00547 - Loss: 0.01674. [ 58 s]\n",
      "Epoch: 60 - 00400/00547 - Loss: 0.02541. [ 62 s]\n",
      "Epoch: 60 - 00420/00547 - Loss: 0.03770. [ 65 s]\n",
      "Epoch: 60 - 00440/00547 - Loss: 0.02036. [ 68 s]\n",
      "Epoch: 60 - 00460/00547 - Loss: 0.03009. [ 71 s]\n",
      "Epoch: 60 - 00480/00547 - Loss: 0.05381. [ 74 s]\n",
      "Epoch: 60 - 00500/00547 - Loss: 0.03854. [ 77 s]\n",
      "Epoch: 60 - 00520/00547 - Loss: 0.01182. [ 80 s]\n",
      "Epoch: 60 - 00540/00547 - Loss: 0.02690. [ 83 s]\n",
      "Epoch: 60 - loss(trn/val):0.02506/0.44697, acc(val):90.74%, lr=0.00010. [84s] @25 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.2099 \n",
      "iou=0.7544 \n",
      "acc=0.9179 \n",
      "sensitivity=0.8043 \n",
      "specificity=0.9582 \n",
      "precision=0.9056 \n",
      "f1=0.8416\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ef5c3",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ea0abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.209880 at epoch 6\n",
      "Minimum training loss: 0.025056 at epoch 60\n",
      "Maximum validation IoU: 0.786848 at epoch 36\n",
      "Maximum training IoU: 0.954035 at epoch 58\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_FCN_Misc_RMS_lr_e-4\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba47774",
   "metadata": {},
   "source": [
    "# Training on \"Tampere\" and \"Misc\" Dataset\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e1b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_FCN_Combined_RMS_lr_e-4'\n",
    "directory = \"../data/training_states/ResNet34_FCN\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d709e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "model = ResNetFCN(resnet34, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72faac",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a0111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2588 images in training dataset\n",
      "412 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.TAMP_OPEN_DOCK_MISC_TRN\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=[\"open\", \"dock\"], data_list_misc=['training'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=[\"channel\"], data_list_misc=['validation'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4836d",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eae494b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "ResNetFCN(\n",
      "  (pretrained_net): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "  )\n",
      "  (intermediate): IntermediateLayerGetter(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): FCNHead(\n",
      "    (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Trainable params:   22387881\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training              2588    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.1 mean:-0.0 std:1.0 / Labels:min:0.0 max:1.0 mean:0.3 std:0.5\n",
      "Validation             412    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.1 mean:-0.1 std:1.0 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00647 - Loss: 0.46704. [  3 s]\n",
      "Epoch: 1 - 00040/00647 - Loss: 0.47511. [  6 s]\n",
      "Epoch: 1 - 00060/00647 - Loss: 0.34970. [  9 s]\n",
      "Epoch: 1 - 00080/00647 - Loss: 0.33894. [ 12 s]\n",
      "Epoch: 1 - 00100/00647 - Loss: 0.40797. [ 15 s]\n",
      "Epoch: 1 - 00120/00647 - Loss: 0.23884. [ 18 s]\n",
      "Epoch: 1 - 00140/00647 - Loss: 0.16547. [ 21 s]\n",
      "Epoch: 1 - 00160/00647 - Loss: 0.16918. [ 24 s]\n",
      "Epoch: 1 - 00180/00647 - Loss: 0.20025. [ 27 s]\n",
      "Epoch: 1 - 00200/00647 - Loss: 0.22362. [ 31 s]\n",
      "Epoch: 1 - 00220/00647 - Loss: 0.19230. [ 34 s]\n",
      "Epoch: 1 - 00240/00647 - Loss: 0.44130. [ 37 s]\n",
      "Epoch: 1 - 00260/00647 - Loss: 0.20453. [ 40 s]\n",
      "Epoch: 1 - 00280/00647 - Loss: 0.18297. [ 43 s]\n",
      "Epoch: 1 - 00300/00647 - Loss: 0.33312. [ 46 s]\n",
      "Epoch: 1 - 00320/00647 - Loss: 0.47042. [ 49 s]\n",
      "Epoch: 1 - 00340/00647 - Loss: 0.24415. [ 52 s]\n",
      "Epoch: 1 - 00360/00647 - Loss: 0.25728. [ 55 s]\n",
      "Epoch: 1 - 00380/00647 - Loss: 0.43908. [ 58 s]\n",
      "Epoch: 1 - 00400/00647 - Loss: 0.23801. [ 62 s]\n",
      "Epoch: 1 - 00420/00647 - Loss: 0.15493. [ 65 s]\n",
      "Epoch: 1 - 00440/00647 - Loss: 0.18053. [ 68 s]\n",
      "Epoch: 1 - 00460/00647 - Loss: 0.22708. [ 71 s]\n",
      "Epoch: 1 - 00480/00647 - Loss: 0.15419. [ 74 s]\n",
      "Epoch: 1 - 00500/00647 - Loss: 0.25068. [ 77 s]\n",
      "Epoch: 1 - 00520/00647 - Loss: 0.30144. [ 80 s]\n",
      "Epoch: 1 - 00540/00647 - Loss: 0.22716. [ 83 s]\n",
      "Epoch: 1 - 00560/00647 - Loss: 0.43168. [ 86 s]\n",
      "Epoch: 1 - 00580/00647 - Loss: 0.19402. [ 90 s]\n",
      "Epoch: 1 - 00600/00647 - Loss: 0.29900. [ 93 s]\n",
      "Epoch: 1 - 00620/00647 - Loss: 0.22506. [ 96 s]\n",
      "Epoch: 1 - 00640/00647 - Loss: 0.11960. [ 99 s]\n",
      "Epoch: 1 - loss(trn/val):0.19790/0.18730, acc(val):92.63%, lr=0.00010 [BEST]. [100s] @25 samples/s \n",
      "Epoch: 2 - 00020/00647 - Loss: 0.21474. [  3 s]\n",
      "Epoch: 2 - 00040/00647 - Loss: 0.31820. [  6 s]\n",
      "Epoch: 2 - 00060/00647 - Loss: 0.10532. [  9 s]\n",
      "Epoch: 2 - 00080/00647 - Loss: 0.19380. [ 12 s]\n",
      "Epoch: 2 - 00100/00647 - Loss: 0.16161. [ 15 s]\n",
      "Epoch: 2 - 00120/00647 - Loss: 0.30434. [ 18 s]\n",
      "Epoch: 2 - 00140/00647 - Loss: 0.19307. [ 21 s]\n",
      "Epoch: 2 - 00160/00647 - Loss: 0.24680. [ 24 s]\n",
      "Epoch: 2 - 00180/00647 - Loss: 0.11173. [ 27 s]\n",
      "Epoch: 2 - 00200/00647 - Loss: 0.13805. [ 31 s]\n",
      "Epoch: 2 - 00220/00647 - Loss: 0.15332. [ 34 s]\n",
      "Epoch: 2 - 00240/00647 - Loss: 0.30614. [ 37 s]\n",
      "Epoch: 2 - 00260/00647 - Loss: 0.15122. [ 40 s]\n",
      "Epoch: 2 - 00280/00647 - Loss: 0.21471. [ 43 s]\n",
      "Epoch: 2 - 00300/00647 - Loss: 0.15728. [ 46 s]\n",
      "Epoch: 2 - 00320/00647 - Loss: 0.25257. [ 49 s]\n",
      "Epoch: 2 - 00340/00647 - Loss: 0.16939. [ 52 s]\n",
      "Epoch: 2 - 00360/00647 - Loss: 0.14652. [ 55 s]\n",
      "Epoch: 2 - 00380/00647 - Loss: 0.19203. [ 58 s]\n",
      "Epoch: 2 - 00400/00647 - Loss: 0.21963. [ 61 s]\n",
      "Epoch: 2 - 00420/00647 - Loss: 0.08779. [ 65 s]\n",
      "Epoch: 2 - 00440/00647 - Loss: 0.15758. [ 68 s]\n",
      "Epoch: 2 - 00460/00647 - Loss: 0.28611. [ 71 s]\n",
      "Epoch: 2 - 00480/00647 - Loss: 0.24093. [ 74 s]\n",
      "Epoch: 2 - 00500/00647 - Loss: 0.19305. [ 77 s]\n",
      "Epoch: 2 - 00520/00647 - Loss: 0.13247. [ 80 s]\n",
      "Epoch: 2 - 00540/00647 - Loss: 0.12319. [ 83 s]\n",
      "Epoch: 2 - 00560/00647 - Loss: 0.18133. [ 86 s]\n",
      "Epoch: 2 - 00580/00647 - Loss: 0.22464. [ 89 s]\n",
      "Epoch: 2 - 00600/00647 - Loss: 0.14227. [ 92 s]\n",
      "Epoch: 2 - 00620/00647 - Loss: 0.15576. [ 96 s]\n",
      "Epoch: 2 - 00640/00647 - Loss: 0.16476. [ 99 s]\n",
      "Epoch: 2 - loss(trn/val):0.21155/0.24129, acc(val):90.34%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 3 - 00020/00647 - Loss: 0.11740. [  3 s]\n",
      "Epoch: 3 - 00040/00647 - Loss: 0.09733. [  6 s]\n",
      "Epoch: 3 - 00060/00647 - Loss: 0.12645. [  9 s]\n",
      "Epoch: 3 - 00080/00647 - Loss: 0.68953. [ 12 s]\n",
      "Epoch: 3 - 00100/00647 - Loss: 0.37030. [ 15 s]\n",
      "Epoch: 3 - 00120/00647 - Loss: 0.12930. [ 18 s]\n",
      "Epoch: 3 - 00140/00647 - Loss: 0.13766. [ 21 s]\n",
      "Epoch: 3 - 00160/00647 - Loss: 0.23715. [ 24 s]\n",
      "Epoch: 3 - 00180/00647 - Loss: 0.09831. [ 27 s]\n",
      "Epoch: 3 - 00200/00647 - Loss: 0.28345. [ 30 s]\n",
      "Epoch: 3 - 00220/00647 - Loss: 0.15566. [ 34 s]\n",
      "Epoch: 3 - 00240/00647 - Loss: 0.09927. [ 37 s]\n",
      "Epoch: 3 - 00260/00647 - Loss: 0.14002. [ 40 s]\n",
      "Epoch: 3 - 00280/00647 - Loss: 0.16965. [ 43 s]\n",
      "Epoch: 3 - 00300/00647 - Loss: 0.09014. [ 46 s]\n",
      "Epoch: 3 - 00320/00647 - Loss: 0.17514. [ 49 s]\n",
      "Epoch: 3 - 00340/00647 - Loss: 0.14979. [ 52 s]\n",
      "Epoch: 3 - 00360/00647 - Loss: 0.25750. [ 55 s]\n",
      "Epoch: 3 - 00380/00647 - Loss: 0.14145. [ 58 s]\n",
      "Epoch: 3 - 00400/00647 - Loss: 0.18119. [ 61 s]\n",
      "Epoch: 3 - 00420/00647 - Loss: 0.19578. [ 65 s]\n",
      "Epoch: 3 - 00440/00647 - Loss: 0.13927. [ 68 s]\n",
      "Epoch: 3 - 00460/00647 - Loss: 0.12171. [ 71 s]\n",
      "Epoch: 3 - 00480/00647 - Loss: 0.10460. [ 74 s]\n",
      "Epoch: 3 - 00500/00647 - Loss: 0.34254. [ 77 s]\n",
      "Epoch: 3 - 00520/00647 - Loss: 0.15442. [ 80 s]\n",
      "Epoch: 3 - 00540/00647 - Loss: 0.16638. [ 83 s]\n",
      "Epoch: 3 - 00560/00647 - Loss: 0.13854. [ 86 s]\n",
      "Epoch: 3 - 00580/00647 - Loss: 0.12539. [ 89 s]\n",
      "Epoch: 3 - 00600/00647 - Loss: 0.27630. [ 92 s]\n",
      "Epoch: 3 - 00620/00647 - Loss: 0.29461. [ 95 s]\n",
      "Epoch: 3 - 00640/00647 - Loss: 0.27597. [ 99 s]\n",
      "Epoch: 3 - loss(trn/val):0.13242/0.14019, acc(val):94.79%, lr=0.00010 [BEST]. [100s] @25 samples/s \n",
      "Epoch: 4 - 00020/00647 - Loss: 0.26362. [  3 s]\n",
      "Epoch: 4 - 00040/00647 - Loss: 0.14003. [  6 s]\n",
      "Epoch: 4 - 00060/00647 - Loss: 0.10098. [  9 s]\n",
      "Epoch: 4 - 00080/00647 - Loss: 0.21031. [ 12 s]\n",
      "Epoch: 4 - 00100/00647 - Loss: 0.24463. [ 15 s]\n",
      "Epoch: 4 - 00120/00647 - Loss: 0.15828. [ 18 s]\n",
      "Epoch: 4 - 00140/00647 - Loss: 0.09659. [ 21 s]\n",
      "Epoch: 4 - 00160/00647 - Loss: 0.20017. [ 24 s]\n",
      "Epoch: 4 - 00180/00647 - Loss: 0.06846. [ 27 s]\n",
      "Epoch: 4 - 00200/00647 - Loss: 0.16295. [ 30 s]\n",
      "Epoch: 4 - 00220/00647 - Loss: 0.18414. [ 34 s]\n",
      "Epoch: 4 - 00240/00647 - Loss: 0.07317. [ 37 s]\n",
      "Epoch: 4 - 00260/00647 - Loss: 0.08868. [ 40 s]\n",
      "Epoch: 4 - 00280/00647 - Loss: 0.13916. [ 43 s]\n",
      "Epoch: 4 - 00300/00647 - Loss: 0.28079. [ 46 s]\n",
      "Epoch: 4 - 00320/00647 - Loss: 0.12563. [ 49 s]\n",
      "Epoch: 4 - 00340/00647 - Loss: 0.16520. [ 52 s]\n",
      "Epoch: 4 - 00360/00647 - Loss: 0.16866. [ 55 s]\n",
      "Epoch: 4 - 00380/00647 - Loss: 0.19957. [ 58 s]\n",
      "Epoch: 4 - 00400/00647 - Loss: 0.13436. [ 61 s]\n",
      "Epoch: 4 - 00420/00647 - Loss: 0.16497. [ 64 s]\n",
      "Epoch: 4 - 00440/00647 - Loss: 0.14837. [ 68 s]\n",
      "Epoch: 4 - 00460/00647 - Loss: 0.13585. [ 71 s]\n",
      "Epoch: 4 - 00480/00647 - Loss: 0.10740. [ 74 s]\n",
      "Epoch: 4 - 00500/00647 - Loss: 0.08811. [ 77 s]\n",
      "Epoch: 4 - 00520/00647 - Loss: 0.12185. [ 80 s]\n",
      "Epoch: 4 - 00540/00647 - Loss: 0.11637. [ 83 s]\n",
      "Epoch: 4 - 00560/00647 - Loss: 0.14870. [ 86 s]\n",
      "Epoch: 4 - 00580/00647 - Loss: 0.13274. [ 89 s]\n",
      "Epoch: 4 - 00600/00647 - Loss: 0.16553. [ 92 s]\n",
      "Epoch: 4 - 00620/00647 - Loss: 0.09465. [ 95 s]\n",
      "Epoch: 4 - 00640/00647 - Loss: 0.11426. [ 99 s]\n",
      "Epoch: 4 - loss(trn/val):0.11086/0.14544, acc(val):94.32%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 5 - 00020/00647 - Loss: 0.32753. [  3 s]\n",
      "Epoch: 5 - 00040/00647 - Loss: 0.14495. [  6 s]\n",
      "Epoch: 5 - 00060/00647 - Loss: 0.11413. [  9 s]\n",
      "Epoch: 5 - 00080/00647 - Loss: 0.11085. [ 12 s]\n",
      "Epoch: 5 - 00100/00647 - Loss: 0.18157. [ 15 s]\n",
      "Epoch: 5 - 00120/00647 - Loss: 0.08714. [ 18 s]\n",
      "Epoch: 5 - 00140/00647 - Loss: 0.06599. [ 21 s]\n",
      "Epoch: 5 - 00160/00647 - Loss: 0.15993. [ 24 s]\n",
      "Epoch: 5 - 00180/00647 - Loss: 0.04120. [ 27 s]\n",
      "Epoch: 5 - 00200/00647 - Loss: 0.07357. [ 30 s]\n",
      "Epoch: 5 - 00220/00647 - Loss: 0.23863. [ 33 s]\n",
      "Epoch: 5 - 00240/00647 - Loss: 0.43370. [ 36 s]\n",
      "Epoch: 5 - 00260/00647 - Loss: 0.14330. [ 40 s]\n",
      "Epoch: 5 - 00280/00647 - Loss: 0.25185. [ 43 s]\n",
      "Epoch: 5 - 00300/00647 - Loss: 0.07311. [ 46 s]\n",
      "Epoch: 5 - 00320/00647 - Loss: 0.09134. [ 49 s]\n",
      "Epoch: 5 - 00340/00647 - Loss: 0.05788. [ 52 s]\n",
      "Epoch: 5 - 00360/00647 - Loss: 0.09995. [ 55 s]\n",
      "Epoch: 5 - 00380/00647 - Loss: 0.08744. [ 58 s]\n",
      "Epoch: 5 - 00400/00647 - Loss: 0.07232. [ 61 s]\n",
      "Epoch: 5 - 00420/00647 - Loss: 0.28918. [ 64 s]\n",
      "Epoch: 5 - 00440/00647 - Loss: 0.09744. [ 67 s]\n",
      "Epoch: 5 - 00460/00647 - Loss: 0.15945. [ 70 s]\n",
      "Epoch: 5 - 00480/00647 - Loss: 0.15225. [ 73 s]\n",
      "Epoch: 5 - 00500/00647 - Loss: 0.39767. [ 77 s]\n",
      "Epoch: 5 - 00520/00647 - Loss: 0.19749. [ 80 s]\n",
      "Epoch: 5 - 00540/00647 - Loss: 0.08487. [ 83 s]\n",
      "Epoch: 5 - 00560/00647 - Loss: 0.15553. [ 86 s]\n",
      "Epoch: 5 - 00580/00647 - Loss: 0.15081. [ 89 s]\n",
      "Epoch: 5 - 00600/00647 - Loss: 0.12795. [ 92 s]\n",
      "Epoch: 5 - 00620/00647 - Loss: 0.04972. [ 95 s]\n",
      "Epoch: 5 - 00640/00647 - Loss: 0.07582. [ 98 s]\n",
      "Epoch: 5 - loss(trn/val):0.11202/0.16762, acc(val):93.73%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 6 - 00020/00647 - Loss: 0.18024. [  3 s]\n",
      "Epoch: 6 - 00040/00647 - Loss: 0.09540. [  6 s]\n",
      "Epoch: 6 - 00060/00647 - Loss: 0.06939. [  9 s]\n",
      "Epoch: 6 - 00080/00647 - Loss: 0.07684. [ 12 s]\n",
      "Epoch: 6 - 00100/00647 - Loss: 0.23512. [ 15 s]\n",
      "Epoch: 6 - 00120/00647 - Loss: 0.13249. [ 18 s]\n",
      "Epoch: 6 - 00140/00647 - Loss: 0.19021. [ 21 s]\n",
      "Epoch: 6 - 00160/00647 - Loss: 0.19298. [ 24 s]\n",
      "Epoch: 6 - 00180/00647 - Loss: 0.14114. [ 27 s]\n",
      "Epoch: 6 - 00200/00647 - Loss: 0.07580. [ 30 s]\n",
      "Epoch: 6 - 00220/00647 - Loss: 0.04763. [ 34 s]\n",
      "Epoch: 6 - 00240/00647 - Loss: 0.04911. [ 37 s]\n",
      "Epoch: 6 - 00260/00647 - Loss: 0.21112. [ 40 s]\n",
      "Epoch: 6 - 00280/00647 - Loss: 0.13791. [ 43 s]\n",
      "Epoch: 6 - 00300/00647 - Loss: 0.24442. [ 46 s]\n",
      "Epoch: 6 - 00320/00647 - Loss: 0.06077. [ 49 s]\n",
      "Epoch: 6 - 00340/00647 - Loss: 0.10836. [ 52 s]\n",
      "Epoch: 6 - 00360/00647 - Loss: 0.09538. [ 55 s]\n",
      "Epoch: 6 - 00380/00647 - Loss: 0.05112. [ 58 s]\n",
      "Epoch: 6 - 00400/00647 - Loss: 0.13947. [ 61 s]\n",
      "Epoch: 6 - 00420/00647 - Loss: 0.07882. [ 64 s]\n",
      "Epoch: 6 - 00440/00647 - Loss: 0.08670. [ 67 s]\n",
      "Epoch: 6 - 00460/00647 - Loss: 0.16099. [ 71 s]\n",
      "Epoch: 6 - 00480/00647 - Loss: 0.05810. [ 74 s]\n",
      "Epoch: 6 - 00500/00647 - Loss: 0.07633. [ 77 s]\n",
      "Epoch: 6 - 00520/00647 - Loss: 0.13566. [ 80 s]\n",
      "Epoch: 6 - 00540/00647 - Loss: 0.09913. [ 83 s]\n",
      "Epoch: 6 - 00560/00647 - Loss: 0.14028. [ 86 s]\n",
      "Epoch: 6 - 00580/00647 - Loss: 0.09801. [ 89 s]\n",
      "Epoch: 6 - 00600/00647 - Loss: 0.22799. [ 92 s]\n",
      "Epoch: 6 - 00620/00647 - Loss: 0.09441. [ 95 s]\n",
      "Epoch: 6 - 00640/00647 - Loss: 0.10029. [ 98 s]\n",
      "Epoch: 6 - loss(trn/val):0.09929/0.12832, acc(val):94.86%, lr=0.00010 [BEST]. [99s] @25 samples/s \n",
      "Epoch: 7 - 00020/00647 - Loss: 0.08920. [  3 s]\n",
      "Epoch: 7 - 00040/00647 - Loss: 0.15974. [  6 s]\n",
      "Epoch: 7 - 00060/00647 - Loss: 0.07667. [  9 s]\n",
      "Epoch: 7 - 00080/00647 - Loss: 0.04922. [ 12 s]\n",
      "Epoch: 7 - 00100/00647 - Loss: 0.12861. [ 15 s]\n",
      "Epoch: 7 - 00120/00647 - Loss: 0.04951. [ 18 s]\n",
      "Epoch: 7 - 00140/00647 - Loss: 0.04860. [ 21 s]\n",
      "Epoch: 7 - 00160/00647 - Loss: 0.06507. [ 24 s]\n",
      "Epoch: 7 - 00180/00647 - Loss: 0.08677. [ 27 s]\n",
      "Epoch: 7 - 00200/00647 - Loss: 0.08062. [ 31 s]\n",
      "Epoch: 7 - 00220/00647 - Loss: 0.10810. [ 34 s]\n",
      "Epoch: 7 - 00240/00647 - Loss: 0.15911. [ 37 s]\n",
      "Epoch: 7 - 00260/00647 - Loss: 0.14304. [ 40 s]\n",
      "Epoch: 7 - 00280/00647 - Loss: 0.04122. [ 43 s]\n",
      "Epoch: 7 - 00300/00647 - Loss: 0.03686. [ 46 s]\n",
      "Epoch: 7 - 00320/00647 - Loss: 0.22008. [ 49 s]\n",
      "Epoch: 7 - 00340/00647 - Loss: 0.07566. [ 53 s]\n",
      "Epoch: 7 - 00360/00647 - Loss: 0.05461. [ 56 s]\n",
      "Epoch: 7 - 00380/00647 - Loss: 0.18512. [ 59 s]\n",
      "Epoch: 7 - 00400/00647 - Loss: 0.11560. [ 62 s]\n",
      "Epoch: 7 - 00420/00647 - Loss: 0.10228. [ 65 s]\n",
      "Epoch: 7 - 00440/00647 - Loss: 0.11605. [ 68 s]\n",
      "Epoch: 7 - 00460/00647 - Loss: 0.13161. [ 72 s]\n",
      "Epoch: 7 - 00480/00647 - Loss: 0.14910. [ 75 s]\n",
      "Epoch: 7 - 00500/00647 - Loss: 0.10896. [ 78 s]\n",
      "Epoch: 7 - 00520/00647 - Loss: 0.09434. [ 81 s]\n",
      "Epoch: 7 - 00540/00647 - Loss: 0.06201. [ 84 s]\n",
      "Epoch: 7 - 00560/00647 - Loss: 0.05274. [ 88 s]\n",
      "Epoch: 7 - 00580/00647 - Loss: 0.19877. [ 91 s]\n",
      "Epoch: 7 - 00600/00647 - Loss: 0.13772. [ 94 s]\n",
      "Epoch: 7 - 00620/00647 - Loss: 0.10798. [ 97 s]\n",
      "Epoch: 7 - 00640/00647 - Loss: 0.10167. [100 s]\n",
      "Epoch: 7 - loss(trn/val):0.09426/0.14074, acc(val):94.41%, lr=0.00010. [101s] @25 samples/s \n",
      "Epoch: 8 - 00020/00647 - Loss: 0.07407. [  3 s]\n",
      "Epoch: 8 - 00040/00647 - Loss: 0.04321. [  6 s]\n",
      "Epoch: 8 - 00060/00647 - Loss: 0.08884. [  9 s]\n",
      "Epoch: 8 - 00080/00647 - Loss: 0.08460. [ 13 s]\n",
      "Epoch: 8 - 00100/00647 - Loss: 0.05893. [ 16 s]\n",
      "Epoch: 8 - 00120/00647 - Loss: 0.06728. [ 19 s]\n",
      "Epoch: 8 - 00140/00647 - Loss: 0.23529. [ 22 s]\n",
      "Epoch: 8 - 00160/00647 - Loss: 0.04300. [ 25 s]\n",
      "Epoch: 8 - 00180/00647 - Loss: 0.15927. [ 28 s]\n",
      "Epoch: 8 - 00200/00647 - Loss: 0.09640. [ 31 s]\n",
      "Epoch: 8 - 00220/00647 - Loss: 0.08833. [ 35 s]\n",
      "Epoch: 8 - 00240/00647 - Loss: 0.09507. [ 38 s]\n",
      "Epoch: 8 - 00260/00647 - Loss: 0.15558. [ 41 s]\n",
      "Epoch: 8 - 00280/00647 - Loss: 0.05787. [ 44 s]\n",
      "Epoch: 8 - 00300/00647 - Loss: 0.11083. [ 47 s]\n",
      "Epoch: 8 - 00320/00647 - Loss: 0.18699. [ 50 s]\n",
      "Epoch: 8 - 00340/00647 - Loss: 0.05762. [ 54 s]\n",
      "Epoch: 8 - 00360/00647 - Loss: 0.07076. [ 57 s]\n",
      "Epoch: 8 - 00380/00647 - Loss: 0.09438. [ 60 s]\n",
      "Epoch: 8 - 00400/00647 - Loss: 0.05788. [ 63 s]\n",
      "Epoch: 8 - 00420/00647 - Loss: 0.08503. [ 66 s]\n",
      "Epoch: 8 - 00440/00647 - Loss: 0.21758. [ 70 s]\n",
      "Epoch: 8 - 00460/00647 - Loss: 0.04339. [ 73 s]\n",
      "Epoch: 8 - 00480/00647 - Loss: 0.13361. [ 76 s]\n",
      "Epoch: 8 - 00500/00647 - Loss: 0.07596. [ 79 s]\n",
      "Epoch: 8 - 00520/00647 - Loss: 0.05638. [ 82 s]\n",
      "Epoch: 8 - 00540/00647 - Loss: 0.08903. [ 86 s]\n",
      "Epoch: 8 - 00560/00647 - Loss: 0.07454. [ 89 s]\n",
      "Epoch: 8 - 00580/00647 - Loss: 0.09867. [ 92 s]\n",
      "Epoch: 8 - 00600/00647 - Loss: 0.06069. [ 95 s]\n",
      "Epoch: 8 - 00620/00647 - Loss: 0.05038. [ 98 s]\n",
      "Epoch: 8 - 00640/00647 - Loss: 0.36906. [101 s]\n",
      "Epoch: 8 - loss(trn/val):0.09358/0.17772, acc(val):94.13%, lr=0.00010. [103s] @25 samples/s \n",
      "Epoch: 9 - 00020/00647 - Loss: 0.04366. [  3 s]\n",
      "Epoch: 9 - 00040/00647 - Loss: 0.07106. [  6 s]\n",
      "Epoch: 9 - 00060/00647 - Loss: 0.10281. [  9 s]\n",
      "Epoch: 9 - 00080/00647 - Loss: 0.08170. [ 13 s]\n",
      "Epoch: 9 - 00100/00647 - Loss: 0.04107. [ 16 s]\n",
      "Epoch: 9 - 00120/00647 - Loss: 0.09671. [ 19 s]\n",
      "Epoch: 9 - 00140/00647 - Loss: 0.13308. [ 22 s]\n",
      "Epoch: 9 - 00160/00647 - Loss: 0.10530. [ 25 s]\n",
      "Epoch: 9 - 00180/00647 - Loss: 0.08636. [ 28 s]\n",
      "Epoch: 9 - 00200/00647 - Loss: 0.08918. [ 31 s]\n",
      "Epoch: 9 - 00220/00647 - Loss: 0.07376. [ 34 s]\n",
      "Epoch: 9 - 00240/00647 - Loss: 0.25490. [ 38 s]\n",
      "Epoch: 9 - 00260/00647 - Loss: 0.04364. [ 41 s]\n",
      "Epoch: 9 - 00280/00647 - Loss: 0.14180. [ 44 s]\n",
      "Epoch: 9 - 00300/00647 - Loss: 0.10111. [ 47 s]\n",
      "Epoch: 9 - 00320/00647 - Loss: 0.07030. [ 50 s]\n",
      "Epoch: 9 - 00340/00647 - Loss: 0.09775. [ 53 s]\n",
      "Epoch: 9 - 00360/00647 - Loss: 0.07207. [ 57 s]\n",
      "Epoch: 9 - 00380/00647 - Loss: 0.09578. [ 60 s]\n",
      "Epoch: 9 - 00400/00647 - Loss: 0.22440. [ 63 s]\n",
      "Epoch: 9 - 00420/00647 - Loss: 0.08847. [ 66 s]\n",
      "Epoch: 9 - 00440/00647 - Loss: 0.11190. [ 69 s]\n",
      "Epoch: 9 - 00460/00647 - Loss: 0.03848. [ 73 s]\n",
      "Epoch: 9 - 00480/00647 - Loss: 0.05128. [ 76 s]\n",
      "Epoch: 9 - 00500/00647 - Loss: 0.13350. [ 79 s]\n",
      "Epoch: 9 - 00520/00647 - Loss: 0.03751. [ 82 s]\n",
      "Epoch: 9 - 00540/00647 - Loss: 0.03217. [ 85 s]\n",
      "Epoch: 9 - 00560/00647 - Loss: 0.05750. [ 89 s]\n",
      "Epoch: 9 - 00580/00647 - Loss: 0.33343. [ 92 s]\n",
      "Epoch: 9 - 00600/00647 - Loss: 0.07177. [ 95 s]\n",
      "Epoch: 9 - 00620/00647 - Loss: 0.09304. [ 98 s]\n",
      "Epoch: 9 - 00640/00647 - Loss: 0.13105. [101 s]\n",
      "Epoch: 9 - loss(trn/val):0.09566/0.17600, acc(val):93.54%, lr=0.00010. [102s] @25 samples/s \n",
      "Epoch: 10 - 00020/00647 - Loss: 0.06532. [  3 s]\n",
      "Epoch: 10 - 00040/00647 - Loss: 0.09289. [  6 s]\n",
      "Epoch: 10 - 00060/00647 - Loss: 0.03918. [  9 s]\n",
      "Epoch: 10 - 00080/00647 - Loss: 0.13857. [ 13 s]\n",
      "Epoch: 10 - 00100/00647 - Loss: 0.04148. [ 16 s]\n",
      "Epoch: 10 - 00120/00647 - Loss: 0.03859. [ 19 s]\n",
      "Epoch: 10 - 00140/00647 - Loss: 0.12741. [ 22 s]\n",
      "Epoch: 10 - 00160/00647 - Loss: 0.06697. [ 25 s]\n",
      "Epoch: 10 - 00180/00647 - Loss: 0.08180. [ 28 s]\n",
      "Epoch: 10 - 00200/00647 - Loss: 0.06424. [ 31 s]\n",
      "Epoch: 10 - 00220/00647 - Loss: 0.08072. [ 34 s]\n",
      "Epoch: 10 - 00240/00647 - Loss: 0.18221. [ 38 s]\n",
      "Epoch: 10 - 00260/00647 - Loss: 0.05217. [ 41 s]\n",
      "Epoch: 10 - 00280/00647 - Loss: 0.05684. [ 44 s]\n",
      "Epoch: 10 - 00300/00647 - Loss: 0.13499. [ 47 s]\n",
      "Epoch: 10 - 00320/00647 - Loss: 0.02791. [ 50 s]\n",
      "Epoch: 10 - 00340/00647 - Loss: 0.11545. [ 54 s]\n",
      "Epoch: 10 - 00360/00647 - Loss: 0.04134. [ 57 s]\n",
      "Epoch: 10 - 00380/00647 - Loss: 0.05932. [ 60 s]\n",
      "Epoch: 10 - 00400/00647 - Loss: 0.05017. [ 63 s]\n",
      "Epoch: 10 - 00420/00647 - Loss: 0.05558. [ 66 s]\n",
      "Epoch: 10 - 00440/00647 - Loss: 0.05078. [ 70 s]\n",
      "Epoch: 10 - 00460/00647 - Loss: 0.09985. [ 73 s]\n",
      "Epoch: 10 - 00480/00647 - Loss: 0.04735. [ 76 s]\n",
      "Epoch: 10 - 00500/00647 - Loss: 0.04303. [ 79 s]\n",
      "Epoch: 10 - 00520/00647 - Loss: 0.06294. [ 83 s]\n",
      "Epoch: 10 - 00540/00647 - Loss: 0.10486. [ 86 s]\n",
      "Epoch: 10 - 00560/00647 - Loss: 0.04986. [ 89 s]\n",
      "Epoch: 10 - 00580/00647 - Loss: 0.07175. [ 92 s]\n",
      "Epoch: 10 - 00600/00647 - Loss: 0.06616. [ 95 s]\n",
      "Epoch: 10 - 00620/00647 - Loss: 0.06154. [ 99 s]\n",
      "Epoch: 10 - 00640/00647 - Loss: 0.04300. [102 s]\n",
      "Epoch: 10 - loss(trn/val):0.07433/0.21167, acc(val):93.34%, lr=0.00010. [103s] @25 samples/s \n",
      "Epoch: 11 - 00020/00647 - Loss: 0.05727. [  3 s]\n",
      "Epoch: 11 - 00040/00647 - Loss: 0.10794. [  6 s]\n",
      "Epoch: 11 - 00060/00647 - Loss: 0.18013. [  9 s]\n",
      "Epoch: 11 - 00080/00647 - Loss: 0.04599. [ 12 s]\n",
      "Epoch: 11 - 00100/00647 - Loss: 0.06257. [ 15 s]\n",
      "Epoch: 11 - 00120/00647 - Loss: 0.06549. [ 18 s]\n",
      "Epoch: 11 - 00140/00647 - Loss: 0.11693. [ 21 s]\n",
      "Epoch: 11 - 00160/00647 - Loss: 0.02817. [ 24 s]\n",
      "Epoch: 11 - 00180/00647 - Loss: 0.07905. [ 27 s]\n",
      "Epoch: 11 - 00200/00647 - Loss: 0.13818. [ 30 s]\n",
      "Epoch: 11 - 00220/00647 - Loss: 0.02831. [ 34 s]\n",
      "Epoch: 11 - 00240/00647 - Loss: 0.07880. [ 37 s]\n",
      "Epoch: 11 - 00260/00647 - Loss: 0.04025. [ 40 s]\n",
      "Epoch: 11 - 00280/00647 - Loss: 0.08598. [ 43 s]\n",
      "Epoch: 11 - 00300/00647 - Loss: 0.05453. [ 46 s]\n",
      "Epoch: 11 - 00320/00647 - Loss: 0.11020. [ 49 s]\n",
      "Epoch: 11 - 00340/00647 - Loss: 0.02822. [ 52 s]\n",
      "Epoch: 11 - 00360/00647 - Loss: 0.05260. [ 55 s]\n",
      "Epoch: 11 - 00380/00647 - Loss: 0.13374. [ 59 s]\n",
      "Epoch: 11 - 00400/00647 - Loss: 0.05749. [ 62 s]\n",
      "Epoch: 11 - 00420/00647 - Loss: 0.05332. [ 65 s]\n",
      "Epoch: 11 - 00440/00647 - Loss: 0.07445. [ 68 s]\n",
      "Epoch: 11 - 00460/00647 - Loss: 0.03252. [ 71 s]\n",
      "Epoch: 11 - 00480/00647 - Loss: 0.12178. [ 74 s]\n",
      "Epoch: 11 - 00500/00647 - Loss: 0.10513. [ 77 s]\n",
      "Epoch: 11 - 00520/00647 - Loss: 0.04498. [ 80 s]\n",
      "Epoch: 11 - 00540/00647 - Loss: 0.06692. [ 84 s]\n",
      "Epoch: 11 - 00560/00647 - Loss: 0.03440. [ 87 s]\n",
      "Epoch: 11 - 00580/00647 - Loss: 0.09449. [ 90 s]\n",
      "Epoch: 11 - 00600/00647 - Loss: 0.07905. [ 93 s]\n",
      "Epoch: 11 - 00620/00647 - Loss: 0.11104. [ 96 s]\n",
      "Epoch: 11 - 00640/00647 - Loss: 0.12035. [ 99 s]\n",
      "Epoch: 11 - loss(trn/val):0.05928/0.13316, acc(val):95.28%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 12 - 00020/00647 - Loss: 0.07243. [  3 s]\n",
      "Epoch: 12 - 00040/00647 - Loss: 0.08106. [  6 s]\n",
      "Epoch: 12 - 00060/00647 - Loss: 0.03192. [  9 s]\n",
      "Epoch: 12 - 00080/00647 - Loss: 0.06281. [ 12 s]\n",
      "Epoch: 12 - 00100/00647 - Loss: 0.06738. [ 15 s]\n",
      "Epoch: 12 - 00120/00647 - Loss: 0.04728. [ 18 s]\n",
      "Epoch: 12 - 00140/00647 - Loss: 0.05048. [ 21 s]\n",
      "Epoch: 12 - 00160/00647 - Loss: 0.08379. [ 24 s]\n",
      "Epoch: 12 - 00180/00647 - Loss: 0.15426. [ 28 s]\n",
      "Epoch: 12 - 00200/00647 - Loss: 0.04139. [ 31 s]\n",
      "Epoch: 12 - 00220/00647 - Loss: 0.54780. [ 34 s]\n",
      "Epoch: 12 - 00240/00647 - Loss: 0.07566. [ 37 s]\n",
      "Epoch: 12 - 00260/00647 - Loss: 0.04894. [ 40 s]\n",
      "Epoch: 12 - 00280/00647 - Loss: 0.02736. [ 43 s]\n",
      "Epoch: 12 - 00300/00647 - Loss: 0.02550. [ 46 s]\n",
      "Epoch: 12 - 00320/00647 - Loss: 0.03975. [ 49 s]\n",
      "Epoch: 12 - 00340/00647 - Loss: 0.06933. [ 52 s]\n",
      "Epoch: 12 - 00360/00647 - Loss: 0.06573. [ 55 s]\n",
      "Epoch: 12 - 00380/00647 - Loss: 0.06059. [ 59 s]\n",
      "Epoch: 12 - 00400/00647 - Loss: 0.07769. [ 62 s]\n",
      "Epoch: 12 - 00420/00647 - Loss: 0.07976. [ 65 s]\n",
      "Epoch: 12 - 00440/00647 - Loss: 0.26058. [ 68 s]\n",
      "Epoch: 12 - 00460/00647 - Loss: 0.09149. [ 71 s]\n",
      "Epoch: 12 - 00480/00647 - Loss: 0.08933. [ 74 s]\n",
      "Epoch: 12 - 00500/00647 - Loss: 0.03746. [ 77 s]\n",
      "Epoch: 12 - 00520/00647 - Loss: 0.05878. [ 80 s]\n",
      "Epoch: 12 - 00540/00647 - Loss: 0.05506. [ 84 s]\n",
      "Epoch: 12 - 00560/00647 - Loss: 0.04653. [ 87 s]\n",
      "Epoch: 12 - 00580/00647 - Loss: 0.07829. [ 90 s]\n",
      "Epoch: 12 - 00600/00647 - Loss: 0.12389. [ 93 s]\n",
      "Epoch: 12 - 00620/00647 - Loss: 0.20247. [ 96 s]\n",
      "Epoch: 12 - 00640/00647 - Loss: 0.06549. [ 99 s]\n",
      "Epoch: 12 - loss(trn/val):0.06227/0.12905, acc(val):95.13%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 13 - 00020/00647 - Loss: 0.05133. [  3 s]\n",
      "Epoch: 13 - 00040/00647 - Loss: 0.08849. [  6 s]\n",
      "Epoch: 13 - 00060/00647 - Loss: 0.07454. [  9 s]\n",
      "Epoch: 13 - 00080/00647 - Loss: 0.02498. [ 12 s]\n",
      "Epoch: 13 - 00100/00647 - Loss: 0.08030. [ 15 s]\n",
      "Epoch: 13 - 00120/00647 - Loss: 0.22235. [ 18 s]\n",
      "Epoch: 13 - 00140/00647 - Loss: 0.03909. [ 21 s]\n",
      "Epoch: 13 - 00160/00647 - Loss: 0.03476. [ 25 s]\n",
      "Epoch: 13 - 00180/00647 - Loss: 0.12886. [ 28 s]\n",
      "Epoch: 13 - 00200/00647 - Loss: 0.04409. [ 31 s]\n",
      "Epoch: 13 - 00220/00647 - Loss: 0.04231. [ 34 s]\n",
      "Epoch: 13 - 00240/00647 - Loss: 0.06103. [ 37 s]\n",
      "Epoch: 13 - 00260/00647 - Loss: 0.06655. [ 40 s]\n",
      "Epoch: 13 - 00280/00647 - Loss: 0.03877. [ 43 s]\n",
      "Epoch: 13 - 00300/00647 - Loss: 0.05143. [ 46 s]\n",
      "Epoch: 13 - 00320/00647 - Loss: 0.03372. [ 49 s]\n",
      "Epoch: 13 - 00340/00647 - Loss: 0.03969. [ 53 s]\n",
      "Epoch: 13 - 00360/00647 - Loss: 0.09048. [ 56 s]\n",
      "Epoch: 13 - 00380/00647 - Loss: 0.02168. [ 59 s]\n",
      "Epoch: 13 - 00400/00647 - Loss: 0.12336. [ 62 s]\n",
      "Epoch: 13 - 00420/00647 - Loss: 0.10066. [ 65 s]\n",
      "Epoch: 13 - 00440/00647 - Loss: 0.07921. [ 68 s]\n",
      "Epoch: 13 - 00460/00647 - Loss: 0.07635. [ 71 s]\n",
      "Epoch: 13 - 00480/00647 - Loss: 0.07022. [ 74 s]\n",
      "Epoch: 13 - 00500/00647 - Loss: 0.10022. [ 77 s]\n",
      "Epoch: 13 - 00520/00647 - Loss: 0.07336. [ 81 s]\n",
      "Epoch: 13 - 00540/00647 - Loss: 0.10228. [ 84 s]\n",
      "Epoch: 13 - 00560/00647 - Loss: 0.07076. [ 87 s]\n",
      "Epoch: 13 - 00580/00647 - Loss: 0.04353. [ 90 s]\n",
      "Epoch: 13 - 00600/00647 - Loss: 0.07227. [ 93 s]\n",
      "Epoch: 13 - 00620/00647 - Loss: 0.04331. [ 96 s]\n",
      "Epoch: 13 - 00640/00647 - Loss: 0.02830. [ 99 s]\n",
      "Epoch: 13 - loss(trn/val):0.05437/0.14666, acc(val):95.09%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 14 - 00020/00647 - Loss: 0.03929. [  3 s]\n",
      "Epoch: 14 - 00040/00647 - Loss: 0.04342. [  6 s]\n",
      "Epoch: 14 - 00060/00647 - Loss: 0.05145. [  9 s]\n",
      "Epoch: 14 - 00080/00647 - Loss: 0.05511. [ 12 s]\n",
      "Epoch: 14 - 00100/00647 - Loss: 0.07492. [ 15 s]\n",
      "Epoch: 14 - 00120/00647 - Loss: 0.06425. [ 18 s]\n",
      "Epoch: 14 - 00140/00647 - Loss: 0.02601. [ 21 s]\n",
      "Epoch: 14 - 00160/00647 - Loss: 0.10242. [ 25 s]\n",
      "Epoch: 14 - 00180/00647 - Loss: 0.08739. [ 28 s]\n",
      "Epoch: 14 - 00200/00647 - Loss: 0.06090. [ 31 s]\n",
      "Epoch: 14 - 00220/00647 - Loss: 0.03062. [ 34 s]\n",
      "Epoch: 14 - 00240/00647 - Loss: 0.06910. [ 37 s]\n",
      "Epoch: 14 - 00260/00647 - Loss: 0.02419. [ 40 s]\n",
      "Epoch: 14 - 00280/00647 - Loss: 0.05397. [ 43 s]\n",
      "Epoch: 14 - 00300/00647 - Loss: 0.03160. [ 46 s]\n",
      "Epoch: 14 - 00320/00647 - Loss: 0.10805. [ 49 s]\n",
      "Epoch: 14 - 00340/00647 - Loss: 0.04153. [ 52 s]\n",
      "Epoch: 14 - 00360/00647 - Loss: 0.05667. [ 55 s]\n",
      "Epoch: 14 - 00380/00647 - Loss: 0.06353. [ 59 s]\n",
      "Epoch: 14 - 00400/00647 - Loss: 0.05594. [ 62 s]\n",
      "Epoch: 14 - 00420/00647 - Loss: 0.07110. [ 65 s]\n",
      "Epoch: 14 - 00440/00647 - Loss: 0.06758. [ 68 s]\n",
      "Epoch: 14 - 00460/00647 - Loss: 0.04005. [ 71 s]\n",
      "Epoch: 14 - 00480/00647 - Loss: 0.04708. [ 74 s]\n",
      "Epoch: 14 - 00500/00647 - Loss: 0.05636. [ 77 s]\n",
      "Epoch: 14 - 00520/00647 - Loss: 0.05784. [ 80 s]\n",
      "Epoch: 14 - 00540/00647 - Loss: 0.06598. [ 84 s]\n",
      "Epoch: 14 - 00560/00647 - Loss: 0.04161. [ 87 s]\n",
      "Epoch: 14 - 00580/00647 - Loss: 0.04001. [ 90 s]\n",
      "Epoch: 14 - 00600/00647 - Loss: 0.07911. [ 93 s]\n",
      "Epoch: 14 - 00620/00647 - Loss: 0.12923. [ 96 s]\n",
      "Epoch: 14 - 00640/00647 - Loss: 0.09758. [ 99 s]\n",
      "Epoch: 14 - loss(trn/val):0.06349/0.14379, acc(val):95.22%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 15 - 00020/00647 - Loss: 0.11371. [  3 s]\n",
      "Epoch: 15 - 00040/00647 - Loss: 0.03732. [  6 s]\n",
      "Epoch: 15 - 00060/00647 - Loss: 0.12799. [  9 s]\n",
      "Epoch: 15 - 00080/00647 - Loss: 0.08018. [ 12 s]\n",
      "Epoch: 15 - 00100/00647 - Loss: 0.05470. [ 15 s]\n",
      "Epoch: 15 - 00120/00647 - Loss: 0.18690. [ 18 s]\n",
      "Epoch: 15 - 00140/00647 - Loss: 0.05509. [ 21 s]\n",
      "Epoch: 15 - 00160/00647 - Loss: 0.03742. [ 24 s]\n",
      "Epoch: 15 - 00180/00647 - Loss: 0.08056. [ 28 s]\n",
      "Epoch: 15 - 00200/00647 - Loss: 0.05117. [ 31 s]\n",
      "Epoch: 15 - 00220/00647 - Loss: 0.02983. [ 34 s]\n",
      "Epoch: 15 - 00240/00647 - Loss: 0.07426. [ 37 s]\n",
      "Epoch: 15 - 00260/00647 - Loss: 0.04159. [ 40 s]\n",
      "Epoch: 15 - 00280/00647 - Loss: 0.04469. [ 43 s]\n",
      "Epoch: 15 - 00300/00647 - Loss: 0.05055. [ 46 s]\n",
      "Epoch: 15 - 00320/00647 - Loss: 0.03211. [ 49 s]\n",
      "Epoch: 15 - 00340/00647 - Loss: 0.06501. [ 52 s]\n",
      "Epoch: 15 - 00360/00647 - Loss: 0.03510. [ 56 s]\n",
      "Epoch: 15 - 00380/00647 - Loss: 0.03454. [ 59 s]\n",
      "Epoch: 15 - 00400/00647 - Loss: 0.05401. [ 62 s]\n",
      "Epoch: 15 - 00420/00647 - Loss: 0.08195. [ 65 s]\n",
      "Epoch: 15 - 00440/00647 - Loss: 0.06040. [ 68 s]\n",
      "Epoch: 15 - 00460/00647 - Loss: 0.05915. [ 71 s]\n",
      "Epoch: 15 - 00480/00647 - Loss: 0.07753. [ 74 s]\n",
      "Epoch: 15 - 00500/00647 - Loss: 0.04709. [ 77 s]\n",
      "Epoch: 15 - 00520/00647 - Loss: 0.02992. [ 81 s]\n",
      "Epoch: 15 - 00540/00647 - Loss: 0.05617. [ 84 s]\n",
      "Epoch: 15 - 00560/00647 - Loss: 0.05651. [ 87 s]\n",
      "Epoch: 15 - 00580/00647 - Loss: 0.06617. [ 90 s]\n",
      "Epoch: 15 - 00600/00647 - Loss: 0.05642. [ 93 s]\n",
      "Epoch: 15 - 00620/00647 - Loss: 0.06828. [ 96 s]\n",
      "Epoch: 15 - 00640/00647 - Loss: 0.02555. [ 99 s]\n",
      "Epoch: 15 - loss(trn/val):0.05406/0.16997, acc(val):94.38%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 16 - 00020/00647 - Loss: 0.03512. [  3 s]\n",
      "Epoch: 16 - 00040/00647 - Loss: 0.05299. [  6 s]\n",
      "Epoch: 16 - 00060/00647 - Loss: 0.04651. [  9 s]\n",
      "Epoch: 16 - 00080/00647 - Loss: 0.03081. [ 12 s]\n",
      "Epoch: 16 - 00100/00647 - Loss: 0.03660. [ 15 s]\n",
      "Epoch: 16 - 00120/00647 - Loss: 0.07651. [ 18 s]\n",
      "Epoch: 16 - 00140/00647 - Loss: 0.05678. [ 21 s]\n",
      "Epoch: 16 - 00160/00647 - Loss: 0.07031. [ 24 s]\n",
      "Epoch: 16 - 00180/00647 - Loss: 0.02930. [ 28 s]\n",
      "Epoch: 16 - 00200/00647 - Loss: 0.02474. [ 31 s]\n",
      "Epoch: 16 - 00220/00647 - Loss: 0.05031. [ 34 s]\n",
      "Epoch: 16 - 00240/00647 - Loss: 0.01663. [ 37 s]\n",
      "Epoch: 16 - 00260/00647 - Loss: 0.03409. [ 40 s]\n",
      "Epoch: 16 - 00280/00647 - Loss: 0.03732. [ 43 s]\n",
      "Epoch: 16 - 00300/00647 - Loss: 0.04412. [ 46 s]\n",
      "Epoch: 16 - 00320/00647 - Loss: 0.06861. [ 49 s]\n",
      "Epoch: 16 - 00340/00647 - Loss: 0.05075. [ 53 s]\n",
      "Epoch: 16 - 00360/00647 - Loss: 0.08485. [ 56 s]\n",
      "Epoch: 16 - 00380/00647 - Loss: 0.08475. [ 59 s]\n",
      "Epoch: 16 - 00400/00647 - Loss: 0.21498. [ 62 s]\n",
      "Epoch: 16 - 00420/00647 - Loss: 0.02350. [ 65 s]\n",
      "Epoch: 16 - 00440/00647 - Loss: 0.10874. [ 68 s]\n",
      "Epoch: 16 - 00460/00647 - Loss: 0.04349. [ 71 s]\n",
      "Epoch: 16 - 00480/00647 - Loss: 0.09358. [ 74 s]\n",
      "Epoch: 16 - 00500/00647 - Loss: 0.05875. [ 77 s]\n",
      "Epoch: 16 - 00520/00647 - Loss: 0.07108. [ 81 s]\n",
      "Epoch: 16 - 00540/00647 - Loss: 0.05096. [ 84 s]\n",
      "Epoch: 16 - 00560/00647 - Loss: 0.14188. [ 87 s]\n",
      "Epoch: 16 - 00580/00647 - Loss: 0.05580. [ 90 s]\n",
      "Epoch: 16 - 00600/00647 - Loss: 0.04482. [ 93 s]\n",
      "Epoch: 16 - 00620/00647 - Loss: 0.04092. [ 96 s]\n",
      "Epoch: 16 - 00640/00647 - Loss: 0.03380. [ 99 s]\n",
      "Epoch: 16 - loss(trn/val):0.04647/0.13079, acc(val):95.63%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 17 - 00020/00647 - Loss: 0.02763. [  3 s]\n",
      "Epoch: 17 - 00040/00647 - Loss: 0.03344. [  6 s]\n",
      "Epoch: 17 - 00060/00647 - Loss: 0.09850. [  9 s]\n",
      "Epoch: 17 - 00080/00647 - Loss: 0.03269. [ 12 s]\n",
      "Epoch: 17 - 00100/00647 - Loss: 0.07568. [ 15 s]\n",
      "Epoch: 17 - 00120/00647 - Loss: 0.07663. [ 18 s]\n",
      "Epoch: 17 - 00140/00647 - Loss: 0.07444. [ 21 s]\n",
      "Epoch: 17 - 00160/00647 - Loss: 0.02641. [ 25 s]\n",
      "Epoch: 17 - 00180/00647 - Loss: 0.03156. [ 28 s]\n",
      "Epoch: 17 - 00200/00647 - Loss: 0.08635. [ 31 s]\n",
      "Epoch: 17 - 00220/00647 - Loss: 0.05637. [ 34 s]\n",
      "Epoch: 17 - 00240/00647 - Loss: 0.03600. [ 37 s]\n",
      "Epoch: 17 - 00260/00647 - Loss: 0.04615. [ 40 s]\n",
      "Epoch: 17 - 00280/00647 - Loss: 0.09991. [ 43 s]\n",
      "Epoch: 17 - 00300/00647 - Loss: 0.04266. [ 46 s]\n",
      "Epoch: 17 - 00320/00647 - Loss: 0.04201. [ 49 s]\n",
      "Epoch: 17 - 00340/00647 - Loss: 0.05383. [ 52 s]\n",
      "Epoch: 17 - 00360/00647 - Loss: 0.03794. [ 56 s]\n",
      "Epoch: 17 - 00380/00647 - Loss: 0.05893. [ 59 s]\n",
      "Epoch: 17 - 00400/00647 - Loss: 0.08400. [ 62 s]\n",
      "Epoch: 17 - 00420/00647 - Loss: 0.03825. [ 65 s]\n",
      "Epoch: 17 - 00440/00647 - Loss: 0.06354. [ 68 s]\n",
      "Epoch: 17 - 00460/00647 - Loss: 0.06249. [ 71 s]\n",
      "Epoch: 17 - 00480/00647 - Loss: 0.07395. [ 74 s]\n",
      "Epoch: 17 - 00500/00647 - Loss: 0.05373. [ 77 s]\n",
      "Epoch: 17 - 00520/00647 - Loss: 0.07814. [ 80 s]\n",
      "Epoch: 17 - 00540/00647 - Loss: 0.03516. [ 84 s]\n",
      "Epoch: 17 - 00560/00647 - Loss: 0.09691. [ 87 s]\n",
      "Epoch: 17 - 00580/00647 - Loss: 0.05898. [ 90 s]\n",
      "Epoch: 17 - 00600/00647 - Loss: 0.06120. [ 93 s]\n",
      "Epoch: 17 - 00620/00647 - Loss: 0.03085. [ 96 s]\n",
      "Epoch: 17 - 00640/00647 - Loss: 0.05372. [ 99 s]\n",
      "Epoch: 17 - loss(trn/val):0.04674/0.19063, acc(val):94.24%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 18 - 00020/00647 - Loss: 0.04792. [  3 s]\n",
      "Epoch: 18 - 00040/00647 - Loss: 0.04585. [  6 s]\n",
      "Epoch: 18 - 00060/00647 - Loss: 0.03842. [  9 s]\n",
      "Epoch: 18 - 00080/00647 - Loss: 0.07742. [ 12 s]\n",
      "Epoch: 18 - 00100/00647 - Loss: 0.18230. [ 15 s]\n",
      "Epoch: 18 - 00120/00647 - Loss: 0.03378. [ 18 s]\n",
      "Epoch: 18 - 00140/00647 - Loss: 0.05634. [ 21 s]\n",
      "Epoch: 18 - 00160/00647 - Loss: 0.05069. [ 24 s]\n",
      "Epoch: 18 - 00180/00647 - Loss: 0.09377. [ 27 s]\n",
      "Epoch: 18 - 00200/00647 - Loss: 0.08018. [ 31 s]\n",
      "Epoch: 18 - 00220/00647 - Loss: 0.02805. [ 34 s]\n",
      "Epoch: 18 - 00240/00647 - Loss: 0.09111. [ 37 s]\n",
      "Epoch: 18 - 00260/00647 - Loss: 0.03438. [ 40 s]\n",
      "Epoch: 18 - 00280/00647 - Loss: 0.05413. [ 43 s]\n",
      "Epoch: 18 - 00300/00647 - Loss: 0.04153. [ 46 s]\n",
      "Epoch: 18 - 00320/00647 - Loss: 0.02841. [ 49 s]\n",
      "Epoch: 18 - 00340/00647 - Loss: 0.03671. [ 52 s]\n",
      "Epoch: 18 - 00360/00647 - Loss: 0.04802. [ 56 s]\n",
      "Epoch: 18 - 00380/00647 - Loss: 0.11219. [ 59 s]\n",
      "Epoch: 18 - 00400/00647 - Loss: 0.08321. [ 62 s]\n",
      "Epoch: 18 - 00420/00647 - Loss: 0.06406. [ 65 s]\n",
      "Epoch: 18 - 00440/00647 - Loss: 0.05839. [ 68 s]\n",
      "Epoch: 18 - 00460/00647 - Loss: 0.07823. [ 71 s]\n",
      "Epoch: 18 - 00480/00647 - Loss: 0.02592. [ 74 s]\n",
      "Epoch: 18 - 00500/00647 - Loss: 0.07553. [ 77 s]\n",
      "Epoch: 18 - 00520/00647 - Loss: 0.04113. [ 80 s]\n",
      "Epoch: 18 - 00540/00647 - Loss: 0.01412. [ 84 s]\n",
      "Epoch: 18 - 00560/00647 - Loss: 0.04234. [ 87 s]\n",
      "Epoch: 18 - 00580/00647 - Loss: 0.08891. [ 90 s]\n",
      "Epoch: 18 - 00600/00647 - Loss: 0.06820. [ 93 s]\n",
      "Epoch: 18 - 00620/00647 - Loss: 0.06242. [ 96 s]\n",
      "Epoch: 18 - 00640/00647 - Loss: 0.07330. [ 99 s]\n",
      "Epoch: 18 - loss(trn/val):0.04850/0.14568, acc(val):95.13%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 19 - 00020/00647 - Loss: 0.07105. [  3 s]\n",
      "Epoch: 19 - 00040/00647 - Loss: 0.07081. [  6 s]\n",
      "Epoch: 19 - 00060/00647 - Loss: 0.08226. [  9 s]\n",
      "Epoch: 19 - 00080/00647 - Loss: 0.05709. [ 12 s]\n",
      "Epoch: 19 - 00100/00647 - Loss: 0.03148. [ 15 s]\n",
      "Epoch: 19 - 00120/00647 - Loss: 0.02872. [ 18 s]\n",
      "Epoch: 19 - 00140/00647 - Loss: 0.04234. [ 21 s]\n",
      "Epoch: 19 - 00160/00647 - Loss: 0.03229. [ 24 s]\n",
      "Epoch: 19 - 00180/00647 - Loss: 0.06631. [ 27 s]\n",
      "Epoch: 19 - 00200/00647 - Loss: 0.03905. [ 31 s]\n",
      "Epoch: 19 - 00220/00647 - Loss: 0.03341. [ 34 s]\n",
      "Epoch: 19 - 00240/00647 - Loss: 0.03255. [ 37 s]\n",
      "Epoch: 19 - 00260/00647 - Loss: 0.03242. [ 40 s]\n",
      "Epoch: 19 - 00280/00647 - Loss: 0.03834. [ 43 s]\n",
      "Epoch: 19 - 00300/00647 - Loss: 0.02710. [ 46 s]\n",
      "Epoch: 19 - 00320/00647 - Loss: 0.04224. [ 49 s]\n",
      "Epoch: 19 - 00340/00647 - Loss: 0.10639. [ 52 s]\n",
      "Epoch: 19 - 00360/00647 - Loss: 0.04226. [ 55 s]\n",
      "Epoch: 19 - 00380/00647 - Loss: 0.04643. [ 58 s]\n",
      "Epoch: 19 - 00400/00647 - Loss: 0.03381. [ 62 s]\n",
      "Epoch: 19 - 00420/00647 - Loss: 0.06582. [ 65 s]\n",
      "Epoch: 19 - 00440/00647 - Loss: 0.03889. [ 68 s]\n",
      "Epoch: 19 - 00460/00647 - Loss: 0.03589. [ 71 s]\n",
      "Epoch: 19 - 00480/00647 - Loss: 0.03386. [ 74 s]\n",
      "Epoch: 19 - 00500/00647 - Loss: 0.04510. [ 77 s]\n",
      "Epoch: 19 - 00520/00647 - Loss: 0.02297. [ 80 s]\n",
      "Epoch: 19 - 00540/00647 - Loss: 0.02242. [ 83 s]\n",
      "Epoch: 19 - 00560/00647 - Loss: 0.05620. [ 86 s]\n",
      "Epoch: 19 - 00580/00647 - Loss: 0.17155. [ 89 s]\n",
      "Epoch: 19 - 00600/00647 - Loss: 0.04579. [ 93 s]\n",
      "Epoch: 19 - 00620/00647 - Loss: 0.03594. [ 96 s]\n",
      "Epoch: 19 - 00640/00647 - Loss: 0.06625. [ 99 s]\n",
      "Epoch: 19 - loss(trn/val):0.04941/0.12843, acc(val):95.72%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 20 - 00020/00647 - Loss: 0.04271. [  3 s]\n",
      "Epoch: 20 - 00040/00647 - Loss: 0.04078. [  6 s]\n",
      "Epoch: 20 - 00060/00647 - Loss: 0.04256. [  9 s]\n",
      "Epoch: 20 - 00080/00647 - Loss: 0.04430. [ 12 s]\n",
      "Epoch: 20 - 00100/00647 - Loss: 0.03163. [ 15 s]\n",
      "Epoch: 20 - 00120/00647 - Loss: 0.09851. [ 18 s]\n",
      "Epoch: 20 - 00140/00647 - Loss: 0.06284. [ 21 s]\n",
      "Epoch: 20 - 00160/00647 - Loss: 0.04489. [ 24 s]\n",
      "Epoch: 20 - 00180/00647 - Loss: 0.04060. [ 27 s]\n",
      "Epoch: 20 - 00200/00647 - Loss: 0.02625. [ 31 s]\n",
      "Epoch: 20 - 00220/00647 - Loss: 0.04396. [ 34 s]\n",
      "Epoch: 20 - 00240/00647 - Loss: 0.03233. [ 37 s]\n",
      "Epoch: 20 - 00260/00647 - Loss: 0.05284. [ 40 s]\n",
      "Epoch: 20 - 00280/00647 - Loss: 0.04067. [ 43 s]\n",
      "Epoch: 20 - 00300/00647 - Loss: 0.04617. [ 46 s]\n",
      "Epoch: 20 - 00320/00647 - Loss: 0.36938. [ 49 s]\n",
      "Epoch: 20 - 00340/00647 - Loss: 0.02828. [ 52 s]\n",
      "Epoch: 20 - 00360/00647 - Loss: 0.03547. [ 55 s]\n",
      "Epoch: 20 - 00380/00647 - Loss: 0.04935. [ 59 s]\n",
      "Epoch: 20 - 00400/00647 - Loss: 0.02715. [ 62 s]\n",
      "Epoch: 20 - 00420/00647 - Loss: 0.09626. [ 65 s]\n",
      "Epoch: 20 - 00440/00647 - Loss: 0.07973. [ 68 s]\n",
      "Epoch: 20 - 00460/00647 - Loss: 0.04607. [ 71 s]\n",
      "Epoch: 20 - 00480/00647 - Loss: 0.08779. [ 74 s]\n",
      "Epoch: 20 - 00500/00647 - Loss: 0.04546. [ 77 s]\n",
      "Epoch: 20 - 00520/00647 - Loss: 0.03742. [ 80 s]\n",
      "Epoch: 20 - 00540/00647 - Loss: 0.02001. [ 83 s]\n",
      "Epoch: 20 - 00560/00647 - Loss: 0.02838. [ 86 s]\n",
      "Epoch: 20 - 00580/00647 - Loss: 0.04371. [ 90 s]\n",
      "Epoch: 20 - 00600/00647 - Loss: 0.06327. [ 93 s]\n",
      "Epoch: 20 - 00620/00647 - Loss: 0.03355. [ 96 s]\n",
      "Epoch: 20 - 00640/00647 - Loss: 0.03274. [ 99 s]\n",
      "Epoch: 20 - loss(trn/val):0.04340/0.13498, acc(val):95.94%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 21 - 00020/00647 - Loss: 0.03963. [  3 s]\n",
      "Epoch: 21 - 00040/00647 - Loss: 0.03100. [  6 s]\n",
      "Epoch: 21 - 00060/00647 - Loss: 0.05716. [  9 s]\n",
      "Epoch: 21 - 00080/00647 - Loss: 0.03520. [ 12 s]\n",
      "Epoch: 21 - 00100/00647 - Loss: 0.01564. [ 15 s]\n",
      "Epoch: 21 - 00120/00647 - Loss: 0.45224. [ 18 s]\n",
      "Epoch: 21 - 00140/00647 - Loss: 0.18605. [ 21 s]\n",
      "Epoch: 21 - 00160/00647 - Loss: 0.04186. [ 24 s]\n",
      "Epoch: 21 - 00180/00647 - Loss: 0.07500. [ 27 s]\n",
      "Epoch: 21 - 00200/00647 - Loss: 0.04005. [ 30 s]\n",
      "Epoch: 21 - 00220/00647 - Loss: 0.04188. [ 34 s]\n",
      "Epoch: 21 - 00240/00647 - Loss: 0.03323. [ 37 s]\n",
      "Epoch: 21 - 00260/00647 - Loss: 0.07160. [ 40 s]\n",
      "Epoch: 21 - 00280/00647 - Loss: 0.07147. [ 43 s]\n",
      "Epoch: 21 - 00300/00647 - Loss: 0.03873. [ 46 s]\n",
      "Epoch: 21 - 00320/00647 - Loss: 0.04564. [ 49 s]\n",
      "Epoch: 21 - 00340/00647 - Loss: 0.05788. [ 52 s]\n",
      "Epoch: 21 - 00360/00647 - Loss: 0.06730. [ 55 s]\n",
      "Epoch: 21 - 00380/00647 - Loss: 0.03688. [ 58 s]\n",
      "Epoch: 21 - 00400/00647 - Loss: 0.03511. [ 61 s]\n",
      "Epoch: 21 - 00420/00647 - Loss: 0.02844. [ 64 s]\n",
      "Epoch: 21 - 00440/00647 - Loss: 0.16948. [ 67 s]\n",
      "Epoch: 21 - 00460/00647 - Loss: 0.29516. [ 70 s]\n",
      "Epoch: 21 - 00480/00647 - Loss: 0.03094. [ 74 s]\n",
      "Epoch: 21 - 00500/00647 - Loss: 0.04142. [ 77 s]\n",
      "Epoch: 21 - 00520/00647 - Loss: 0.02539. [ 80 s]\n",
      "Epoch: 21 - 00540/00647 - Loss: 0.05168. [ 83 s]\n",
      "Epoch: 21 - 00560/00647 - Loss: 0.01559. [ 86 s]\n",
      "Epoch: 21 - 00580/00647 - Loss: 0.01577. [ 89 s]\n",
      "Epoch: 21 - 00600/00647 - Loss: 0.03055. [ 92 s]\n",
      "Epoch: 21 - 00620/00647 - Loss: 0.05388. [ 95 s]\n",
      "Epoch: 21 - 00640/00647 - Loss: 0.01675. [ 98 s]\n",
      "Epoch: 21 - loss(trn/val):0.04383/0.15264, acc(val):95.38%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 22 - 00020/00647 - Loss: 0.04557. [  3 s]\n",
      "Epoch: 22 - 00040/00647 - Loss: 0.02605. [  6 s]\n",
      "Epoch: 22 - 00060/00647 - Loss: 0.06631. [  9 s]\n",
      "Epoch: 22 - 00080/00647 - Loss: 0.04418. [ 12 s]\n",
      "Epoch: 22 - 00100/00647 - Loss: 0.06069. [ 15 s]\n",
      "Epoch: 22 - 00120/00647 - Loss: 0.05279. [ 18 s]\n",
      "Epoch: 22 - 00140/00647 - Loss: 0.01505. [ 21 s]\n",
      "Epoch: 22 - 00160/00647 - Loss: 0.02683. [ 24 s]\n",
      "Epoch: 22 - 00180/00647 - Loss: 0.03632. [ 27 s]\n",
      "Epoch: 22 - 00200/00647 - Loss: 0.09021. [ 31 s]\n",
      "Epoch: 22 - 00220/00647 - Loss: 0.04037. [ 34 s]\n",
      "Epoch: 22 - 00240/00647 - Loss: 0.03418. [ 37 s]\n",
      "Epoch: 22 - 00260/00647 - Loss: 0.11882. [ 40 s]\n",
      "Epoch: 22 - 00280/00647 - Loss: 0.02900. [ 43 s]\n",
      "Epoch: 22 - 00300/00647 - Loss: 0.02694. [ 46 s]\n",
      "Epoch: 22 - 00320/00647 - Loss: 0.04825. [ 49 s]\n",
      "Epoch: 22 - 00340/00647 - Loss: 0.06788. [ 52 s]\n",
      "Epoch: 22 - 00360/00647 - Loss: 0.02335. [ 55 s]\n",
      "Epoch: 22 - 00380/00647 - Loss: 0.03272. [ 58 s]\n",
      "Epoch: 22 - 00400/00647 - Loss: 0.04807. [ 61 s]\n",
      "Epoch: 22 - 00420/00647 - Loss: 0.02366. [ 65 s]\n",
      "Epoch: 22 - 00440/00647 - Loss: 0.04402. [ 68 s]\n",
      "Epoch: 22 - 00460/00647 - Loss: 0.03087. [ 71 s]\n",
      "Epoch: 22 - 00480/00647 - Loss: 0.06199. [ 74 s]\n",
      "Epoch: 22 - 00500/00647 - Loss: 0.05040. [ 77 s]\n",
      "Epoch: 22 - 00520/00647 - Loss: 0.03560. [ 80 s]\n",
      "Epoch: 22 - 00540/00647 - Loss: 0.04354. [ 83 s]\n",
      "Epoch: 22 - 00560/00647 - Loss: 0.05713. [ 86 s]\n",
      "Epoch: 22 - 00580/00647 - Loss: 0.05238. [ 89 s]\n",
      "Epoch: 22 - 00600/00647 - Loss: 0.05449. [ 93 s]\n",
      "Epoch: 22 - 00620/00647 - Loss: 0.03305. [ 96 s]\n",
      "Epoch: 22 - 00640/00647 - Loss: 0.01993. [ 99 s]\n",
      "Epoch: 22 - loss(trn/val):0.05557/0.21296, acc(val):93.63%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 23 - 00020/00647 - Loss: 0.04222. [  3 s]\n",
      "Epoch: 23 - 00040/00647 - Loss: 0.04355. [  6 s]\n",
      "Epoch: 23 - 00060/00647 - Loss: 0.01871. [  9 s]\n",
      "Epoch: 23 - 00080/00647 - Loss: 0.02775. [ 12 s]\n",
      "Epoch: 23 - 00100/00647 - Loss: 0.02165. [ 15 s]\n",
      "Epoch: 23 - 00120/00647 - Loss: 0.03675. [ 19 s]\n",
      "Epoch: 23 - 00140/00647 - Loss: 0.04435. [ 22 s]\n",
      "Epoch: 23 - 00160/00647 - Loss: 0.03276. [ 25 s]\n",
      "Epoch: 23 - 00180/00647 - Loss: 0.04686. [ 28 s]\n",
      "Epoch: 23 - 00200/00647 - Loss: 0.03661. [ 31 s]\n",
      "Epoch: 23 - 00220/00647 - Loss: 0.03672. [ 34 s]\n",
      "Epoch: 23 - 00240/00647 - Loss: 0.05156. [ 37 s]\n",
      "Epoch: 23 - 00260/00647 - Loss: 0.05060. [ 40 s]\n",
      "Epoch: 23 - 00280/00647 - Loss: 0.02874. [ 44 s]\n",
      "Epoch: 23 - 00300/00647 - Loss: 0.05402. [ 47 s]\n",
      "Epoch: 23 - 00320/00647 - Loss: 0.05084. [ 50 s]\n",
      "Epoch: 23 - 00340/00647 - Loss: 0.04533. [ 53 s]\n",
      "Epoch: 23 - 00360/00647 - Loss: 0.04002. [ 56 s]\n",
      "Epoch: 23 - 00380/00647 - Loss: 0.04469. [ 59 s]\n",
      "Epoch: 23 - 00400/00647 - Loss: 0.05011. [ 63 s]\n",
      "Epoch: 23 - 00420/00647 - Loss: 0.04327. [ 66 s]\n",
      "Epoch: 23 - 00440/00647 - Loss: 0.04134. [ 69 s]\n",
      "Epoch: 23 - 00460/00647 - Loss: 0.02985. [ 72 s]\n",
      "Epoch: 23 - 00480/00647 - Loss: 0.02371. [ 75 s]\n",
      "Epoch: 23 - 00500/00647 - Loss: 0.05383. [ 78 s]\n",
      "Epoch: 23 - 00520/00647 - Loss: 0.07658. [ 81 s]\n",
      "Epoch: 23 - 00540/00647 - Loss: 0.03121. [ 84 s]\n",
      "Epoch: 23 - 00560/00647 - Loss: 0.03997. [ 88 s]\n",
      "Epoch: 23 - 00580/00647 - Loss: 0.11217. [ 91 s]\n",
      "Epoch: 23 - 00600/00647 - Loss: 0.03339. [ 94 s]\n",
      "Epoch: 23 - 00620/00647 - Loss: 0.07300. [ 97 s]\n",
      "Epoch: 23 - 00640/00647 - Loss: 0.04860. [100 s]\n",
      "Epoch: 23 - loss(trn/val):0.04374/0.16394, acc(val):94.60%, lr=0.00010. [101s] @25 samples/s \n",
      "Epoch: 24 - 00020/00647 - Loss: 0.04322. [  3 s]\n",
      "Epoch: 24 - 00040/00647 - Loss: 0.04477. [  6 s]\n",
      "Epoch: 24 - 00060/00647 - Loss: 0.03510. [  9 s]\n",
      "Epoch: 24 - 00080/00647 - Loss: 0.03672. [ 12 s]\n",
      "Epoch: 24 - 00100/00647 - Loss: 0.03823. [ 15 s]\n",
      "Epoch: 24 - 00120/00647 - Loss: 0.04802. [ 18 s]\n",
      "Epoch: 24 - 00140/00647 - Loss: 0.04319. [ 22 s]\n",
      "Epoch: 24 - 00160/00647 - Loss: 0.27274. [ 25 s]\n",
      "Epoch: 24 - 00180/00647 - Loss: 0.11203. [ 28 s]\n",
      "Epoch: 24 - 00200/00647 - Loss: 0.13866. [ 31 s]\n",
      "Epoch: 24 - 00220/00647 - Loss: 0.05790. [ 34 s]\n",
      "Epoch: 24 - 00240/00647 - Loss: 0.03881. [ 37 s]\n",
      "Epoch: 24 - 00260/00647 - Loss: 0.03434. [ 40 s]\n",
      "Epoch: 24 - 00280/00647 - Loss: 0.04471. [ 43 s]\n",
      "Epoch: 24 - 00300/00647 - Loss: 0.03678. [ 47 s]\n",
      "Epoch: 24 - 00320/00647 - Loss: 0.02605. [ 50 s]\n",
      "Epoch: 24 - 00340/00647 - Loss: 0.02294. [ 53 s]\n",
      "Epoch: 24 - 00360/00647 - Loss: 0.02076. [ 56 s]\n",
      "Epoch: 24 - 00380/00647 - Loss: 0.02828. [ 59 s]\n",
      "Epoch: 24 - 00400/00647 - Loss: 0.05002. [ 62 s]\n",
      "Epoch: 24 - 00420/00647 - Loss: 0.04936. [ 66 s]\n",
      "Epoch: 24 - 00440/00647 - Loss: 0.03315. [ 69 s]\n",
      "Epoch: 24 - 00460/00647 - Loss: 0.05925. [ 72 s]\n",
      "Epoch: 24 - 00480/00647 - Loss: 0.03546. [ 75 s]\n",
      "Epoch: 24 - 00500/00647 - Loss: 0.06297. [ 78 s]\n",
      "Epoch: 24 - 00520/00647 - Loss: 0.01794. [ 81 s]\n",
      "Epoch: 24 - 00540/00647 - Loss: 0.07664. [ 84 s]\n",
      "Epoch: 24 - 00560/00647 - Loss: 0.02654. [ 88 s]\n",
      "Epoch: 24 - 00580/00647 - Loss: 0.05140. [ 91 s]\n",
      "Epoch: 24 - 00600/00647 - Loss: 0.03069. [ 94 s]\n",
      "Epoch: 24 - 00620/00647 - Loss: 0.02523. [ 97 s]\n",
      "Epoch: 24 - 00640/00647 - Loss: 0.06014. [100 s]\n",
      "Epoch: 24 - loss(trn/val):0.03873/0.19454, acc(val):94.11%, lr=0.00010. [101s] @25 samples/s \n",
      "Epoch: 25 - 00020/00647 - Loss: 0.04228. [  3 s]\n",
      "Epoch: 25 - 00040/00647 - Loss: 0.03713. [  6 s]\n",
      "Epoch: 25 - 00060/00647 - Loss: 0.04216. [  9 s]\n",
      "Epoch: 25 - 00080/00647 - Loss: 0.05888. [ 12 s]\n",
      "Epoch: 25 - 00100/00647 - Loss: 0.03850. [ 15 s]\n",
      "Epoch: 25 - 00120/00647 - Loss: 0.04588. [ 19 s]\n",
      "Epoch: 25 - 00140/00647 - Loss: 0.14251. [ 22 s]\n",
      "Epoch: 25 - 00160/00647 - Loss: 0.06913. [ 25 s]\n",
      "Epoch: 25 - 00180/00647 - Loss: 0.05796. [ 28 s]\n",
      "Epoch: 25 - 00200/00647 - Loss: 0.02183. [ 31 s]\n",
      "Epoch: 25 - 00220/00647 - Loss: 0.01919. [ 34 s]\n",
      "Epoch: 25 - 00240/00647 - Loss: 0.06798. [ 37 s]\n",
      "Epoch: 25 - 00260/00647 - Loss: 0.03897. [ 40 s]\n",
      "Epoch: 25 - 00280/00647 - Loss: 0.04782. [ 43 s]\n",
      "Epoch: 25 - 00300/00647 - Loss: 0.02049. [ 47 s]\n",
      "Epoch: 25 - 00320/00647 - Loss: 0.03256. [ 50 s]\n",
      "Epoch: 25 - 00340/00647 - Loss: 0.09468. [ 53 s]\n",
      "Epoch: 25 - 00360/00647 - Loss: 0.05287. [ 56 s]\n",
      "Epoch: 25 - 00380/00647 - Loss: 0.06545. [ 60 s]\n",
      "Epoch: 25 - 00400/00647 - Loss: 0.04174. [ 63 s]\n",
      "Epoch: 25 - 00420/00647 - Loss: 0.02757. [ 66 s]\n",
      "Epoch: 25 - 00440/00647 - Loss: 0.04911. [ 69 s]\n",
      "Epoch: 25 - 00460/00647 - Loss: 0.07222. [ 72 s]\n",
      "Epoch: 25 - 00480/00647 - Loss: 0.02831. [ 75 s]\n",
      "Epoch: 25 - 00500/00647 - Loss: 0.04537. [ 79 s]\n",
      "Epoch: 25 - 00520/00647 - Loss: 0.01405. [ 82 s]\n",
      "Epoch: 25 - 00540/00647 - Loss: 0.03461. [ 85 s]\n",
      "Epoch: 25 - 00560/00647 - Loss: 0.01862. [ 88 s]\n",
      "Epoch: 25 - 00580/00647 - Loss: 0.09541. [ 91 s]\n",
      "Epoch: 25 - 00600/00647 - Loss: 0.05831. [ 94 s]\n",
      "Epoch: 25 - 00620/00647 - Loss: 0.02694. [ 97 s]\n",
      "Epoch: 25 - 00640/00647 - Loss: 0.02891. [101 s]\n",
      "Epoch: 25 - loss(trn/val):0.03614/0.16443, acc(val):95.12%, lr=0.00010. [102s] @25 samples/s \n",
      "Epoch: 26 - 00020/00647 - Loss: 0.03027. [  3 s]\n",
      "Epoch: 26 - 00040/00647 - Loss: 0.04193. [  6 s]\n",
      "Epoch: 26 - 00060/00647 - Loss: 0.04234. [  9 s]\n",
      "Epoch: 26 - 00080/00647 - Loss: 0.02881. [ 12 s]\n",
      "Epoch: 26 - 00100/00647 - Loss: 0.09665. [ 15 s]\n",
      "Epoch: 26 - 00120/00647 - Loss: 0.03140. [ 18 s]\n",
      "Epoch: 26 - 00140/00647 - Loss: 0.02310. [ 22 s]\n",
      "Epoch: 26 - 00160/00647 - Loss: 0.02568. [ 25 s]\n",
      "Epoch: 26 - 00180/00647 - Loss: 0.07690. [ 28 s]\n",
      "Epoch: 26 - 00200/00647 - Loss: 0.03668. [ 31 s]\n",
      "Epoch: 26 - 00220/00647 - Loss: 0.07927. [ 34 s]\n",
      "Epoch: 26 - 00240/00647 - Loss: 0.11311. [ 37 s]\n",
      "Epoch: 26 - 00260/00647 - Loss: 0.01610. [ 40 s]\n",
      "Epoch: 26 - 00280/00647 - Loss: 0.02673. [ 43 s]\n",
      "Epoch: 26 - 00300/00647 - Loss: 0.04334. [ 47 s]\n",
      "Epoch: 26 - 00320/00647 - Loss: 0.03467. [ 50 s]\n",
      "Epoch: 26 - 00340/00647 - Loss: 0.05364. [ 53 s]\n",
      "Epoch: 26 - 00360/00647 - Loss: 0.02942. [ 56 s]\n",
      "Epoch: 26 - 00380/00647 - Loss: 0.06203. [ 59 s]\n",
      "Epoch: 26 - 00400/00647 - Loss: 0.03882. [ 62 s]\n",
      "Epoch: 26 - 00420/00647 - Loss: 0.04806. [ 65 s]\n",
      "Epoch: 26 - 00440/00647 - Loss: 0.09120. [ 69 s]\n",
      "Epoch: 26 - 00460/00647 - Loss: 0.03715. [ 72 s]\n",
      "Epoch: 26 - 00480/00647 - Loss: 0.04645. [ 75 s]\n",
      "Epoch: 26 - 00500/00647 - Loss: 0.02518. [ 78 s]\n",
      "Epoch: 26 - 00520/00647 - Loss: 0.06940. [ 81 s]\n",
      "Epoch: 26 - 00540/00647 - Loss: 0.02391. [ 84 s]\n",
      "Epoch: 26 - 00560/00647 - Loss: 0.02007. [ 88 s]\n",
      "Epoch: 26 - 00580/00647 - Loss: 0.03687. [ 91 s]\n",
      "Epoch: 26 - 00600/00647 - Loss: 0.05628. [ 94 s]\n",
      "Epoch: 26 - 00620/00647 - Loss: 0.01873. [ 97 s]\n",
      "Epoch: 26 - 00640/00647 - Loss: 0.07467. [100 s]\n",
      "Epoch: 26 - loss(trn/val):0.04363/0.17026, acc(val):94.85%, lr=0.00010. [101s] @25 samples/s \n",
      "Epoch: 27 - 00020/00647 - Loss: 0.02534. [  3 s]\n",
      "Epoch: 27 - 00040/00647 - Loss: 0.05407. [  6 s]\n",
      "Epoch: 27 - 00060/00647 - Loss: 0.05337. [ 10 s]\n",
      "Epoch: 27 - 00080/00647 - Loss: 0.02436. [ 13 s]\n",
      "Epoch: 27 - 00100/00647 - Loss: 0.01630. [ 16 s]\n",
      "Epoch: 27 - 00120/00647 - Loss: 0.06019. [ 19 s]\n",
      "Epoch: 27 - 00140/00647 - Loss: 0.03081. [ 22 s]\n",
      "Epoch: 27 - 00160/00647 - Loss: 0.03674. [ 25 s]\n",
      "Epoch: 27 - 00180/00647 - Loss: 0.03074. [ 28 s]\n",
      "Epoch: 27 - 00200/00647 - Loss: 0.05555. [ 32 s]\n",
      "Epoch: 27 - 00220/00647 - Loss: 0.02951. [ 35 s]\n",
      "Epoch: 27 - 00240/00647 - Loss: 0.05619. [ 38 s]\n",
      "Epoch: 27 - 00260/00647 - Loss: 0.03353. [ 41 s]\n",
      "Epoch: 27 - 00280/00647 - Loss: 0.04971. [ 44 s]\n",
      "Epoch: 27 - 00300/00647 - Loss: 0.03365. [ 47 s]\n",
      "Epoch: 27 - 00320/00647 - Loss: 0.06569. [ 51 s]\n",
      "Epoch: 27 - 00340/00647 - Loss: 0.04158. [ 54 s]\n",
      "Epoch: 27 - 00360/00647 - Loss: 0.03806. [ 57 s]\n",
      "Epoch: 27 - 00380/00647 - Loss: 0.12288. [ 60 s]\n",
      "Epoch: 27 - 00400/00647 - Loss: 0.04156. [ 64 s]\n",
      "Epoch: 27 - 00420/00647 - Loss: 0.04395. [ 67 s]\n",
      "Epoch: 27 - 00440/00647 - Loss: 0.04870. [ 70 s]\n",
      "Epoch: 27 - 00460/00647 - Loss: 0.06234. [ 73 s]\n",
      "Epoch: 27 - 00480/00647 - Loss: 0.04467. [ 76 s]\n",
      "Epoch: 27 - 00500/00647 - Loss: 0.02990. [ 80 s]\n",
      "Epoch: 27 - 00520/00647 - Loss: 0.02327. [ 83 s]\n",
      "Epoch: 27 - 00540/00647 - Loss: 0.05452. [ 86 s]\n",
      "Epoch: 27 - 00560/00647 - Loss: 0.04410. [ 89 s]\n",
      "Epoch: 27 - 00580/00647 - Loss: 0.03878. [ 93 s]\n",
      "Epoch: 27 - 00600/00647 - Loss: 0.05274. [ 96 s]\n",
      "Epoch: 27 - 00620/00647 - Loss: 0.05988. [ 99 s]\n",
      "Epoch: 27 - 00640/00647 - Loss: 0.04836. [102 s]\n",
      "Epoch: 27 - loss(trn/val):0.03594/0.13053, acc(val):96.03%, lr=0.00010. [103s] @24 samples/s \n",
      "Epoch: 28 - 00020/00647 - Loss: 0.01711. [  3 s]\n",
      "Epoch: 28 - 00040/00647 - Loss: 0.03464. [  6 s]\n",
      "Epoch: 28 - 00060/00647 - Loss: 0.02600. [  9 s]\n",
      "Epoch: 28 - 00080/00647 - Loss: 0.06786. [ 12 s]\n",
      "Epoch: 28 - 00100/00647 - Loss: 0.04797. [ 16 s]\n",
      "Epoch: 28 - 00120/00647 - Loss: 0.01334. [ 19 s]\n",
      "Epoch: 28 - 00140/00647 - Loss: 0.03588. [ 22 s]\n",
      "Epoch: 28 - 00160/00647 - Loss: 0.05146. [ 25 s]\n",
      "Epoch: 28 - 00180/00647 - Loss: 0.01943. [ 28 s]\n",
      "Epoch: 28 - 00200/00647 - Loss: 0.05155. [ 31 s]\n",
      "Epoch: 28 - 00220/00647 - Loss: 0.02685. [ 35 s]\n",
      "Epoch: 28 - 00240/00647 - Loss: 0.05624. [ 38 s]\n",
      "Epoch: 28 - 00260/00647 - Loss: 0.04596. [ 41 s]\n",
      "Epoch: 28 - 00280/00647 - Loss: 0.05919. [ 44 s]\n",
      "Epoch: 28 - 00300/00647 - Loss: 0.01854. [ 47 s]\n",
      "Epoch: 28 - 00320/00647 - Loss: 0.03174. [ 50 s]\n",
      "Epoch: 28 - 00340/00647 - Loss: 0.02644. [ 54 s]\n",
      "Epoch: 28 - 00360/00647 - Loss: 0.01638. [ 57 s]\n",
      "Epoch: 28 - 00380/00647 - Loss: 0.02439. [ 60 s]\n",
      "Epoch: 28 - 00400/00647 - Loss: 0.03370. [ 63 s]\n",
      "Epoch: 28 - 00420/00647 - Loss: 0.03838. [ 66 s]\n",
      "Epoch: 28 - 00440/00647 - Loss: 0.05883. [ 70 s]\n",
      "Epoch: 28 - 00460/00647 - Loss: 0.02724. [ 73 s]\n",
      "Epoch: 28 - 00480/00647 - Loss: 0.03269. [ 76 s]\n",
      "Epoch: 28 - 00500/00647 - Loss: 0.02566. [ 79 s]\n",
      "Epoch: 28 - 00520/00647 - Loss: 0.07320. [ 83 s]\n",
      "Epoch: 28 - 00540/00647 - Loss: 0.32519. [ 86 s]\n",
      "Epoch: 28 - 00560/00647 - Loss: 0.07851. [ 89 s]\n",
      "Epoch: 28 - 00580/00647 - Loss: 0.04132. [ 92 s]\n",
      "Epoch: 28 - 00600/00647 - Loss: 0.05696. [ 95 s]\n",
      "Epoch: 28 - 00620/00647 - Loss: 0.04714. [ 99 s]\n",
      "Epoch: 28 - 00640/00647 - Loss: 0.01981. [102 s]\n",
      "Epoch: 28 - loss(trn/val):0.03506/0.13483, acc(val):96.12%, lr=0.00010. [103s] @25 samples/s \n",
      "Epoch: 29 - 00020/00647 - Loss: 0.04387. [  3 s]\n",
      "Epoch: 29 - 00040/00647 - Loss: 0.03408. [  6 s]\n",
      "Epoch: 29 - 00060/00647 - Loss: 0.02015. [ 10 s]\n",
      "Epoch: 29 - 00080/00647 - Loss: 0.02650. [ 13 s]\n",
      "Epoch: 29 - 00100/00647 - Loss: 0.03465. [ 16 s]\n",
      "Epoch: 29 - 00120/00647 - Loss: 0.02287. [ 19 s]\n",
      "Epoch: 29 - 00140/00647 - Loss: 0.02449. [ 22 s]\n",
      "Epoch: 29 - 00160/00647 - Loss: 0.03174. [ 25 s]\n",
      "Epoch: 29 - 00180/00647 - Loss: 0.04301. [ 28 s]\n",
      "Epoch: 29 - 00200/00647 - Loss: 0.02576. [ 32 s]\n",
      "Epoch: 29 - 00220/00647 - Loss: 0.02461. [ 35 s]\n",
      "Epoch: 29 - 00240/00647 - Loss: 0.02787. [ 38 s]\n",
      "Epoch: 29 - 00260/00647 - Loss: 0.02864. [ 41 s]\n",
      "Epoch: 29 - 00280/00647 - Loss: 0.07087. [ 44 s]\n",
      "Epoch: 29 - 00300/00647 - Loss: 0.02569. [ 47 s]\n",
      "Epoch: 29 - 00320/00647 - Loss: 0.05121. [ 51 s]\n",
      "Epoch: 29 - 00340/00647 - Loss: 0.04362. [ 54 s]\n",
      "Epoch: 29 - 00360/00647 - Loss: 0.02514. [ 57 s]\n",
      "Epoch: 29 - 00380/00647 - Loss: 0.03592. [ 60 s]\n",
      "Epoch: 29 - 00400/00647 - Loss: 0.09797. [ 64 s]\n",
      "Epoch: 29 - 00420/00647 - Loss: 0.04502. [ 67 s]\n",
      "Epoch: 29 - 00440/00647 - Loss: 0.06122. [ 70 s]\n",
      "Epoch: 29 - 00460/00647 - Loss: 0.03349. [ 74 s]\n",
      "Epoch: 29 - 00480/00647 - Loss: 0.04805. [ 77 s]\n",
      "Epoch: 29 - 00500/00647 - Loss: 0.02813. [ 80 s]\n",
      "Epoch: 29 - 00520/00647 - Loss: 0.06873. [ 83 s]\n",
      "Epoch: 29 - 00540/00647 - Loss: 0.05224. [ 86 s]\n",
      "Epoch: 29 - 00560/00647 - Loss: 0.01695. [ 90 s]\n",
      "Epoch: 29 - 00580/00647 - Loss: 0.07201. [ 93 s]\n",
      "Epoch: 29 - 00600/00647 - Loss: 0.03910. [ 96 s]\n",
      "Epoch: 29 - 00620/00647 - Loss: 0.03874. [ 99 s]\n",
      "Epoch: 29 - 00640/00647 - Loss: 0.01302. [103 s]\n",
      "Epoch: 29 - loss(trn/val):0.03330/0.16102, acc(val):95.44%, lr=0.00010. [104s] @24 samples/s \n",
      "Epoch: 30 - 00020/00647 - Loss: 0.03139. [  3 s]\n",
      "Epoch: 30 - 00040/00647 - Loss: 0.02443. [  6 s]\n",
      "Epoch: 30 - 00060/00647 - Loss: 0.02053. [  9 s]\n",
      "Epoch: 30 - 00080/00647 - Loss: 0.04492. [ 12 s]\n",
      "Epoch: 30 - 00100/00647 - Loss: 0.02580. [ 16 s]\n",
      "Epoch: 30 - 00120/00647 - Loss: 0.01496. [ 19 s]\n",
      "Epoch: 30 - 00140/00647 - Loss: 0.03287. [ 22 s]\n",
      "Epoch: 30 - 00160/00647 - Loss: 0.02719. [ 25 s]\n",
      "Epoch: 30 - 00180/00647 - Loss: 0.03157. [ 28 s]\n",
      "Epoch: 30 - 00200/00647 - Loss: 0.03578. [ 31 s]\n",
      "Epoch: 30 - 00220/00647 - Loss: 0.05189. [ 34 s]\n",
      "Epoch: 30 - 00240/00647 - Loss: 0.02100. [ 37 s]\n",
      "Epoch: 30 - 00260/00647 - Loss: 0.03570. [ 40 s]\n",
      "Epoch: 30 - 00280/00647 - Loss: 0.01544. [ 43 s]\n",
      "Epoch: 30 - 00300/00647 - Loss: 0.01947. [ 46 s]\n",
      "Epoch: 30 - 00320/00647 - Loss: 0.03301. [ 50 s]\n",
      "Epoch: 30 - 00340/00647 - Loss: 0.04546. [ 53 s]\n",
      "Epoch: 30 - 00360/00647 - Loss: 0.02439. [ 56 s]\n",
      "Epoch: 30 - 00380/00647 - Loss: 0.04709. [ 59 s]\n",
      "Epoch: 30 - 00400/00647 - Loss: 0.04741. [ 62 s]\n",
      "Epoch: 30 - 00420/00647 - Loss: 0.02165. [ 65 s]\n",
      "Epoch: 30 - 00440/00647 - Loss: 0.02721. [ 68 s]\n",
      "Epoch: 30 - 00460/00647 - Loss: 0.01535. [ 71 s]\n",
      "Epoch: 30 - 00480/00647 - Loss: 0.02306. [ 75 s]\n",
      "Epoch: 30 - 00500/00647 - Loss: 0.03518. [ 78 s]\n",
      "Epoch: 30 - 00520/00647 - Loss: 0.01782. [ 81 s]\n",
      "Epoch: 30 - 00540/00647 - Loss: 0.04563. [ 84 s]\n",
      "Epoch: 30 - 00560/00647 - Loss: 0.02644. [ 87 s]\n",
      "Epoch: 30 - 00580/00647 - Loss: 0.01627. [ 90 s]\n",
      "Epoch: 30 - 00600/00647 - Loss: 0.04685. [ 93 s]\n",
      "Epoch: 30 - 00620/00647 - Loss: 0.03564. [ 96 s]\n",
      "Epoch: 30 - 00640/00647 - Loss: 0.03231. [ 99 s]\n",
      "Epoch: 30 - loss(trn/val):0.03391/0.15064, acc(val):95.91%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 31 - 00020/00647 - Loss: 0.04840. [  3 s]\n",
      "Epoch: 31 - 00040/00647 - Loss: 0.01787. [  6 s]\n",
      "Epoch: 31 - 00060/00647 - Loss: 0.05362. [  9 s]\n",
      "Epoch: 31 - 00080/00647 - Loss: 0.05629. [ 12 s]\n",
      "Epoch: 31 - 00100/00647 - Loss: 0.02185. [ 15 s]\n",
      "Epoch: 31 - 00120/00647 - Loss: 0.09661. [ 18 s]\n",
      "Epoch: 31 - 00140/00647 - Loss: 0.05309. [ 21 s]\n",
      "Epoch: 31 - 00160/00647 - Loss: 0.03179. [ 24 s]\n",
      "Epoch: 31 - 00180/00647 - Loss: 0.02952. [ 28 s]\n",
      "Epoch: 31 - 00200/00647 - Loss: 0.05609. [ 31 s]\n",
      "Epoch: 31 - 00220/00647 - Loss: 0.03692. [ 34 s]\n",
      "Epoch: 31 - 00240/00647 - Loss: 0.02792. [ 37 s]\n",
      "Epoch: 31 - 00260/00647 - Loss: 0.05283. [ 40 s]\n",
      "Epoch: 31 - 00280/00647 - Loss: 0.01573. [ 43 s]\n",
      "Epoch: 31 - 00300/00647 - Loss: 0.02996. [ 46 s]\n",
      "Epoch: 31 - 00320/00647 - Loss: 0.02253. [ 49 s]\n",
      "Epoch: 31 - 00340/00647 - Loss: 0.04036. [ 52 s]\n",
      "Epoch: 31 - 00360/00647 - Loss: 0.03928. [ 55 s]\n",
      "Epoch: 31 - 00380/00647 - Loss: 0.03372. [ 58 s]\n",
      "Epoch: 31 - 00400/00647 - Loss: 0.02850. [ 61 s]\n",
      "Epoch: 31 - 00420/00647 - Loss: 0.06322. [ 64 s]\n",
      "Epoch: 31 - 00440/00647 - Loss: 0.03576. [ 68 s]\n",
      "Epoch: 31 - 00460/00647 - Loss: 0.02924. [ 71 s]\n",
      "Epoch: 31 - 00480/00647 - Loss: 0.03733. [ 74 s]\n",
      "Epoch: 31 - 00500/00647 - Loss: 0.04065. [ 77 s]\n",
      "Epoch: 31 - 00520/00647 - Loss: 0.02852. [ 80 s]\n",
      "Epoch: 31 - 00540/00647 - Loss: 0.07125. [ 83 s]\n",
      "Epoch: 31 - 00560/00647 - Loss: 0.04276. [ 86 s]\n",
      "Epoch: 31 - 00580/00647 - Loss: 0.03927. [ 89 s]\n",
      "Epoch: 31 - 00600/00647 - Loss: 0.01843. [ 92 s]\n",
      "Epoch: 31 - 00620/00647 - Loss: 0.03127. [ 95 s]\n",
      "Epoch: 31 - 00640/00647 - Loss: 0.04192. [ 98 s]\n",
      "Epoch: 31 - loss(trn/val):0.03360/0.16787, acc(val):95.39%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 32 - 00020/00647 - Loss: 0.04124. [  3 s]\n",
      "Epoch: 32 - 00040/00647 - Loss: 0.03649. [  6 s]\n",
      "Epoch: 32 - 00060/00647 - Loss: 0.02425. [  9 s]\n",
      "Epoch: 32 - 00080/00647 - Loss: 0.05531. [ 12 s]\n",
      "Epoch: 32 - 00100/00647 - Loss: 0.04963. [ 15 s]\n",
      "Epoch: 32 - 00120/00647 - Loss: 0.06233. [ 18 s]\n",
      "Epoch: 32 - 00140/00647 - Loss: 0.02599. [ 21 s]\n",
      "Epoch: 32 - 00160/00647 - Loss: 0.11430. [ 24 s]\n",
      "Epoch: 32 - 00180/00647 - Loss: 0.04038. [ 27 s]\n",
      "Epoch: 32 - 00200/00647 - Loss: 0.03769. [ 31 s]\n",
      "Epoch: 32 - 00220/00647 - Loss: 0.03806. [ 34 s]\n",
      "Epoch: 32 - 00240/00647 - Loss: 0.02333. [ 37 s]\n",
      "Epoch: 32 - 00260/00647 - Loss: 0.03200. [ 40 s]\n",
      "Epoch: 32 - 00280/00647 - Loss: 0.03286. [ 43 s]\n",
      "Epoch: 32 - 00300/00647 - Loss: 0.04482. [ 46 s]\n",
      "Epoch: 32 - 00320/00647 - Loss: 0.01772. [ 49 s]\n",
      "Epoch: 32 - 00340/00647 - Loss: 0.02392. [ 52 s]\n",
      "Epoch: 32 - 00360/00647 - Loss: 0.02949. [ 55 s]\n",
      "Epoch: 32 - 00380/00647 - Loss: 0.01716. [ 58 s]\n",
      "Epoch: 32 - 00400/00647 - Loss: 0.04256. [ 61 s]\n",
      "Epoch: 32 - 00420/00647 - Loss: 0.03367. [ 64 s]\n",
      "Epoch: 32 - 00440/00647 - Loss: 0.02381. [ 67 s]\n",
      "Epoch: 32 - 00460/00647 - Loss: 0.03296. [ 71 s]\n",
      "Epoch: 32 - 00480/00647 - Loss: 0.01785. [ 74 s]\n",
      "Epoch: 32 - 00500/00647 - Loss: 0.03059. [ 77 s]\n",
      "Epoch: 32 - 00520/00647 - Loss: 0.02963. [ 80 s]\n",
      "Epoch: 32 - 00540/00647 - Loss: 0.03736. [ 83 s]\n",
      "Epoch: 32 - 00560/00647 - Loss: 0.03065. [ 86 s]\n",
      "Epoch: 32 - 00580/00647 - Loss: 0.02997. [ 89 s]\n",
      "Epoch: 32 - 00600/00647 - Loss: 0.03022. [ 92 s]\n",
      "Epoch: 32 - 00620/00647 - Loss: 0.03998. [ 95 s]\n",
      "Epoch: 32 - 00640/00647 - Loss: 0.04248. [ 98 s]\n",
      "Epoch: 32 - loss(trn/val):0.03028/0.16330, acc(val):95.53%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 33 - 00020/00647 - Loss: 0.01316. [  3 s]\n",
      "Epoch: 33 - 00040/00647 - Loss: 0.02026. [  6 s]\n",
      "Epoch: 33 - 00060/00647 - Loss: 0.04221. [  9 s]\n",
      "Epoch: 33 - 00080/00647 - Loss: 0.02143. [ 12 s]\n",
      "Epoch: 33 - 00100/00647 - Loss: 0.04952. [ 15 s]\n",
      "Epoch: 33 - 00120/00647 - Loss: 0.02877. [ 18 s]\n",
      "Epoch: 33 - 00140/00647 - Loss: 0.02106. [ 21 s]\n",
      "Epoch: 33 - 00160/00647 - Loss: 0.08594. [ 24 s]\n",
      "Epoch: 33 - 00180/00647 - Loss: 0.09252. [ 27 s]\n",
      "Epoch: 33 - 00200/00647 - Loss: 0.13536. [ 30 s]\n",
      "Epoch: 33 - 00220/00647 - Loss: 0.03573. [ 34 s]\n",
      "Epoch: 33 - 00240/00647 - Loss: 0.02683. [ 37 s]\n",
      "Epoch: 33 - 00260/00647 - Loss: 0.01829. [ 40 s]\n",
      "Epoch: 33 - 00280/00647 - Loss: 0.03653. [ 43 s]\n",
      "Epoch: 33 - 00300/00647 - Loss: 0.03844. [ 46 s]\n",
      "Epoch: 33 - 00320/00647 - Loss: 0.04892. [ 49 s]\n",
      "Epoch: 33 - 00340/00647 - Loss: 0.01595. [ 52 s]\n",
      "Epoch: 33 - 00360/00647 - Loss: 0.03050. [ 55 s]\n",
      "Epoch: 33 - 00380/00647 - Loss: 0.02253. [ 58 s]\n",
      "Epoch: 33 - 00400/00647 - Loss: 0.01932. [ 61 s]\n",
      "Epoch: 33 - 00420/00647 - Loss: 0.04091. [ 64 s]\n",
      "Epoch: 33 - 00440/00647 - Loss: 0.02728. [ 67 s]\n",
      "Epoch: 33 - 00460/00647 - Loss: 0.03877. [ 70 s]\n",
      "Epoch: 33 - 00480/00647 - Loss: 0.03890. [ 74 s]\n",
      "Epoch: 33 - 00500/00647 - Loss: 0.01853. [ 77 s]\n",
      "Epoch: 33 - 00520/00647 - Loss: 0.03152. [ 80 s]\n",
      "Epoch: 33 - 00540/00647 - Loss: 0.07207. [ 83 s]\n",
      "Epoch: 33 - 00560/00647 - Loss: 0.02074. [ 86 s]\n",
      "Epoch: 33 - 00580/00647 - Loss: 0.02519. [ 89 s]\n",
      "Epoch: 33 - 00600/00647 - Loss: 0.03090. [ 92 s]\n",
      "Epoch: 33 - 00620/00647 - Loss: 0.04559. [ 95 s]\n",
      "Epoch: 33 - 00640/00647 - Loss: 0.03207. [ 98 s]\n",
      "Epoch: 33 - loss(trn/val):0.03223/0.15362, acc(val):95.57%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 34 - 00020/00647 - Loss: 0.04712. [  3 s]\n",
      "Epoch: 34 - 00040/00647 - Loss: 0.02383. [  6 s]\n",
      "Epoch: 34 - 00060/00647 - Loss: 0.04414. [  9 s]\n",
      "Epoch: 34 - 00080/00647 - Loss: 0.02560. [ 12 s]\n",
      "Epoch: 34 - 00100/00647 - Loss: 0.01639. [ 15 s]\n",
      "Epoch: 34 - 00120/00647 - Loss: 0.02289. [ 18 s]\n",
      "Epoch: 34 - 00140/00647 - Loss: 0.01708. [ 21 s]\n",
      "Epoch: 34 - 00160/00647 - Loss: 0.02502. [ 24 s]\n",
      "Epoch: 34 - 00180/00647 - Loss: 0.01856. [ 27 s]\n",
      "Epoch: 34 - 00200/00647 - Loss: 0.03518. [ 30 s]\n",
      "Epoch: 34 - 00220/00647 - Loss: 0.02508. [ 33 s]\n",
      "Epoch: 34 - 00240/00647 - Loss: 0.03799. [ 37 s]\n",
      "Epoch: 34 - 00260/00647 - Loss: 0.02384. [ 40 s]\n",
      "Epoch: 34 - 00280/00647 - Loss: 0.06982. [ 43 s]\n",
      "Epoch: 34 - 00300/00647 - Loss: 0.02894. [ 46 s]\n",
      "Epoch: 34 - 00320/00647 - Loss: 0.03563. [ 49 s]\n",
      "Epoch: 34 - 00340/00647 - Loss: 0.03197. [ 52 s]\n",
      "Epoch: 34 - 00360/00647 - Loss: 0.01732. [ 55 s]\n",
      "Epoch: 34 - 00380/00647 - Loss: 0.04085. [ 58 s]\n",
      "Epoch: 34 - 00400/00647 - Loss: 0.03104. [ 61 s]\n",
      "Epoch: 34 - 00420/00647 - Loss: 0.02383. [ 64 s]\n",
      "Epoch: 34 - 00440/00647 - Loss: 0.07129. [ 68 s]\n",
      "Epoch: 34 - 00460/00647 - Loss: 0.06649. [ 71 s]\n",
      "Epoch: 34 - 00480/00647 - Loss: 0.04658. [ 74 s]\n",
      "Epoch: 34 - 00500/00647 - Loss: 0.03237. [ 77 s]\n",
      "Epoch: 34 - 00520/00647 - Loss: 0.01242. [ 80 s]\n",
      "Epoch: 34 - 00540/00647 - Loss: 0.02951. [ 83 s]\n",
      "Epoch: 34 - 00560/00647 - Loss: 0.04577. [ 86 s]\n",
      "Epoch: 34 - 00580/00647 - Loss: 0.03581. [ 89 s]\n",
      "Epoch: 34 - 00600/00647 - Loss: 0.01997. [ 92 s]\n",
      "Epoch: 34 - 00620/00647 - Loss: 0.04128. [ 96 s]\n",
      "Epoch: 34 - 00640/00647 - Loss: 0.02909. [ 99 s]\n",
      "Epoch: 34 - loss(trn/val):0.03058/0.14907, acc(val):96.12%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 35 - 00020/00647 - Loss: 0.01860. [  3 s]\n",
      "Epoch: 35 - 00040/00647 - Loss: 0.04039. [  6 s]\n",
      "Epoch: 35 - 00060/00647 - Loss: 0.02447. [  9 s]\n",
      "Epoch: 35 - 00080/00647 - Loss: 0.03952. [ 12 s]\n",
      "Epoch: 35 - 00100/00647 - Loss: 0.03269. [ 15 s]\n",
      "Epoch: 35 - 00120/00647 - Loss: 0.01562. [ 18 s]\n",
      "Epoch: 35 - 00140/00647 - Loss: 0.04312. [ 21 s]\n",
      "Epoch: 35 - 00160/00647 - Loss: 0.02438. [ 24 s]\n",
      "Epoch: 35 - 00180/00647 - Loss: 0.03269. [ 27 s]\n",
      "Epoch: 35 - 00200/00647 - Loss: 0.03116. [ 31 s]\n",
      "Epoch: 35 - 00220/00647 - Loss: 0.03453. [ 34 s]\n",
      "Epoch: 35 - 00240/00647 - Loss: 0.03494. [ 37 s]\n",
      "Epoch: 35 - 00260/00647 - Loss: 0.05436. [ 40 s]\n",
      "Epoch: 35 - 00280/00647 - Loss: 0.02900. [ 43 s]\n",
      "Epoch: 35 - 00300/00647 - Loss: 0.08346. [ 46 s]\n",
      "Epoch: 35 - 00320/00647 - Loss: 0.03351. [ 49 s]\n",
      "Epoch: 35 - 00340/00647 - Loss: 0.04439. [ 52 s]\n",
      "Epoch: 35 - 00360/00647 - Loss: 0.03369. [ 55 s]\n",
      "Epoch: 35 - 00380/00647 - Loss: 0.03100. [ 59 s]\n",
      "Epoch: 35 - 00400/00647 - Loss: 0.01594. [ 62 s]\n",
      "Epoch: 35 - 00420/00647 - Loss: 0.02492. [ 65 s]\n",
      "Epoch: 35 - 00440/00647 - Loss: 0.03180. [ 68 s]\n",
      "Epoch: 35 - 00460/00647 - Loss: 0.02850. [ 71 s]\n",
      "Epoch: 35 - 00480/00647 - Loss: 0.03071. [ 74 s]\n",
      "Epoch: 35 - 00500/00647 - Loss: 0.01692. [ 77 s]\n",
      "Epoch: 35 - 00520/00647 - Loss: 0.02523. [ 80 s]\n",
      "Epoch: 35 - 00540/00647 - Loss: 0.03734. [ 83 s]\n",
      "Epoch: 35 - 00560/00647 - Loss: 0.14038. [ 86 s]\n",
      "Epoch: 35 - 00580/00647 - Loss: 0.02784. [ 89 s]\n",
      "Epoch: 35 - 00600/00647 - Loss: 0.02967. [ 93 s]\n",
      "Epoch: 35 - 00620/00647 - Loss: 0.03029. [ 96 s]\n",
      "Epoch: 35 - 00640/00647 - Loss: 0.05441. [ 99 s]\n",
      "Epoch: 35 - loss(trn/val):0.03165/0.19015, acc(val):94.77%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 36 - 00020/00647 - Loss: 0.04302. [  3 s]\n",
      "Epoch: 36 - 00040/00647 - Loss: 0.04604. [  6 s]\n",
      "Epoch: 36 - 00060/00647 - Loss: 0.02955. [  9 s]\n",
      "Epoch: 36 - 00080/00647 - Loss: 0.02029. [ 12 s]\n",
      "Epoch: 36 - 00100/00647 - Loss: 0.02900. [ 15 s]\n",
      "Epoch: 36 - 00120/00647 - Loss: 0.04018. [ 18 s]\n",
      "Epoch: 36 - 00140/00647 - Loss: 0.02332. [ 21 s]\n",
      "Epoch: 36 - 00160/00647 - Loss: 0.01396. [ 24 s]\n",
      "Epoch: 36 - 00180/00647 - Loss: 0.02516. [ 27 s]\n",
      "Epoch: 36 - 00200/00647 - Loss: 0.02330. [ 31 s]\n",
      "Epoch: 36 - 00220/00647 - Loss: 0.01303. [ 34 s]\n",
      "Epoch: 36 - 00240/00647 - Loss: 0.01737. [ 37 s]\n",
      "Epoch: 36 - 00260/00647 - Loss: 0.04605. [ 40 s]\n",
      "Epoch: 36 - 00280/00647 - Loss: 0.03153. [ 43 s]\n",
      "Epoch: 36 - 00300/00647 - Loss: 0.02417. [ 46 s]\n",
      "Epoch: 36 - 00320/00647 - Loss: 0.03320. [ 49 s]\n",
      "Epoch: 36 - 00340/00647 - Loss: 0.02945. [ 52 s]\n",
      "Epoch: 36 - 00360/00647 - Loss: 0.03838. [ 55 s]\n",
      "Epoch: 36 - 00380/00647 - Loss: 0.02023. [ 58 s]\n",
      "Epoch: 36 - 00400/00647 - Loss: 0.01374. [ 61 s]\n",
      "Epoch: 36 - 00420/00647 - Loss: 0.02425. [ 64 s]\n",
      "Epoch: 36 - 00440/00647 - Loss: 0.04271. [ 68 s]\n",
      "Epoch: 36 - 00460/00647 - Loss: 0.02364. [ 71 s]\n",
      "Epoch: 36 - 00480/00647 - Loss: 0.02737. [ 74 s]\n",
      "Epoch: 36 - 00500/00647 - Loss: 0.02238. [ 77 s]\n",
      "Epoch: 36 - 00520/00647 - Loss: 0.02504. [ 80 s]\n",
      "Epoch: 36 - 00540/00647 - Loss: 0.02314. [ 83 s]\n",
      "Epoch: 36 - 00560/00647 - Loss: 0.04370. [ 86 s]\n",
      "Epoch: 36 - 00580/00647 - Loss: 0.04864. [ 89 s]\n",
      "Epoch: 36 - 00600/00647 - Loss: 0.04227. [ 92 s]\n",
      "Epoch: 36 - 00620/00647 - Loss: 0.04737. [ 95 s]\n",
      "Epoch: 36 - 00640/00647 - Loss: 0.02634. [ 98 s]\n",
      "Epoch: 36 - loss(trn/val):0.03314/0.16313, acc(val):95.83%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 37 - 00020/00647 - Loss: 0.04987. [  3 s]\n",
      "Epoch: 37 - 00040/00647 - Loss: 0.04719. [  6 s]\n",
      "Epoch: 37 - 00060/00647 - Loss: 0.01521. [  9 s]\n",
      "Epoch: 37 - 00080/00647 - Loss: 0.02522. [ 12 s]\n",
      "Epoch: 37 - 00100/00647 - Loss: 0.02410. [ 15 s]\n",
      "Epoch: 37 - 00120/00647 - Loss: 0.01648. [ 18 s]\n",
      "Epoch: 37 - 00140/00647 - Loss: 0.03098. [ 21 s]\n",
      "Epoch: 37 - 00160/00647 - Loss: 0.03415. [ 24 s]\n",
      "Epoch: 37 - 00180/00647 - Loss: 0.04554. [ 27 s]\n",
      "Epoch: 37 - 00200/00647 - Loss: 0.04340. [ 31 s]\n",
      "Epoch: 37 - 00220/00647 - Loss: 0.05447. [ 34 s]\n",
      "Epoch: 37 - 00240/00647 - Loss: 0.03435. [ 37 s]\n",
      "Epoch: 37 - 00260/00647 - Loss: 0.08254. [ 40 s]\n",
      "Epoch: 37 - 00280/00647 - Loss: 0.02250. [ 43 s]\n",
      "Epoch: 37 - 00300/00647 - Loss: 0.01361. [ 46 s]\n",
      "Epoch: 37 - 00320/00647 - Loss: 0.06243. [ 49 s]\n",
      "Epoch: 37 - 00340/00647 - Loss: 0.07898. [ 52 s]\n",
      "Epoch: 37 - 00360/00647 - Loss: 0.05088. [ 55 s]\n",
      "Epoch: 37 - 00380/00647 - Loss: 0.02056. [ 58 s]\n",
      "Epoch: 37 - 00400/00647 - Loss: 0.01982. [ 61 s]\n",
      "Epoch: 37 - 00420/00647 - Loss: 0.01805. [ 65 s]\n",
      "Epoch: 37 - 00440/00647 - Loss: 0.03297. [ 68 s]\n",
      "Epoch: 37 - 00460/00647 - Loss: 0.04694. [ 71 s]\n",
      "Epoch: 37 - 00480/00647 - Loss: 0.03068. [ 74 s]\n",
      "Epoch: 37 - 00500/00647 - Loss: 0.02587. [ 77 s]\n",
      "Epoch: 37 - 00520/00647 - Loss: 0.01844. [ 80 s]\n",
      "Epoch: 37 - 00540/00647 - Loss: 0.03717. [ 83 s]\n",
      "Epoch: 37 - 00560/00647 - Loss: 0.03122. [ 86 s]\n",
      "Epoch: 37 - 00580/00647 - Loss: 0.08415. [ 89 s]\n",
      "Epoch: 37 - 00600/00647 - Loss: 0.04515. [ 93 s]\n",
      "Epoch: 37 - 00620/00647 - Loss: 0.06052. [ 96 s]\n",
      "Epoch: 37 - 00640/00647 - Loss: 0.02584. [ 99 s]\n",
      "Epoch: 37 - loss(trn/val):0.03373/0.18404, acc(val):95.26%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 38 - 00020/00647 - Loss: 0.05822. [  3 s]\n",
      "Epoch: 38 - 00040/00647 - Loss: 0.03108. [  6 s]\n",
      "Epoch: 38 - 00060/00647 - Loss: 0.02438. [  9 s]\n",
      "Epoch: 38 - 00080/00647 - Loss: 0.05284. [ 12 s]\n",
      "Epoch: 38 - 00100/00647 - Loss: 0.02022. [ 15 s]\n",
      "Epoch: 38 - 00120/00647 - Loss: 0.03744. [ 18 s]\n",
      "Epoch: 38 - 00140/00647 - Loss: 0.02328. [ 21 s]\n",
      "Epoch: 38 - 00160/00647 - Loss: 0.06954. [ 24 s]\n",
      "Epoch: 38 - 00180/00647 - Loss: 0.02536. [ 27 s]\n",
      "Epoch: 38 - 00200/00647 - Loss: 0.03533. [ 30 s]\n",
      "Epoch: 38 - 00220/00647 - Loss: 0.03334. [ 34 s]\n",
      "Epoch: 38 - 00240/00647 - Loss: 0.01801. [ 37 s]\n",
      "Epoch: 38 - 00260/00647 - Loss: 0.05380. [ 40 s]\n",
      "Epoch: 38 - 00280/00647 - Loss: 0.02906. [ 43 s]\n",
      "Epoch: 38 - 00300/00647 - Loss: 0.06079. [ 46 s]\n",
      "Epoch: 38 - 00320/00647 - Loss: 0.03707. [ 49 s]\n",
      "Epoch: 38 - 00340/00647 - Loss: 0.01813. [ 52 s]\n",
      "Epoch: 38 - 00360/00647 - Loss: 0.08440. [ 55 s]\n",
      "Epoch: 38 - 00380/00647 - Loss: 0.04332. [ 58 s]\n",
      "Epoch: 38 - 00400/00647 - Loss: 0.02773. [ 62 s]\n",
      "Epoch: 38 - 00420/00647 - Loss: 0.02285. [ 65 s]\n",
      "Epoch: 38 - 00440/00647 - Loss: 0.03189. [ 68 s]\n",
      "Epoch: 38 - 00460/00647 - Loss: 0.01954. [ 71 s]\n",
      "Epoch: 38 - 00480/00647 - Loss: 0.02503. [ 74 s]\n",
      "Epoch: 38 - 00500/00647 - Loss: 0.04611. [ 77 s]\n",
      "Epoch: 38 - 00520/00647 - Loss: 0.04692. [ 80 s]\n",
      "Epoch: 38 - 00540/00647 - Loss: 0.02246. [ 83 s]\n",
      "Epoch: 38 - 00560/00647 - Loss: 0.03622. [ 86 s]\n",
      "Epoch: 38 - 00580/00647 - Loss: 0.01963. [ 89 s]\n",
      "Epoch: 38 - 00600/00647 - Loss: 0.03176. [ 92 s]\n",
      "Epoch: 38 - 00620/00647 - Loss: 0.03137. [ 96 s]\n",
      "Epoch: 38 - 00640/00647 - Loss: 0.04270. [ 99 s]\n",
      "Epoch: 38 - loss(trn/val):0.02957/0.20207, acc(val):94.90%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 39 - 00020/00647 - Loss: 0.02862. [  3 s]\n",
      "Epoch: 39 - 00040/00647 - Loss: 0.02862. [  6 s]\n",
      "Epoch: 39 - 00060/00647 - Loss: 0.02694. [  9 s]\n",
      "Epoch: 39 - 00080/00647 - Loss: 0.01751. [ 12 s]\n",
      "Epoch: 39 - 00100/00647 - Loss: 0.02753. [ 15 s]\n",
      "Epoch: 39 - 00120/00647 - Loss: 0.01876. [ 18 s]\n",
      "Epoch: 39 - 00140/00647 - Loss: 0.03454. [ 21 s]\n",
      "Epoch: 39 - 00160/00647 - Loss: 0.03012. [ 24 s]\n",
      "Epoch: 39 - 00180/00647 - Loss: 0.04108. [ 27 s]\n",
      "Epoch: 39 - 00200/00647 - Loss: 0.03254. [ 31 s]\n",
      "Epoch: 39 - 00220/00647 - Loss: 0.01644. [ 34 s]\n",
      "Epoch: 39 - 00240/00647 - Loss: 0.02129. [ 37 s]\n",
      "Epoch: 39 - 00260/00647 - Loss: 0.02937. [ 40 s]\n",
      "Epoch: 39 - 00280/00647 - Loss: 0.03791. [ 43 s]\n",
      "Epoch: 39 - 00300/00647 - Loss: 0.04692. [ 46 s]\n",
      "Epoch: 39 - 00320/00647 - Loss: 0.04300. [ 49 s]\n",
      "Epoch: 39 - 00340/00647 - Loss: 0.04126. [ 52 s]\n",
      "Epoch: 39 - 00360/00647 - Loss: 0.03143. [ 55 s]\n",
      "Epoch: 39 - 00380/00647 - Loss: 0.06365. [ 58 s]\n",
      "Epoch: 39 - 00400/00647 - Loss: 0.06296. [ 61 s]\n",
      "Epoch: 39 - 00420/00647 - Loss: 0.02901. [ 64 s]\n",
      "Epoch: 39 - 00440/00647 - Loss: 0.02996. [ 67 s]\n",
      "Epoch: 39 - 00460/00647 - Loss: 0.03617. [ 71 s]\n",
      "Epoch: 39 - 00480/00647 - Loss: 0.03021. [ 74 s]\n",
      "Epoch: 39 - 00500/00647 - Loss: 0.01451. [ 77 s]\n",
      "Epoch: 39 - 00520/00647 - Loss: 0.04166. [ 80 s]\n",
      "Epoch: 39 - 00540/00647 - Loss: 0.03366. [ 83 s]\n",
      "Epoch: 39 - 00560/00647 - Loss: 0.03079. [ 86 s]\n",
      "Epoch: 39 - 00580/00647 - Loss: 0.02185. [ 89 s]\n",
      "Epoch: 39 - 00600/00647 - Loss: 0.05938. [ 92 s]\n",
      "Epoch: 39 - 00620/00647 - Loss: 0.02945. [ 95 s]\n",
      "Epoch: 39 - 00640/00647 - Loss: 0.08698. [ 98 s]\n",
      "Epoch: 39 - loss(trn/val):0.03150/0.20748, acc(val):95.13%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 40 - 00020/00647 - Loss: 0.02787. [  3 s]\n",
      "Epoch: 40 - 00040/00647 - Loss: 0.01247. [  6 s]\n",
      "Epoch: 40 - 00060/00647 - Loss: 0.09676. [  9 s]\n",
      "Epoch: 40 - 00080/00647 - Loss: 0.02093. [ 12 s]\n",
      "Epoch: 40 - 00100/00647 - Loss: 0.02170. [ 15 s]\n",
      "Epoch: 40 - 00120/00647 - Loss: 0.01833. [ 18 s]\n",
      "Epoch: 40 - 00140/00647 - Loss: 0.04800. [ 21 s]\n",
      "Epoch: 40 - 00160/00647 - Loss: 0.02364. [ 24 s]\n",
      "Epoch: 40 - 00180/00647 - Loss: 0.04827. [ 27 s]\n",
      "Epoch: 40 - 00200/00647 - Loss: 0.03276. [ 30 s]\n",
      "Epoch: 40 - 00220/00647 - Loss: 0.01537. [ 33 s]\n",
      "Epoch: 40 - 00240/00647 - Loss: 0.04176. [ 37 s]\n",
      "Epoch: 40 - 00260/00647 - Loss: 0.02514. [ 40 s]\n",
      "Epoch: 40 - 00280/00647 - Loss: 0.03521. [ 43 s]\n",
      "Epoch: 40 - 00300/00647 - Loss: 0.05007. [ 46 s]\n",
      "Epoch: 40 - 00320/00647 - Loss: 0.02744. [ 49 s]\n",
      "Epoch: 40 - 00340/00647 - Loss: 0.03892. [ 52 s]\n",
      "Epoch: 40 - 00360/00647 - Loss: 0.01651. [ 55 s]\n",
      "Epoch: 40 - 00380/00647 - Loss: 0.10454. [ 58 s]\n",
      "Epoch: 40 - 00400/00647 - Loss: 0.03448. [ 61 s]\n",
      "Epoch: 40 - 00420/00647 - Loss: 0.02164. [ 64 s]\n",
      "Epoch: 40 - 00440/00647 - Loss: 0.01720. [ 67 s]\n",
      "Epoch: 40 - 00460/00647 - Loss: 0.01579. [ 71 s]\n",
      "Epoch: 40 - 00480/00647 - Loss: 0.01848. [ 74 s]\n",
      "Epoch: 40 - 00500/00647 - Loss: 0.03379. [ 77 s]\n",
      "Epoch: 40 - 00520/00647 - Loss: 0.07673. [ 80 s]\n",
      "Epoch: 40 - 00540/00647 - Loss: 0.05793. [ 83 s]\n",
      "Epoch: 40 - 00560/00647 - Loss: 0.03064. [ 86 s]\n",
      "Epoch: 40 - 00580/00647 - Loss: 0.02338. [ 89 s]\n",
      "Epoch: 40 - 00600/00647 - Loss: 0.01762. [ 92 s]\n",
      "Epoch: 40 - 00620/00647 - Loss: 0.02756. [ 95 s]\n",
      "Epoch: 40 - 00640/00647 - Loss: 0.03609. [ 98 s]\n",
      "Epoch: 40 - loss(trn/val):0.03205/0.16395, acc(val):95.70%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 41 - 00020/00647 - Loss: 0.01752. [  3 s]\n",
      "Epoch: 41 - 00040/00647 - Loss: 0.02806. [  6 s]\n",
      "Epoch: 41 - 00060/00647 - Loss: 0.01331. [  9 s]\n",
      "Epoch: 41 - 00080/00647 - Loss: 0.01617. [ 12 s]\n",
      "Epoch: 41 - 00100/00647 - Loss: 0.04844. [ 15 s]\n",
      "Epoch: 41 - 00120/00647 - Loss: 0.03107. [ 18 s]\n",
      "Epoch: 41 - 00140/00647 - Loss: 0.01675. [ 21 s]\n",
      "Epoch: 41 - 00160/00647 - Loss: 0.01669. [ 24 s]\n",
      "Epoch: 41 - 00180/00647 - Loss: 0.01959. [ 27 s]\n",
      "Epoch: 41 - 00200/00647 - Loss: 0.03154. [ 30 s]\n",
      "Epoch: 41 - 00220/00647 - Loss: 0.01682. [ 33 s]\n",
      "Epoch: 41 - 00240/00647 - Loss: 0.02518. [ 37 s]\n",
      "Epoch: 41 - 00260/00647 - Loss: 0.03612. [ 40 s]\n",
      "Epoch: 41 - 00280/00647 - Loss: 0.04249. [ 43 s]\n",
      "Epoch: 41 - 00300/00647 - Loss: 0.04817. [ 46 s]\n",
      "Epoch: 41 - 00320/00647 - Loss: 0.02154. [ 49 s]\n",
      "Epoch: 41 - 00340/00647 - Loss: 0.01622. [ 52 s]\n",
      "Epoch: 41 - 00360/00647 - Loss: 0.04886. [ 55 s]\n",
      "Epoch: 41 - 00380/00647 - Loss: 0.02276. [ 58 s]\n",
      "Epoch: 41 - 00400/00647 - Loss: 0.01320. [ 61 s]\n",
      "Epoch: 41 - 00420/00647 - Loss: 0.02433. [ 64 s]\n",
      "Epoch: 41 - 00440/00647 - Loss: 0.01748. [ 67 s]\n",
      "Epoch: 41 - 00460/00647 - Loss: 0.04933. [ 71 s]\n",
      "Epoch: 41 - 00480/00647 - Loss: 0.02348. [ 74 s]\n",
      "Epoch: 41 - 00500/00647 - Loss: 0.01759. [ 77 s]\n",
      "Epoch: 41 - 00520/00647 - Loss: 0.03951. [ 80 s]\n",
      "Epoch: 41 - 00540/00647 - Loss: 0.05869. [ 83 s]\n",
      "Epoch: 41 - 00560/00647 - Loss: 0.03640. [ 86 s]\n",
      "Epoch: 41 - 00580/00647 - Loss: 0.06412. [ 89 s]\n",
      "Epoch: 41 - 00600/00647 - Loss: 0.20104. [ 92 s]\n",
      "Epoch: 41 - 00620/00647 - Loss: 0.06192. [ 95 s]\n",
      "Epoch: 41 - 00640/00647 - Loss: 0.05915. [ 98 s]\n",
      "Epoch: 41 - loss(trn/val):0.03558/0.16802, acc(val):95.01%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 42 - 00020/00647 - Loss: 0.02140. [  3 s]\n",
      "Epoch: 42 - 00040/00647 - Loss: 0.04395. [  6 s]\n",
      "Epoch: 42 - 00060/00647 - Loss: 0.05460. [  9 s]\n",
      "Epoch: 42 - 00080/00647 - Loss: 0.02719. [ 12 s]\n",
      "Epoch: 42 - 00100/00647 - Loss: 0.03327. [ 15 s]\n",
      "Epoch: 42 - 00120/00647 - Loss: 0.04539. [ 18 s]\n",
      "Epoch: 42 - 00140/00647 - Loss: 0.02109. [ 21 s]\n",
      "Epoch: 42 - 00160/00647 - Loss: 0.02736. [ 24 s]\n",
      "Epoch: 42 - 00180/00647 - Loss: 0.02173. [ 28 s]\n",
      "Epoch: 42 - 00200/00647 - Loss: 0.04459. [ 31 s]\n",
      "Epoch: 42 - 00220/00647 - Loss: 0.02926. [ 34 s]\n",
      "Epoch: 42 - 00240/00647 - Loss: 0.01745. [ 37 s]\n",
      "Epoch: 42 - 00260/00647 - Loss: 0.02926. [ 40 s]\n",
      "Epoch: 42 - 00280/00647 - Loss: 0.03539. [ 43 s]\n",
      "Epoch: 42 - 00300/00647 - Loss: 0.02085. [ 46 s]\n",
      "Epoch: 42 - 00320/00647 - Loss: 0.01553. [ 49 s]\n",
      "Epoch: 42 - 00340/00647 - Loss: 0.01454. [ 52 s]\n",
      "Epoch: 42 - 00360/00647 - Loss: 0.02996. [ 56 s]\n",
      "Epoch: 42 - 00380/00647 - Loss: 0.03277. [ 59 s]\n",
      "Epoch: 42 - 00400/00647 - Loss: 0.01985. [ 62 s]\n",
      "Epoch: 42 - 00420/00647 - Loss: 0.02661. [ 65 s]\n",
      "Epoch: 42 - 00440/00647 - Loss: 0.01005. [ 68 s]\n",
      "Epoch: 42 - 00460/00647 - Loss: 0.02232. [ 71 s]\n",
      "Epoch: 42 - 00480/00647 - Loss: 0.05164. [ 74 s]\n",
      "Epoch: 42 - 00500/00647 - Loss: 0.03666. [ 77 s]\n",
      "Epoch: 42 - 00520/00647 - Loss: 0.03800. [ 81 s]\n",
      "Epoch: 42 - 00540/00647 - Loss: 0.01746. [ 84 s]\n",
      "Epoch: 42 - 00560/00647 - Loss: 0.03440. [ 87 s]\n",
      "Epoch: 42 - 00580/00647 - Loss: 0.02806. [ 90 s]\n",
      "Epoch: 42 - 00600/00647 - Loss: 0.01988. [ 93 s]\n",
      "Epoch: 42 - 00620/00647 - Loss: 0.02056. [ 96 s]\n",
      "Epoch: 42 - 00640/00647 - Loss: 0.02383. [ 99 s]\n",
      "Epoch: 42 - loss(trn/val):0.02902/0.17287, acc(val):95.34%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 43 - 00020/00647 - Loss: 0.02402. [  3 s]\n",
      "Epoch: 43 - 00040/00647 - Loss: 0.02291. [  6 s]\n",
      "Epoch: 43 - 00060/00647 - Loss: 0.02737. [  9 s]\n",
      "Epoch: 43 - 00080/00647 - Loss: 0.03435. [ 12 s]\n",
      "Epoch: 43 - 00100/00647 - Loss: 0.02441. [ 15 s]\n",
      "Epoch: 43 - 00120/00647 - Loss: 0.03741. [ 18 s]\n",
      "Epoch: 43 - 00140/00647 - Loss: 0.02700. [ 21 s]\n",
      "Epoch: 43 - 00160/00647 - Loss: 0.03406. [ 25 s]\n",
      "Epoch: 43 - 00180/00647 - Loss: 0.01360. [ 28 s]\n",
      "Epoch: 43 - 00200/00647 - Loss: 0.03397. [ 31 s]\n",
      "Epoch: 43 - 00220/00647 - Loss: 0.04347. [ 34 s]\n",
      "Epoch: 43 - 00240/00647 - Loss: 0.03192. [ 37 s]\n",
      "Epoch: 43 - 00260/00647 - Loss: 0.02038. [ 40 s]\n",
      "Epoch: 43 - 00280/00647 - Loss: 0.04126. [ 43 s]\n",
      "Epoch: 43 - 00300/00647 - Loss: 0.02444. [ 46 s]\n",
      "Epoch: 43 - 00320/00647 - Loss: 0.03217. [ 49 s]\n",
      "Epoch: 43 - 00340/00647 - Loss: 0.01459. [ 52 s]\n",
      "Epoch: 43 - 00360/00647 - Loss: 0.01671. [ 55 s]\n",
      "Epoch: 43 - 00380/00647 - Loss: 0.02335. [ 59 s]\n",
      "Epoch: 43 - 00400/00647 - Loss: 0.01991. [ 62 s]\n",
      "Epoch: 43 - 00420/00647 - Loss: 0.03432. [ 65 s]\n",
      "Epoch: 43 - 00440/00647 - Loss: 0.01897. [ 68 s]\n",
      "Epoch: 43 - 00460/00647 - Loss: 0.02623. [ 71 s]\n",
      "Epoch: 43 - 00480/00647 - Loss: 0.01827. [ 74 s]\n",
      "Epoch: 43 - 00500/00647 - Loss: 0.03670. [ 77 s]\n",
      "Epoch: 43 - 00520/00647 - Loss: 0.03562. [ 80 s]\n",
      "Epoch: 43 - 00540/00647 - Loss: 0.02318. [ 84 s]\n",
      "Epoch: 43 - 00560/00647 - Loss: 0.02936. [ 87 s]\n",
      "Epoch: 43 - 00580/00647 - Loss: 0.06229. [ 90 s]\n",
      "Epoch: 43 - 00600/00647 - Loss: 0.04670. [ 93 s]\n",
      "Epoch: 43 - 00620/00647 - Loss: 0.03875. [ 96 s]\n",
      "Epoch: 43 - 00640/00647 - Loss: 0.04485. [ 99 s]\n",
      "Epoch: 43 - loss(trn/val):0.02795/0.19296, acc(val):95.48%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 44 - 00020/00647 - Loss: 0.03594. [  3 s]\n",
      "Epoch: 44 - 00040/00647 - Loss: 0.02601. [  6 s]\n",
      "Epoch: 44 - 00060/00647 - Loss: 0.01665. [  9 s]\n",
      "Epoch: 44 - 00080/00647 - Loss: 0.03139. [ 12 s]\n",
      "Epoch: 44 - 00100/00647 - Loss: 0.02107. [ 15 s]\n",
      "Epoch: 44 - 00120/00647 - Loss: 0.01718. [ 18 s]\n",
      "Epoch: 44 - 00140/00647 - Loss: 0.04131. [ 21 s]\n",
      "Epoch: 44 - 00160/00647 - Loss: 0.01454. [ 25 s]\n",
      "Epoch: 44 - 00180/00647 - Loss: 0.03235. [ 28 s]\n",
      "Epoch: 44 - 00200/00647 - Loss: 0.02304. [ 31 s]\n",
      "Epoch: 44 - 00220/00647 - Loss: 0.03552. [ 34 s]\n",
      "Epoch: 44 - 00240/00647 - Loss: 0.02314. [ 37 s]\n",
      "Epoch: 44 - 00260/00647 - Loss: 0.04213. [ 40 s]\n",
      "Epoch: 44 - 00280/00647 - Loss: 0.01757. [ 43 s]\n",
      "Epoch: 44 - 00300/00647 - Loss: 0.02727. [ 46 s]\n",
      "Epoch: 44 - 00320/00647 - Loss: 0.02018. [ 49 s]\n",
      "Epoch: 44 - 00340/00647 - Loss: 0.01176. [ 53 s]\n",
      "Epoch: 44 - 00360/00647 - Loss: 0.03161. [ 56 s]\n",
      "Epoch: 44 - 00380/00647 - Loss: 0.03097. [ 59 s]\n",
      "Epoch: 44 - 00400/00647 - Loss: 0.03190. [ 62 s]\n",
      "Epoch: 44 - 00420/00647 - Loss: 0.03421. [ 65 s]\n",
      "Epoch: 44 - 00440/00647 - Loss: 0.02634. [ 68 s]\n",
      "Epoch: 44 - 00460/00647 - Loss: 0.03479. [ 71 s]\n",
      "Epoch: 44 - 00480/00647 - Loss: 0.01864. [ 74 s]\n",
      "Epoch: 44 - 00500/00647 - Loss: 0.01965. [ 77 s]\n",
      "Epoch: 44 - 00520/00647 - Loss: 0.03612. [ 81 s]\n",
      "Epoch: 44 - 00540/00647 - Loss: 0.02336. [ 84 s]\n",
      "Epoch: 44 - 00560/00647 - Loss: 0.01602. [ 87 s]\n",
      "Epoch: 44 - 00580/00647 - Loss: 0.02787. [ 90 s]\n",
      "Epoch: 44 - 00600/00647 - Loss: 0.03761. [ 93 s]\n",
      "Epoch: 44 - 00620/00647 - Loss: 0.01327. [ 96 s]\n",
      "Epoch: 44 - 00640/00647 - Loss: 0.01689. [ 99 s]\n",
      "Epoch: 44 - loss(trn/val):0.02703/0.19545, acc(val):95.20%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 45 - 00020/00647 - Loss: 0.02489. [  3 s]\n",
      "Epoch: 45 - 00040/00647 - Loss: 0.03330. [  6 s]\n",
      "Epoch: 45 - 00060/00647 - Loss: 0.01293. [  9 s]\n",
      "Epoch: 45 - 00080/00647 - Loss: 0.04637. [ 12 s]\n",
      "Epoch: 45 - 00100/00647 - Loss: 0.03086. [ 15 s]\n",
      "Epoch: 45 - 00120/00647 - Loss: 0.02529. [ 18 s]\n",
      "Epoch: 45 - 00140/00647 - Loss: 0.02791. [ 21 s]\n",
      "Epoch: 45 - 00160/00647 - Loss: 0.01981. [ 24 s]\n",
      "Epoch: 45 - 00180/00647 - Loss: 0.03825. [ 28 s]\n",
      "Epoch: 45 - 00200/00647 - Loss: 0.03826. [ 31 s]\n",
      "Epoch: 45 - 00220/00647 - Loss: 0.05241. [ 34 s]\n",
      "Epoch: 45 - 00240/00647 - Loss: 0.04244. [ 37 s]\n",
      "Epoch: 45 - 00260/00647 - Loss: 0.04612. [ 40 s]\n",
      "Epoch: 45 - 00280/00647 - Loss: 0.01845. [ 43 s]\n",
      "Epoch: 45 - 00300/00647 - Loss: 0.02803. [ 46 s]\n",
      "Epoch: 45 - 00320/00647 - Loss: 0.02970. [ 49 s]\n",
      "Epoch: 45 - 00340/00647 - Loss: 0.06840. [ 52 s]\n",
      "Epoch: 45 - 00360/00647 - Loss: 0.02328. [ 55 s]\n",
      "Epoch: 45 - 00380/00647 - Loss: 0.07716. [ 59 s]\n",
      "Epoch: 45 - 00400/00647 - Loss: 0.01783. [ 62 s]\n",
      "Epoch: 45 - 00420/00647 - Loss: 0.03018. [ 65 s]\n",
      "Epoch: 45 - 00440/00647 - Loss: 0.05516. [ 68 s]\n",
      "Epoch: 45 - 00460/00647 - Loss: 0.01846. [ 71 s]\n",
      "Epoch: 45 - 00480/00647 - Loss: 0.02010. [ 74 s]\n",
      "Epoch: 45 - 00500/00647 - Loss: 0.01783. [ 77 s]\n",
      "Epoch: 45 - 00520/00647 - Loss: 0.04658. [ 80 s]\n",
      "Epoch: 45 - 00540/00647 - Loss: 0.04864. [ 83 s]\n",
      "Epoch: 45 - 00560/00647 - Loss: 0.02178. [ 87 s]\n",
      "Epoch: 45 - 00580/00647 - Loss: 0.01297. [ 90 s]\n",
      "Epoch: 45 - 00600/00647 - Loss: 0.02183. [ 93 s]\n",
      "Epoch: 45 - 00620/00647 - Loss: 0.01563. [ 96 s]\n",
      "Epoch: 45 - 00640/00647 - Loss: 0.02330. [ 99 s]\n",
      "Epoch: 45 - loss(trn/val):0.02650/0.18540, acc(val):95.56%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 46 - 00020/00647 - Loss: 0.02219. [  3 s]\n",
      "Epoch: 46 - 00040/00647 - Loss: 0.04392. [  6 s]\n",
      "Epoch: 46 - 00060/00647 - Loss: 0.02932. [  9 s]\n",
      "Epoch: 46 - 00080/00647 - Loss: 0.03160. [ 12 s]\n",
      "Epoch: 46 - 00100/00647 - Loss: 0.03463. [ 15 s]\n",
      "Epoch: 46 - 00120/00647 - Loss: 0.08411. [ 18 s]\n",
      "Epoch: 46 - 00140/00647 - Loss: 0.02120. [ 21 s]\n",
      "Epoch: 46 - 00160/00647 - Loss: 0.07936. [ 24 s]\n",
      "Epoch: 46 - 00180/00647 - Loss: 0.02484. [ 27 s]\n",
      "Epoch: 46 - 00200/00647 - Loss: 0.04566. [ 31 s]\n",
      "Epoch: 46 - 00220/00647 - Loss: 0.04592. [ 34 s]\n",
      "Epoch: 46 - 00240/00647 - Loss: 0.02566. [ 37 s]\n",
      "Epoch: 46 - 00260/00647 - Loss: 0.04229. [ 40 s]\n",
      "Epoch: 46 - 00280/00647 - Loss: 0.03547. [ 43 s]\n",
      "Epoch: 46 - 00300/00647 - Loss: 0.04016. [ 46 s]\n",
      "Epoch: 46 - 00320/00647 - Loss: 0.08193. [ 49 s]\n",
      "Epoch: 46 - 00340/00647 - Loss: 0.02368. [ 52 s]\n",
      "Epoch: 46 - 00360/00647 - Loss: 0.02156. [ 55 s]\n",
      "Epoch: 46 - 00380/00647 - Loss: 0.04160. [ 59 s]\n",
      "Epoch: 46 - 00400/00647 - Loss: 0.01644. [ 62 s]\n",
      "Epoch: 46 - 00420/00647 - Loss: 0.03398. [ 65 s]\n",
      "Epoch: 46 - 00440/00647 - Loss: 0.02924. [ 68 s]\n",
      "Epoch: 46 - 00460/00647 - Loss: 0.05480. [ 71 s]\n",
      "Epoch: 46 - 00480/00647 - Loss: 0.02030. [ 74 s]\n",
      "Epoch: 46 - 00500/00647 - Loss: 0.03937. [ 77 s]\n",
      "Epoch: 46 - 00520/00647 - Loss: 0.04772. [ 80 s]\n",
      "Epoch: 46 - 00540/00647 - Loss: 0.02124. [ 83 s]\n",
      "Epoch: 46 - 00560/00647 - Loss: 0.05907. [ 86 s]\n",
      "Epoch: 46 - 00580/00647 - Loss: 0.02924. [ 90 s]\n",
      "Epoch: 46 - 00600/00647 - Loss: 0.02892. [ 93 s]\n",
      "Epoch: 46 - 00620/00647 - Loss: 0.02165. [ 96 s]\n",
      "Epoch: 46 - 00640/00647 - Loss: 0.03813. [ 99 s]\n",
      "Epoch: 46 - loss(trn/val):0.02614/0.23737, acc(val):94.78%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 47 - 00020/00647 - Loss: 0.03224. [  3 s]\n",
      "Epoch: 47 - 00040/00647 - Loss: 0.01725. [  6 s]\n",
      "Epoch: 47 - 00060/00647 - Loss: 0.01633. [  9 s]\n",
      "Epoch: 47 - 00080/00647 - Loss: 0.02043. [ 12 s]\n",
      "Epoch: 47 - 00100/00647 - Loss: 0.02559. [ 15 s]\n",
      "Epoch: 47 - 00120/00647 - Loss: 0.02073. [ 18 s]\n",
      "Epoch: 47 - 00140/00647 - Loss: 0.04517. [ 21 s]\n",
      "Epoch: 47 - 00160/00647 - Loss: 0.03257. [ 24 s]\n",
      "Epoch: 47 - 00180/00647 - Loss: 0.03020. [ 27 s]\n",
      "Epoch: 47 - 00200/00647 - Loss: 0.04300. [ 31 s]\n",
      "Epoch: 47 - 00220/00647 - Loss: 0.02968. [ 34 s]\n",
      "Epoch: 47 - 00240/00647 - Loss: 0.02155. [ 37 s]\n",
      "Epoch: 47 - 00260/00647 - Loss: 0.01960. [ 40 s]\n",
      "Epoch: 47 - 00280/00647 - Loss: 0.03003. [ 43 s]\n",
      "Epoch: 47 - 00300/00647 - Loss: 0.03582. [ 46 s]\n",
      "Epoch: 47 - 00320/00647 - Loss: 0.01376. [ 49 s]\n",
      "Epoch: 47 - 00340/00647 - Loss: 0.01284. [ 52 s]\n",
      "Epoch: 47 - 00360/00647 - Loss: 0.02419. [ 55 s]\n",
      "Epoch: 47 - 00380/00647 - Loss: 0.02736. [ 58 s]\n",
      "Epoch: 47 - 00400/00647 - Loss: 0.04180. [ 61 s]\n",
      "Epoch: 47 - 00420/00647 - Loss: 0.03299. [ 64 s]\n",
      "Epoch: 47 - 00440/00647 - Loss: 0.04213. [ 68 s]\n",
      "Epoch: 47 - 00460/00647 - Loss: 0.02452. [ 71 s]\n",
      "Epoch: 47 - 00480/00647 - Loss: 0.01483. [ 74 s]\n",
      "Epoch: 47 - 00500/00647 - Loss: 0.01846. [ 77 s]\n",
      "Epoch: 47 - 00520/00647 - Loss: 0.02378. [ 80 s]\n",
      "Epoch: 47 - 00540/00647 - Loss: 0.02270. [ 83 s]\n",
      "Epoch: 47 - 00560/00647 - Loss: 0.02944. [ 86 s]\n",
      "Epoch: 47 - 00580/00647 - Loss: 0.03279. [ 89 s]\n",
      "Epoch: 47 - 00600/00647 - Loss: 0.01043. [ 92 s]\n",
      "Epoch: 47 - 00620/00647 - Loss: 0.02183. [ 96 s]\n",
      "Epoch: 47 - 00640/00647 - Loss: 0.02291. [ 99 s]\n",
      "Epoch: 47 - loss(trn/val):0.02871/0.14906, acc(val):95.68%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 48 - 00020/00647 - Loss: 0.02685. [  3 s]\n",
      "Epoch: 48 - 00040/00647 - Loss: 0.02005. [  6 s]\n",
      "Epoch: 48 - 00060/00647 - Loss: 0.03010. [  9 s]\n",
      "Epoch: 48 - 00080/00647 - Loss: 0.01185. [ 12 s]\n",
      "Epoch: 48 - 00100/00647 - Loss: 0.02438. [ 15 s]\n",
      "Epoch: 48 - 00120/00647 - Loss: 0.01831. [ 18 s]\n",
      "Epoch: 48 - 00140/00647 - Loss: 0.05381. [ 21 s]\n",
      "Epoch: 48 - 00160/00647 - Loss: 0.02646. [ 24 s]\n",
      "Epoch: 48 - 00180/00647 - Loss: 0.02197. [ 27 s]\n",
      "Epoch: 48 - 00200/00647 - Loss: 0.02613. [ 30 s]\n",
      "Epoch: 48 - 00220/00647 - Loss: 0.04117. [ 34 s]\n",
      "Epoch: 48 - 00240/00647 - Loss: 0.01959. [ 37 s]\n",
      "Epoch: 48 - 00260/00647 - Loss: 0.02824. [ 40 s]\n",
      "Epoch: 48 - 00280/00647 - Loss: 0.05192. [ 43 s]\n",
      "Epoch: 48 - 00300/00647 - Loss: 0.02683. [ 46 s]\n",
      "Epoch: 48 - 00320/00647 - Loss: 0.06813. [ 49 s]\n",
      "Epoch: 48 - 00340/00647 - Loss: 0.01658. [ 52 s]\n",
      "Epoch: 48 - 00360/00647 - Loss: 0.05566. [ 55 s]\n",
      "Epoch: 48 - 00380/00647 - Loss: 0.03599. [ 58 s]\n",
      "Epoch: 48 - 00400/00647 - Loss: 0.02543. [ 61 s]\n",
      "Epoch: 48 - 00420/00647 - Loss: 0.01998. [ 65 s]\n",
      "Epoch: 48 - 00440/00647 - Loss: 0.01614. [ 68 s]\n",
      "Epoch: 48 - 00460/00647 - Loss: 0.04336. [ 71 s]\n",
      "Epoch: 48 - 00480/00647 - Loss: 0.03413. [ 74 s]\n",
      "Epoch: 48 - 00500/00647 - Loss: 0.01426. [ 77 s]\n",
      "Epoch: 48 - 00520/00647 - Loss: 0.03304. [ 80 s]\n",
      "Epoch: 48 - 00540/00647 - Loss: 0.03673. [ 83 s]\n",
      "Epoch: 48 - 00560/00647 - Loss: 0.02326. [ 86 s]\n",
      "Epoch: 48 - 00580/00647 - Loss: 0.01473. [ 89 s]\n",
      "Epoch: 48 - 00600/00647 - Loss: 0.03468. [ 92 s]\n",
      "Epoch: 48 - 00620/00647 - Loss: 0.01546. [ 96 s]\n",
      "Epoch: 48 - 00640/00647 - Loss: 0.02409. [ 99 s]\n",
      "Epoch: 48 - loss(trn/val):0.03168/0.15272, acc(val):95.35%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 49 - 00020/00647 - Loss: 0.02668. [  3 s]\n",
      "Epoch: 49 - 00040/00647 - Loss: 0.02151. [  6 s]\n",
      "Epoch: 49 - 00060/00647 - Loss: 0.02219. [  9 s]\n",
      "Epoch: 49 - 00080/00647 - Loss: 0.01425. [ 12 s]\n",
      "Epoch: 49 - 00100/00647 - Loss: 0.01950. [ 15 s]\n",
      "Epoch: 49 - 00120/00647 - Loss: 0.02160. [ 18 s]\n",
      "Epoch: 49 - 00140/00647 - Loss: 0.05114. [ 21 s]\n",
      "Epoch: 49 - 00160/00647 - Loss: 0.02198. [ 24 s]\n",
      "Epoch: 49 - 00180/00647 - Loss: 0.03541. [ 27 s]\n",
      "Epoch: 49 - 00200/00647 - Loss: 0.06086. [ 30 s]\n",
      "Epoch: 49 - 00220/00647 - Loss: 0.02408. [ 34 s]\n",
      "Epoch: 49 - 00240/00647 - Loss: 0.03747. [ 37 s]\n",
      "Epoch: 49 - 00260/00647 - Loss: 0.02294. [ 40 s]\n",
      "Epoch: 49 - 00280/00647 - Loss: 0.02537. [ 43 s]\n",
      "Epoch: 49 - 00300/00647 - Loss: 0.01425. [ 46 s]\n",
      "Epoch: 49 - 00320/00647 - Loss: 0.01740. [ 49 s]\n",
      "Epoch: 49 - 00340/00647 - Loss: 0.04243. [ 52 s]\n",
      "Epoch: 49 - 00360/00647 - Loss: 0.01120. [ 55 s]\n",
      "Epoch: 49 - 00380/00647 - Loss: 0.01646. [ 58 s]\n",
      "Epoch: 49 - 00400/00647 - Loss: 0.01772. [ 62 s]\n",
      "Epoch: 49 - 00420/00647 - Loss: 0.01592. [ 65 s]\n",
      "Epoch: 49 - 00440/00647 - Loss: 0.02264. [ 68 s]\n",
      "Epoch: 49 - 00460/00647 - Loss: 0.01344. [ 71 s]\n",
      "Epoch: 49 - 00480/00647 - Loss: 0.02039. [ 74 s]\n",
      "Epoch: 49 - 00500/00647 - Loss: 0.01798. [ 77 s]\n",
      "Epoch: 49 - 00520/00647 - Loss: 0.01849. [ 80 s]\n",
      "Epoch: 49 - 00540/00647 - Loss: 0.02123. [ 83 s]\n",
      "Epoch: 49 - 00560/00647 - Loss: 0.04469. [ 86 s]\n",
      "Epoch: 49 - 00580/00647 - Loss: 0.02168. [ 90 s]\n",
      "Epoch: 49 - 00600/00647 - Loss: 0.04153. [ 93 s]\n",
      "Epoch: 49 - 00620/00647 - Loss: 0.07341. [ 96 s]\n",
      "Epoch: 49 - 00640/00647 - Loss: 0.03773. [ 99 s]\n",
      "Epoch: 49 - loss(trn/val):0.05764/0.15342, acc(val):95.85%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 50 - 00020/00647 - Loss: 0.10032. [  3 s]\n",
      "Epoch: 50 - 00040/00647 - Loss: 0.00984. [  6 s]\n",
      "Epoch: 50 - 00060/00647 - Loss: 0.05531. [  9 s]\n",
      "Epoch: 50 - 00080/00647 - Loss: 0.01007. [ 12 s]\n",
      "Epoch: 50 - 00100/00647 - Loss: 0.01952. [ 15 s]\n",
      "Epoch: 50 - 00120/00647 - Loss: 0.01712. [ 18 s]\n",
      "Epoch: 50 - 00140/00647 - Loss: 0.01421. [ 21 s]\n",
      "Epoch: 50 - 00160/00647 - Loss: 0.02169. [ 24 s]\n",
      "Epoch: 50 - 00180/00647 - Loss: 0.01314. [ 28 s]\n",
      "Epoch: 50 - 00200/00647 - Loss: 0.04019. [ 31 s]\n",
      "Epoch: 50 - 00220/00647 - Loss: 0.02108. [ 34 s]\n",
      "Epoch: 50 - 00240/00647 - Loss: 0.01854. [ 37 s]\n",
      "Epoch: 50 - 00260/00647 - Loss: 0.16123. [ 40 s]\n",
      "Epoch: 50 - 00280/00647 - Loss: 0.04985. [ 43 s]\n",
      "Epoch: 50 - 00300/00647 - Loss: 0.15353. [ 46 s]\n",
      "Epoch: 50 - 00320/00647 - Loss: 0.03095. [ 49 s]\n",
      "Epoch: 50 - 00340/00647 - Loss: 0.01823. [ 52 s]\n",
      "Epoch: 50 - 00360/00647 - Loss: 0.03374. [ 55 s]\n",
      "Epoch: 50 - 00380/00647 - Loss: 0.01883. [ 59 s]\n",
      "Epoch: 50 - 00400/00647 - Loss: 0.04534. [ 62 s]\n",
      "Epoch: 50 - 00420/00647 - Loss: 0.02859. [ 65 s]\n",
      "Epoch: 50 - 00440/00647 - Loss: 0.02468. [ 68 s]\n",
      "Epoch: 50 - 00460/00647 - Loss: 0.02402. [ 71 s]\n",
      "Epoch: 50 - 00480/00647 - Loss: 0.02140. [ 74 s]\n",
      "Epoch: 50 - 00500/00647 - Loss: 0.03639. [ 77 s]\n",
      "Epoch: 50 - 00520/00647 - Loss: 0.01216. [ 80 s]\n",
      "Epoch: 50 - 00540/00647 - Loss: 0.01349. [ 83 s]\n",
      "Epoch: 50 - 00560/00647 - Loss: 0.01819. [ 87 s]\n",
      "Epoch: 50 - 00580/00647 - Loss: 0.02302. [ 90 s]\n",
      "Epoch: 50 - 00600/00647 - Loss: 0.07587. [ 93 s]\n",
      "Epoch: 50 - 00620/00647 - Loss: 0.01457. [ 96 s]\n",
      "Epoch: 50 - 00640/00647 - Loss: 0.01619. [ 99 s]\n",
      "Epoch: 50 - loss(trn/val):0.02383/0.21095, acc(val):95.24%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 51 - 00020/00647 - Loss: 0.02046. [  3 s]\n",
      "Epoch: 51 - 00040/00647 - Loss: 0.02978. [  6 s]\n",
      "Epoch: 51 - 00060/00647 - Loss: 0.02223. [  9 s]\n",
      "Epoch: 51 - 00080/00647 - Loss: 0.02054. [ 12 s]\n",
      "Epoch: 51 - 00100/00647 - Loss: 0.02555. [ 15 s]\n",
      "Epoch: 51 - 00120/00647 - Loss: 0.02694. [ 18 s]\n",
      "Epoch: 51 - 00140/00647 - Loss: 0.02680. [ 21 s]\n",
      "Epoch: 51 - 00160/00647 - Loss: 0.04647. [ 24 s]\n",
      "Epoch: 51 - 00180/00647 - Loss: 0.03389. [ 27 s]\n",
      "Epoch: 51 - 00200/00647 - Loss: 0.04094. [ 31 s]\n",
      "Epoch: 51 - 00220/00647 - Loss: 0.02506. [ 34 s]\n",
      "Epoch: 51 - 00240/00647 - Loss: 0.01983. [ 37 s]\n",
      "Epoch: 51 - 00260/00647 - Loss: 0.03604. [ 40 s]\n",
      "Epoch: 51 - 00280/00647 - Loss: 0.03651. [ 43 s]\n",
      "Epoch: 51 - 00300/00647 - Loss: 0.02534. [ 46 s]\n",
      "Epoch: 51 - 00320/00647 - Loss: 0.02508. [ 49 s]\n",
      "Epoch: 51 - 00340/00647 - Loss: 0.06521. [ 52 s]\n",
      "Epoch: 51 - 00360/00647 - Loss: 0.04715. [ 55 s]\n",
      "Epoch: 51 - 00380/00647 - Loss: 0.02832. [ 58 s]\n",
      "Epoch: 51 - 00400/00647 - Loss: 0.01963. [ 62 s]\n",
      "Epoch: 51 - 00420/00647 - Loss: 0.03457. [ 65 s]\n",
      "Epoch: 51 - 00440/00647 - Loss: 0.01121. [ 68 s]\n",
      "Epoch: 51 - 00460/00647 - Loss: 0.04014. [ 71 s]\n",
      "Epoch: 51 - 00480/00647 - Loss: 0.03251. [ 74 s]\n",
      "Epoch: 51 - 00500/00647 - Loss: 0.03541. [ 77 s]\n",
      "Epoch: 51 - 00520/00647 - Loss: 0.00912. [ 80 s]\n",
      "Epoch: 51 - 00540/00647 - Loss: 0.05057. [ 83 s]\n",
      "Epoch: 51 - 00560/00647 - Loss: 0.02467. [ 86 s]\n",
      "Epoch: 51 - 00580/00647 - Loss: 0.03282. [ 89 s]\n",
      "Epoch: 51 - 00600/00647 - Loss: 0.01665. [ 93 s]\n",
      "Epoch: 51 - 00620/00647 - Loss: 0.02880. [ 96 s]\n",
      "Epoch: 51 - 00640/00647 - Loss: 0.03187. [ 99 s]\n",
      "Epoch: 51 - loss(trn/val):0.02929/0.17825, acc(val):95.54%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 52 - 00020/00647 - Loss: 0.01790. [  3 s]\n",
      "Epoch: 52 - 00040/00647 - Loss: 0.02109. [  6 s]\n",
      "Epoch: 52 - 00060/00647 - Loss: 0.02135. [  9 s]\n",
      "Epoch: 52 - 00080/00647 - Loss: 0.02641. [ 12 s]\n",
      "Epoch: 52 - 00100/00647 - Loss: 0.03302. [ 15 s]\n",
      "Epoch: 52 - 00120/00647 - Loss: 0.03996. [ 18 s]\n",
      "Epoch: 52 - 00140/00647 - Loss: 0.02136. [ 21 s]\n",
      "Epoch: 52 - 00160/00647 - Loss: 0.03157. [ 24 s]\n",
      "Epoch: 52 - 00180/00647 - Loss: 0.01654. [ 27 s]\n",
      "Epoch: 52 - 00200/00647 - Loss: 0.04302. [ 31 s]\n",
      "Epoch: 52 - 00220/00647 - Loss: 0.09033. [ 34 s]\n",
      "Epoch: 52 - 00240/00647 - Loss: 0.02189. [ 37 s]\n",
      "Epoch: 52 - 00260/00647 - Loss: 0.02639. [ 40 s]\n",
      "Epoch: 52 - 00280/00647 - Loss: 0.01991. [ 43 s]\n",
      "Epoch: 52 - 00300/00647 - Loss: 0.01674. [ 46 s]\n",
      "Epoch: 52 - 00320/00647 - Loss: 0.02568. [ 49 s]\n",
      "Epoch: 52 - 00340/00647 - Loss: 0.02760. [ 52 s]\n",
      "Epoch: 52 - 00360/00647 - Loss: 0.02168. [ 55 s]\n",
      "Epoch: 52 - 00380/00647 - Loss: 0.03954. [ 59 s]\n",
      "Epoch: 52 - 00400/00647 - Loss: 0.01019. [ 62 s]\n",
      "Epoch: 52 - 00420/00647 - Loss: 0.01841. [ 65 s]\n",
      "Epoch: 52 - 00440/00647 - Loss: 0.02462. [ 68 s]\n",
      "Epoch: 52 - 00460/00647 - Loss: 0.04919. [ 71 s]\n",
      "Epoch: 52 - 00480/00647 - Loss: 0.02105. [ 74 s]\n",
      "Epoch: 52 - 00500/00647 - Loss: 0.01638. [ 77 s]\n",
      "Epoch: 52 - 00520/00647 - Loss: 0.01695. [ 80 s]\n",
      "Epoch: 52 - 00540/00647 - Loss: 0.03349. [ 83 s]\n",
      "Epoch: 52 - 00560/00647 - Loss: 0.02262. [ 86 s]\n",
      "Epoch: 52 - 00580/00647 - Loss: 0.04997. [ 90 s]\n",
      "Epoch: 52 - 00600/00647 - Loss: 0.02561. [ 93 s]\n",
      "Epoch: 52 - 00620/00647 - Loss: 0.03280. [ 96 s]\n",
      "Epoch: 52 - 00640/00647 - Loss: 0.03743. [ 99 s]\n",
      "Epoch: 52 - loss(trn/val):0.06130/0.22033, acc(val):93.17%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 53 - 00020/00647 - Loss: 0.02866. [  3 s]\n",
      "Epoch: 53 - 00040/00647 - Loss: 0.02818. [  6 s]\n",
      "Epoch: 53 - 00060/00647 - Loss: 0.02624. [  9 s]\n",
      "Epoch: 53 - 00080/00647 - Loss: 0.01088. [ 12 s]\n",
      "Epoch: 53 - 00100/00647 - Loss: 0.02647. [ 15 s]\n",
      "Epoch: 53 - 00120/00647 - Loss: 0.01325. [ 18 s]\n",
      "Epoch: 53 - 00140/00647 - Loss: 0.01444. [ 21 s]\n",
      "Epoch: 53 - 00160/00647 - Loss: 0.02416. [ 24 s]\n",
      "Epoch: 53 - 00180/00647 - Loss: 0.02833. [ 28 s]\n",
      "Epoch: 53 - 00200/00647 - Loss: 0.02298. [ 31 s]\n",
      "Epoch: 53 - 00220/00647 - Loss: 0.01540. [ 34 s]\n",
      "Epoch: 53 - 00240/00647 - Loss: 0.02503. [ 37 s]\n",
      "Epoch: 53 - 00260/00647 - Loss: 0.03140. [ 40 s]\n",
      "Epoch: 53 - 00280/00647 - Loss: 0.06292. [ 43 s]\n",
      "Epoch: 53 - 00300/00647 - Loss: 0.04156. [ 46 s]\n",
      "Epoch: 53 - 00320/00647 - Loss: 0.04090. [ 49 s]\n",
      "Epoch: 53 - 00340/00647 - Loss: 0.02923. [ 52 s]\n",
      "Epoch: 53 - 00360/00647 - Loss: 0.02575. [ 55 s]\n",
      "Epoch: 53 - 00380/00647 - Loss: 0.01692. [ 59 s]\n",
      "Epoch: 53 - 00400/00647 - Loss: 0.02332. [ 62 s]\n",
      "Epoch: 53 - 00420/00647 - Loss: 0.03519. [ 65 s]\n",
      "Epoch: 53 - 00440/00647 - Loss: 0.03533. [ 68 s]\n",
      "Epoch: 53 - 00460/00647 - Loss: 0.03116. [ 71 s]\n",
      "Epoch: 53 - 00480/00647 - Loss: 0.02008. [ 74 s]\n",
      "Epoch: 53 - 00500/00647 - Loss: 0.02308. [ 77 s]\n",
      "Epoch: 53 - 00520/00647 - Loss: 0.02695. [ 80 s]\n",
      "Epoch: 53 - 00540/00647 - Loss: 0.01585. [ 83 s]\n",
      "Epoch: 53 - 00560/00647 - Loss: 0.01881. [ 86 s]\n",
      "Epoch: 53 - 00580/00647 - Loss: 0.01680. [ 90 s]\n",
      "Epoch: 53 - 00600/00647 - Loss: 0.02077. [ 93 s]\n",
      "Epoch: 53 - 00620/00647 - Loss: 0.01951. [ 96 s]\n",
      "Epoch: 53 - 00640/00647 - Loss: 0.04621. [ 99 s]\n",
      "Epoch: 53 - loss(trn/val):0.03169/0.23171, acc(val):94.67%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 54 - 00020/00647 - Loss: 0.07234. [  3 s]\n",
      "Epoch: 54 - 00040/00647 - Loss: 0.03031. [  6 s]\n",
      "Epoch: 54 - 00060/00647 - Loss: 0.02274. [  9 s]\n",
      "Epoch: 54 - 00080/00647 - Loss: 0.01785. [ 12 s]\n",
      "Epoch: 54 - 00100/00647 - Loss: 0.01484. [ 15 s]\n",
      "Epoch: 54 - 00120/00647 - Loss: 0.03125. [ 18 s]\n",
      "Epoch: 54 - 00140/00647 - Loss: 0.02407. [ 21 s]\n",
      "Epoch: 54 - 00160/00647 - Loss: 0.02530. [ 24 s]\n",
      "Epoch: 54 - 00180/00647 - Loss: 0.02217. [ 27 s]\n",
      "Epoch: 54 - 00200/00647 - Loss: 0.02473. [ 30 s]\n",
      "Epoch: 54 - 00220/00647 - Loss: 0.00967. [ 33 s]\n",
      "Epoch: 54 - 00240/00647 - Loss: 0.01426. [ 37 s]\n",
      "Epoch: 54 - 00260/00647 - Loss: 0.03575. [ 40 s]\n",
      "Epoch: 54 - 00280/00647 - Loss: 0.07400. [ 43 s]\n",
      "Epoch: 54 - 00300/00647 - Loss: 0.03920. [ 46 s]\n",
      "Epoch: 54 - 00320/00647 - Loss: 0.02151. [ 49 s]\n",
      "Epoch: 54 - 00340/00647 - Loss: 0.03525. [ 52 s]\n",
      "Epoch: 54 - 00360/00647 - Loss: 0.02050. [ 55 s]\n",
      "Epoch: 54 - 00380/00647 - Loss: 0.01958. [ 58 s]\n",
      "Epoch: 54 - 00400/00647 - Loss: 0.04307. [ 61 s]\n",
      "Epoch: 54 - 00420/00647 - Loss: 0.03223. [ 64 s]\n",
      "Epoch: 54 - 00440/00647 - Loss: 0.05989. [ 67 s]\n",
      "Epoch: 54 - 00460/00647 - Loss: 0.02623. [ 71 s]\n",
      "Epoch: 54 - 00480/00647 - Loss: 0.01643. [ 74 s]\n",
      "Epoch: 54 - 00500/00647 - Loss: 0.03110. [ 77 s]\n",
      "Epoch: 54 - 00520/00647 - Loss: 0.03750. [ 80 s]\n",
      "Epoch: 54 - 00540/00647 - Loss: 0.04950. [ 83 s]\n",
      "Epoch: 54 - 00560/00647 - Loss: 0.01920. [ 86 s]\n",
      "Epoch: 54 - 00580/00647 - Loss: 0.03084. [ 89 s]\n",
      "Epoch: 54 - 00600/00647 - Loss: 0.01538. [ 92 s]\n",
      "Epoch: 54 - 00620/00647 - Loss: 0.01933. [ 95 s]\n",
      "Epoch: 54 - 00640/00647 - Loss: 0.02428. [ 98 s]\n",
      "Epoch: 54 - loss(trn/val):0.02389/0.16450, acc(val):95.53%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 55 - 00020/00647 - Loss: 0.01830. [  3 s]\n",
      "Epoch: 55 - 00040/00647 - Loss: 0.01432. [  6 s]\n",
      "Epoch: 55 - 00060/00647 - Loss: 0.01658. [  9 s]\n",
      "Epoch: 55 - 00080/00647 - Loss: 0.01667. [ 12 s]\n",
      "Epoch: 55 - 00100/00647 - Loss: 0.03011. [ 15 s]\n",
      "Epoch: 55 - 00120/00647 - Loss: 0.02570. [ 18 s]\n",
      "Epoch: 55 - 00140/00647 - Loss: 0.01963. [ 21 s]\n",
      "Epoch: 55 - 00160/00647 - Loss: 0.02452. [ 24 s]\n",
      "Epoch: 55 - 00180/00647 - Loss: 0.06551. [ 27 s]\n",
      "Epoch: 55 - 00200/00647 - Loss: 0.02979. [ 30 s]\n",
      "Epoch: 55 - 00220/00647 - Loss: 0.04349. [ 34 s]\n",
      "Epoch: 55 - 00240/00647 - Loss: 0.03274. [ 37 s]\n",
      "Epoch: 55 - 00260/00647 - Loss: 0.04787. [ 40 s]\n",
      "Epoch: 55 - 00280/00647 - Loss: 0.02149. [ 43 s]\n",
      "Epoch: 55 - 00300/00647 - Loss: 0.02349. [ 46 s]\n",
      "Epoch: 55 - 00320/00647 - Loss: 0.02676. [ 49 s]\n",
      "Epoch: 55 - 00340/00647 - Loss: 0.03269. [ 52 s]\n",
      "Epoch: 55 - 00360/00647 - Loss: 0.03341. [ 55 s]\n",
      "Epoch: 55 - 00380/00647 - Loss: 0.02450. [ 58 s]\n",
      "Epoch: 55 - 00400/00647 - Loss: 0.01803. [ 61 s]\n",
      "Epoch: 55 - 00420/00647 - Loss: 0.02411. [ 64 s]\n",
      "Epoch: 55 - 00440/00647 - Loss: 0.02913. [ 68 s]\n",
      "Epoch: 55 - 00460/00647 - Loss: 0.03643. [ 71 s]\n",
      "Epoch: 55 - 00480/00647 - Loss: 0.02562. [ 74 s]\n",
      "Epoch: 55 - 00500/00647 - Loss: 0.02541. [ 77 s]\n",
      "Epoch: 55 - 00520/00647 - Loss: 0.02480. [ 80 s]\n",
      "Epoch: 55 - 00540/00647 - Loss: 0.05688. [ 83 s]\n",
      "Epoch: 55 - 00560/00647 - Loss: 0.02483. [ 86 s]\n",
      "Epoch: 55 - 00580/00647 - Loss: 0.01538. [ 89 s]\n",
      "Epoch: 55 - 00600/00647 - Loss: 0.01020. [ 92 s]\n",
      "Epoch: 55 - 00620/00647 - Loss: 0.01708. [ 95 s]\n",
      "Epoch: 55 - 00640/00647 - Loss: 0.02374. [ 99 s]\n",
      "Epoch: 55 - loss(trn/val):0.02464/0.17830, acc(val):95.71%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 56 - 00020/00647 - Loss: 0.02666. [  3 s]\n",
      "Epoch: 56 - 00040/00647 - Loss: 0.02340. [  6 s]\n",
      "Epoch: 56 - 00060/00647 - Loss: 0.02819. [  9 s]\n",
      "Epoch: 56 - 00080/00647 - Loss: 0.02084. [ 12 s]\n",
      "Epoch: 56 - 00100/00647 - Loss: 0.01790. [ 15 s]\n",
      "Epoch: 56 - 00120/00647 - Loss: 0.01055. [ 18 s]\n",
      "Epoch: 56 - 00140/00647 - Loss: 0.01657. [ 21 s]\n",
      "Epoch: 56 - 00160/00647 - Loss: 0.01337. [ 24 s]\n",
      "Epoch: 56 - 00180/00647 - Loss: 0.02433. [ 27 s]\n",
      "Epoch: 56 - 00200/00647 - Loss: 0.02115. [ 30 s]\n",
      "Epoch: 56 - 00220/00647 - Loss: 0.02442. [ 34 s]\n",
      "Epoch: 56 - 00240/00647 - Loss: 0.04068. [ 37 s]\n",
      "Epoch: 56 - 00260/00647 - Loss: 0.01369. [ 40 s]\n",
      "Epoch: 56 - 00280/00647 - Loss: 0.02160. [ 43 s]\n",
      "Epoch: 56 - 00300/00647 - Loss: 0.01492. [ 46 s]\n",
      "Epoch: 56 - 00320/00647 - Loss: 0.02420. [ 49 s]\n",
      "Epoch: 56 - 00340/00647 - Loss: 0.03995. [ 52 s]\n",
      "Epoch: 56 - 00360/00647 - Loss: 0.02673. [ 55 s]\n",
      "Epoch: 56 - 00380/00647 - Loss: 0.03065. [ 58 s]\n",
      "Epoch: 56 - 00400/00647 - Loss: 0.01842. [ 61 s]\n",
      "Epoch: 56 - 00420/00647 - Loss: 0.02615. [ 64 s]\n",
      "Epoch: 56 - 00440/00647 - Loss: 0.03172. [ 67 s]\n",
      "Epoch: 56 - 00460/00647 - Loss: 0.04231. [ 71 s]\n",
      "Epoch: 56 - 00480/00647 - Loss: 0.16650. [ 74 s]\n",
      "Epoch: 56 - 00500/00647 - Loss: 0.06639. [ 77 s]\n",
      "Epoch: 56 - 00520/00647 - Loss: 0.03211. [ 80 s]\n",
      "Epoch: 56 - 00540/00647 - Loss: 0.02337. [ 83 s]\n",
      "Epoch: 56 - 00560/00647 - Loss: 0.01910. [ 86 s]\n",
      "Epoch: 56 - 00580/00647 - Loss: 0.03344. [ 89 s]\n",
      "Epoch: 56 - 00600/00647 - Loss: 0.04216. [ 92 s]\n",
      "Epoch: 56 - 00620/00647 - Loss: 0.03516. [ 95 s]\n",
      "Epoch: 56 - 00640/00647 - Loss: 0.02751. [ 98 s]\n",
      "Epoch: 56 - loss(trn/val):0.02565/0.22605, acc(val):95.06%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 57 - 00020/00647 - Loss: 0.02629. [  3 s]\n",
      "Epoch: 57 - 00040/00647 - Loss: 0.01114. [  6 s]\n",
      "Epoch: 57 - 00060/00647 - Loss: 0.02942. [  9 s]\n",
      "Epoch: 57 - 00080/00647 - Loss: 0.02967. [ 12 s]\n",
      "Epoch: 57 - 00100/00647 - Loss: 0.02623. [ 15 s]\n",
      "Epoch: 57 - 00120/00647 - Loss: 0.03299. [ 18 s]\n",
      "Epoch: 57 - 00140/00647 - Loss: 0.02941. [ 21 s]\n",
      "Epoch: 57 - 00160/00647 - Loss: 0.01273. [ 24 s]\n",
      "Epoch: 57 - 00180/00647 - Loss: 0.01104. [ 27 s]\n",
      "Epoch: 57 - 00200/00647 - Loss: 0.01427. [ 30 s]\n",
      "Epoch: 57 - 00220/00647 - Loss: 0.01556. [ 33 s]\n",
      "Epoch: 57 - 00240/00647 - Loss: 0.02205. [ 37 s]\n",
      "Epoch: 57 - 00260/00647 - Loss: 0.01205. [ 40 s]\n",
      "Epoch: 57 - 00280/00647 - Loss: 0.02782. [ 43 s]\n",
      "Epoch: 57 - 00300/00647 - Loss: 0.01263. [ 46 s]\n",
      "Epoch: 57 - 00320/00647 - Loss: 0.02533. [ 49 s]\n",
      "Epoch: 57 - 00340/00647 - Loss: 0.05235. [ 52 s]\n",
      "Epoch: 57 - 00360/00647 - Loss: 0.02814. [ 55 s]\n",
      "Epoch: 57 - 00380/00647 - Loss: 0.02476. [ 58 s]\n",
      "Epoch: 57 - 00400/00647 - Loss: 0.01451. [ 61 s]\n",
      "Epoch: 57 - 00420/00647 - Loss: 0.07293. [ 64 s]\n",
      "Epoch: 57 - 00440/00647 - Loss: 0.02408. [ 67 s]\n",
      "Epoch: 57 - 00460/00647 - Loss: 0.03748. [ 70 s]\n",
      "Epoch: 57 - 00480/00647 - Loss: 0.02945. [ 74 s]\n",
      "Epoch: 57 - 00500/00647 - Loss: 0.04987. [ 77 s]\n",
      "Epoch: 57 - 00520/00647 - Loss: 0.01397. [ 80 s]\n",
      "Epoch: 57 - 00540/00647 - Loss: 0.02880. [ 83 s]\n",
      "Epoch: 57 - 00560/00647 - Loss: 0.03376. [ 86 s]\n",
      "Epoch: 57 - 00580/00647 - Loss: 0.03410. [ 89 s]\n",
      "Epoch: 57 - 00600/00647 - Loss: 0.02684. [ 92 s]\n",
      "Epoch: 57 - 00620/00647 - Loss: 0.02436. [ 95 s]\n",
      "Epoch: 57 - 00640/00647 - Loss: 0.03399. [ 98 s]\n",
      "Epoch: 57 - loss(trn/val):0.02435/0.24940, acc(val):94.74%, lr=0.00010. [99s] @25 samples/s \n",
      "Epoch: 58 - 00020/00647 - Loss: 0.03817. [  3 s]\n",
      "Epoch: 58 - 00040/00647 - Loss: 0.01513. [  6 s]\n",
      "Epoch: 58 - 00060/00647 - Loss: 0.01935. [  9 s]\n",
      "Epoch: 58 - 00080/00647 - Loss: 0.01170. [ 12 s]\n",
      "Epoch: 58 - 00100/00647 - Loss: 0.00844. [ 15 s]\n",
      "Epoch: 58 - 00120/00647 - Loss: 0.02931. [ 18 s]\n",
      "Epoch: 58 - 00140/00647 - Loss: 0.01634. [ 21 s]\n",
      "Epoch: 58 - 00160/00647 - Loss: 0.01941. [ 24 s]\n",
      "Epoch: 58 - 00180/00647 - Loss: 0.02105. [ 27 s]\n",
      "Epoch: 58 - 00200/00647 - Loss: 0.01951. [ 30 s]\n",
      "Epoch: 58 - 00220/00647 - Loss: 0.03618. [ 33 s]\n",
      "Epoch: 58 - 00240/00647 - Loss: 0.04871. [ 37 s]\n",
      "Epoch: 58 - 00260/00647 - Loss: 0.03149. [ 40 s]\n",
      "Epoch: 58 - 00280/00647 - Loss: 0.02841. [ 43 s]\n",
      "Epoch: 58 - 00300/00647 - Loss: 0.02168. [ 46 s]\n",
      "Epoch: 58 - 00320/00647 - Loss: 0.03117. [ 49 s]\n",
      "Epoch: 58 - 00340/00647 - Loss: 0.02759. [ 52 s]\n",
      "Epoch: 58 - 00360/00647 - Loss: 0.02435. [ 55 s]\n",
      "Epoch: 58 - 00380/00647 - Loss: 0.05287. [ 58 s]\n",
      "Epoch: 58 - 00400/00647 - Loss: 0.01999. [ 61 s]\n",
      "Epoch: 58 - 00420/00647 - Loss: 0.04974. [ 64 s]\n",
      "Epoch: 58 - 00440/00647 - Loss: 0.02075. [ 67 s]\n",
      "Epoch: 58 - 00460/00647 - Loss: 0.01875. [ 70 s]\n",
      "Epoch: 58 - 00480/00647 - Loss: 0.01517. [ 74 s]\n",
      "Epoch: 58 - 00500/00647 - Loss: 0.01937. [ 77 s]\n",
      "Epoch: 58 - 00520/00647 - Loss: 0.02444. [ 80 s]\n",
      "Epoch: 58 - 00540/00647 - Loss: 0.02901. [ 83 s]\n",
      "Epoch: 58 - 00560/00647 - Loss: 0.01995. [ 86 s]\n",
      "Epoch: 58 - 00580/00647 - Loss: 0.04520. [ 89 s]\n",
      "Epoch: 58 - 00600/00647 - Loss: 0.01639. [ 92 s]\n",
      "Epoch: 58 - 00620/00647 - Loss: 0.02418. [ 95 s]\n",
      "Epoch: 58 - 00640/00647 - Loss: 0.02568. [ 98 s]\n",
      "Epoch: 58 - loss(trn/val):0.02499/0.25852, acc(val):94.86%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 59 - 00020/00647 - Loss: 0.03490. [  3 s]\n",
      "Epoch: 59 - 00040/00647 - Loss: 0.02012. [  6 s]\n",
      "Epoch: 59 - 00060/00647 - Loss: 0.02789. [  9 s]\n",
      "Epoch: 59 - 00080/00647 - Loss: 0.01844. [ 12 s]\n",
      "Epoch: 59 - 00100/00647 - Loss: 0.02439. [ 15 s]\n",
      "Epoch: 59 - 00120/00647 - Loss: 0.02240. [ 18 s]\n",
      "Epoch: 59 - 00140/00647 - Loss: 0.02608. [ 21 s]\n",
      "Epoch: 59 - 00160/00647 - Loss: 0.03265. [ 24 s]\n",
      "Epoch: 59 - 00180/00647 - Loss: 0.02012. [ 27 s]\n",
      "Epoch: 59 - 00200/00647 - Loss: 0.01407. [ 31 s]\n",
      "Epoch: 59 - 00220/00647 - Loss: 0.03476. [ 34 s]\n",
      "Epoch: 59 - 00240/00647 - Loss: 0.01628. [ 37 s]\n",
      "Epoch: 59 - 00260/00647 - Loss: 0.03866. [ 40 s]\n",
      "Epoch: 59 - 00280/00647 - Loss: 0.01541. [ 43 s]\n",
      "Epoch: 59 - 00300/00647 - Loss: 0.02255. [ 46 s]\n",
      "Epoch: 59 - 00320/00647 - Loss: 0.02367. [ 49 s]\n",
      "Epoch: 59 - 00340/00647 - Loss: 0.05580. [ 52 s]\n",
      "Epoch: 59 - 00360/00647 - Loss: 0.03629. [ 55 s]\n",
      "Epoch: 59 - 00380/00647 - Loss: 0.02596. [ 58 s]\n",
      "Epoch: 59 - 00400/00647 - Loss: 0.04620. [ 61 s]\n",
      "Epoch: 59 - 00420/00647 - Loss: 0.02219. [ 64 s]\n",
      "Epoch: 59 - 00440/00647 - Loss: 0.03863. [ 68 s]\n",
      "Epoch: 59 - 00460/00647 - Loss: 0.03703. [ 71 s]\n",
      "Epoch: 59 - 00480/00647 - Loss: 0.01530. [ 74 s]\n",
      "Epoch: 59 - 00500/00647 - Loss: 0.02339. [ 77 s]\n",
      "Epoch: 59 - 00520/00647 - Loss: 0.04370. [ 80 s]\n",
      "Epoch: 59 - 00540/00647 - Loss: 0.04862. [ 83 s]\n",
      "Epoch: 59 - 00560/00647 - Loss: 0.03220. [ 86 s]\n",
      "Epoch: 59 - 00580/00647 - Loss: 0.03210. [ 89 s]\n",
      "Epoch: 59 - 00600/00647 - Loss: 0.01729. [ 92 s]\n",
      "Epoch: 59 - 00620/00647 - Loss: 0.02056. [ 96 s]\n",
      "Epoch: 59 - 00640/00647 - Loss: 0.02085. [ 99 s]\n",
      "Epoch: 59 - loss(trn/val):0.02444/0.21462, acc(val):95.43%, lr=0.00010. [100s] @25 samples/s \n",
      "Epoch: 60 - 00020/00647 - Loss: 0.03530. [  3 s]\n",
      "Epoch: 60 - 00040/00647 - Loss: 0.03435. [  6 s]\n",
      "Epoch: 60 - 00060/00647 - Loss: 0.03561. [  9 s]\n",
      "Epoch: 60 - 00080/00647 - Loss: 0.03769. [ 12 s]\n",
      "Epoch: 60 - 00100/00647 - Loss: 0.02409. [ 15 s]\n",
      "Epoch: 60 - 00120/00647 - Loss: 0.01269. [ 18 s]\n",
      "Epoch: 60 - 00140/00647 - Loss: 0.03131. [ 22 s]\n",
      "Epoch: 60 - 00160/00647 - Loss: 0.05122. [ 25 s]\n",
      "Epoch: 60 - 00180/00647 - Loss: 0.04991. [ 28 s]\n",
      "Epoch: 60 - 00200/00647 - Loss: 0.05078. [ 31 s]\n",
      "Epoch: 60 - 00220/00647 - Loss: 0.06010. [ 34 s]\n",
      "Epoch: 60 - 00240/00647 - Loss: 0.05900. [ 37 s]\n",
      "Epoch: 60 - 00260/00647 - Loss: 0.01372. [ 40 s]\n",
      "Epoch: 60 - 00280/00647 - Loss: 0.04192. [ 43 s]\n",
      "Epoch: 60 - 00300/00647 - Loss: 0.04475. [ 46 s]\n",
      "Epoch: 60 - 00320/00647 - Loss: 0.04943. [ 50 s]\n",
      "Epoch: 60 - 00340/00647 - Loss: 0.01983. [ 53 s]\n",
      "Epoch: 60 - 00360/00647 - Loss: 0.02290. [ 56 s]\n",
      "Epoch: 60 - 00380/00647 - Loss: 0.04355. [ 59 s]\n",
      "Epoch: 60 - 00400/00647 - Loss: 0.01771. [ 62 s]\n",
      "Epoch: 60 - 00420/00647 - Loss: 0.01873. [ 65 s]\n",
      "Epoch: 60 - 00440/00647 - Loss: 0.02997. [ 68 s]\n",
      "Epoch: 60 - 00460/00647 - Loss: 0.02312. [ 71 s]\n",
      "Epoch: 60 - 00480/00647 - Loss: 0.02213. [ 74 s]\n",
      "Epoch: 60 - 00500/00647 - Loss: 0.01422. [ 78 s]\n",
      "Epoch: 60 - 00520/00647 - Loss: 0.01530. [ 81 s]\n",
      "Epoch: 60 - 00540/00647 - Loss: 0.03335. [ 84 s]\n",
      "Epoch: 60 - 00560/00647 - Loss: 0.01228. [ 87 s]\n",
      "Epoch: 60 - 00580/00647 - Loss: 0.03432. [ 90 s]\n",
      "Epoch: 60 - 00600/00647 - Loss: 0.03577. [ 93 s]\n",
      "Epoch: 60 - 00620/00647 - Loss: 0.04289. [ 96 s]\n",
      "Epoch: 60 - 00640/00647 - Loss: 0.02446. [ 99 s]\n",
      "Epoch: 60 - loss(trn/val):0.03928/0.28207, acc(val):93.20%, lr=0.00010. [100s] @25 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.1283 \n",
      "iou=0.8528 \n",
      "acc=0.9486 \n",
      "sensitivity=0.8952 \n",
      "specificity=0.9652 \n",
      "precision=0.9308 \n",
      "f1=0.9048\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d9c81",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7bcb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.128324 at epoch 6\n",
      "Minimum training loss: 0.023834 at epoch 50\n",
      "Maximum validation IoU: 0.886632 at epoch 49\n",
      "Maximum training IoU: 0.963240 at epoch 54\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_FCN_Combined_RMS_lr_e-4\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b217e992df2403db0b9de090ee314288d6f0bee849e7ba4278c2249a611da357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
