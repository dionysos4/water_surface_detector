{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3eee070",
   "metadata": {},
   "source": [
    "# Training ResNet34 based Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37db1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, sys\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path: sys.path.insert(0, dir1)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from ummon import *\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from models.resnet34_encoder_decoder import ResNet, FCN8s\n",
    "from utils.water_segmentation import Water\n",
    "from utils import metrics, dataset_statistics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56673d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.segmentation_analyzer import SegmentationAnalyzer\n",
    "from utils.segmentation_trainer import SegmentationTrainer, SegmentationLogger\n",
    "import json\n",
    "\n",
    "def train(data_loader_trn, data_load_val, opt, model, epochs, path, additional_logger):\n",
    "    \n",
    "   \n",
    "    \n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # training state\n",
    "    trs = Trainingstate()\n",
    "\n",
    "    #Additional logger\n",
    "    seg_logger = additional_logger\n",
    "\n",
    "    # optimizer\n",
    "    #opt = optim.Adam(model.parameters(),lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=False)\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with Logger(loglevel=10, logdir='.', log_batch_interval=20) as lg:\n",
    "    \n",
    "        # trainer\n",
    "        trn = SegmentationTrainer(lg, model, loss, opt, trainingstate=trs, model_filename=path, \n",
    "                              combined_training_epochs=0, additional_logger=seg_logger)\n",
    "    \n",
    "        # train\n",
    "        trn.fit(data_loader_trn, epochs=epochs, validation_set=data_loader_val, analyzer=SegmentationAnalyzer)\n",
    "    \n",
    "    \n",
    "        ## evaluate on test set\n",
    "        trs.load_weights_best_validation_(model)\n",
    "        ev = SegmentationAnalyzer.evaluate(model, loss, data_loader_val, lg)\n",
    "        lg.info(\"Performance on validation set: \\nloss={:6.4f} \\niou={:.4f} \\nacc={:.4f} \\nsensitivity={:.4f} \\nspecificity={:.4f} \\nprecision={:.4f} \\nf1={:.4f}\".format(\n",
    "            ev[\"loss\"], ev[\"iou\"],ev[\"accuracy\"], ev[\"sensitivity\"], ev[\"specificity\"], ev[\"precision\"], ev[\"f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb805a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_training(data):\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "    plt.rcParams['axes.xmargin'] = 0\n",
    "    plt.plot(data[\"epoch_trn\"], data[\"loss_trn\"], label=\"training loss\")\n",
    "    plt.plot(data[\"epoch_trn\"], data[\"loss_val\"], label=\"validation loss\")\n",
    "    plt.xlabel('Epochen')\n",
    "    plt.ylabel('BCE loss')\n",
    "    plt.legend(loc='best')   \n",
    "    plt.show()\n",
    "    print(\"Minimum validation loss: {0:f} at epoch {1}\".format(min(data[\"loss_val\"]), data[\"loss_val\"].index(min(data[\"loss_val\"]))+1))\n",
    "    print(\"Minimum training loss: {0:f} at epoch {1}\".format(min(data[\"loss_trn\"]), data[\"loss_trn\"].index(min(data[\"loss_trn\"]))+1))\n",
    "    \n",
    "    print(\"Maximum validation IoU: {0:f} at epoch {1}\".format(max(data[\"iou_val\"]), data[\"iou_val\"].index(max(data[\"iou_val\"]))+1))\n",
    "    print(\"Maximum training IoU: {0:f} at epoch {1}\".format(max(data[\"iou_trn\"]), data[\"iou_trn\"].index(max(data[\"iou_trn\"]))+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc4ddc",
   "metadata": {},
   "source": [
    "# Training on \"Tampere\" Dataset only\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2728cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_Enc_Dec_Tamp_RMS_lr_e-4'\n",
    "directory = \"../data/training_states/ResNet34_Enc_Dec\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0\n",
    "\n",
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "res_model = ResNet(resnet34)\n",
    "model = FCN8s(res_model, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550a40f",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d4e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 images in training dataset\n",
      "200 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.TAMP_OPEN_DOCK\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=['open','dock'], data_list_misc=[],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=['channel'], data_list_misc=[],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569fbebb",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c360ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "FCN8s(\n",
      "  (pretrained_net): ResNet(\n",
      "    (resnet): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (intermediate): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (deconv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Trainable params:   23513705\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training               400    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-2.0 max:2.2 mean:-0.5 std:1.1 / Labels:min:0.0 max:1.0 mean:0.2 std:0.4\n",
      "Validation             200    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-2.0 max:2.2 mean:-0.2 std:1.0 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00100 - Loss: 0.10121. [  5 s]\n",
      "Epoch: 1 - 00040/00100 - Loss: 0.06936. [  9 s]\n",
      "Epoch: 1 - 00060/00100 - Loss: 0.05907. [ 14 s]\n",
      "Epoch: 1 - 00080/00100 - Loss: 0.04705. [ 18 s]\n",
      "Epoch: 1 - 00100/00100 - Loss: 0.04440. [ 23 s]\n",
      "Epoch: 1 - loss(trn/val):0.04722/0.08143, acc(val):98.02%, lr=0.00010 [BEST]. [23s] @17 samples/s \n",
      "Epoch: 2 - 00020/00100 - Loss: 0.04174. [  5 s]\n",
      "Epoch: 2 - 00040/00100 - Loss: 0.04196. [  9 s]\n",
      "Epoch: 2 - 00060/00100 - Loss: 0.05039. [ 14 s]\n",
      "Epoch: 2 - 00080/00100 - Loss: 0.04692. [ 18 s]\n",
      "Epoch: 2 - 00100/00100 - Loss: 0.03607. [ 23 s]\n",
      "Epoch: 2 - loss(trn/val):0.03548/0.06389, acc(val):98.43%, lr=0.00010 [BEST]. [23s] @17 samples/s \n",
      "Epoch: 3 - 00020/00100 - Loss: 0.03685. [  5 s]\n",
      "Epoch: 3 - 00040/00100 - Loss: 0.02966. [  9 s]\n",
      "Epoch: 3 - 00060/00100 - Loss: 0.03874. [ 14 s]\n",
      "Epoch: 3 - 00080/00100 - Loss: 0.04157. [ 18 s]\n",
      "Epoch: 3 - 00100/00100 - Loss: 0.02960. [ 23 s]\n",
      "Epoch: 3 - loss(trn/val):0.02833/0.06018, acc(val):98.42%, lr=0.00010 [BEST]. [23s] @17 samples/s \n",
      "Epoch: 4 - 00020/00100 - Loss: 0.02527. [  5 s]\n",
      "Epoch: 4 - 00040/00100 - Loss: 0.03048. [  9 s]\n",
      "Epoch: 4 - 00060/00100 - Loss: 0.03028. [ 14 s]\n",
      "Epoch: 4 - 00080/00100 - Loss: 0.03321. [ 18 s]\n",
      "Epoch: 4 - 00100/00100 - Loss: 0.01922. [ 23 s]\n",
      "Epoch: 4 - loss(trn/val):0.02406/0.04924, acc(val):98.72%, lr=0.00010 [BEST]. [23s] @17 samples/s \n",
      "Epoch: 5 - 00020/00100 - Loss: 0.02358. [  5 s]\n",
      "Epoch: 5 - 00040/00100 - Loss: 0.01991. [ 10 s]\n",
      "Epoch: 5 - 00060/00100 - Loss: 0.03016. [ 14 s]\n",
      "Epoch: 5 - 00080/00100 - Loss: 0.01839. [ 19 s]\n",
      "Epoch: 5 - 00100/00100 - Loss: 0.01917. [ 24 s]\n",
      "Epoch: 5 - loss(trn/val):0.01955/0.05442, acc(val):98.41%, lr=0.00010. [24s] @16 samples/s \n",
      "Epoch: 6 - 00020/00100 - Loss: 0.02784. [  5 s]\n",
      "Epoch: 6 - 00040/00100 - Loss: 0.06716. [  9 s]\n",
      "Epoch: 6 - 00060/00100 - Loss: 0.01389. [ 14 s]\n",
      "Epoch: 6 - 00080/00100 - Loss: 0.01445. [ 18 s]\n",
      "Epoch: 6 - 00100/00100 - Loss: 0.01831. [ 23 s]\n",
      "Epoch: 6 - loss(trn/val):0.01645/0.05853, acc(val):98.36%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 7 - 00020/00100 - Loss: 0.01130. [  5 s]\n",
      "Epoch: 7 - 00040/00100 - Loss: 0.01260. [  9 s]\n",
      "Epoch: 7 - 00060/00100 - Loss: 0.01190. [ 14 s]\n",
      "Epoch: 7 - 00080/00100 - Loss: 0.01315. [ 18 s]\n",
      "Epoch: 7 - 00100/00100 - Loss: 0.01413. [ 23 s]\n",
      "Epoch: 7 - loss(trn/val):0.01428/0.04837, acc(val):98.62%, lr=0.00010 [BEST]. [23s] @16 samples/s \n",
      "Epoch: 8 - 00020/00100 - Loss: 0.01200. [  5 s]\n",
      "Epoch: 8 - 00040/00100 - Loss: 0.01622. [  9 s]\n",
      "Epoch: 8 - 00060/00100 - Loss: 0.01479. [ 14 s]\n",
      "Epoch: 8 - 00080/00100 - Loss: 0.01153. [ 18 s]\n",
      "Epoch: 8 - 00100/00100 - Loss: 0.01421. [ 23 s]\n",
      "Epoch: 8 - loss(trn/val):0.01285/0.04853, acc(val):98.62%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 9 - 00020/00100 - Loss: 0.00933. [  5 s]\n",
      "Epoch: 9 - 00040/00100 - Loss: 0.01075. [  9 s]\n",
      "Epoch: 9 - 00060/00100 - Loss: 0.01662. [ 14 s]\n",
      "Epoch: 9 - 00080/00100 - Loss: 0.00931. [ 18 s]\n",
      "Epoch: 9 - 00100/00100 - Loss: 0.00969. [ 23 s]\n",
      "Epoch: 9 - loss(trn/val):0.01160/0.04623, acc(val):98.70%, lr=0.00010 [BEST]. [23s] @16 samples/s \n",
      "Epoch: 10 - 00020/00100 - Loss: 0.01116. [  5 s]\n",
      "Epoch: 10 - 00040/00100 - Loss: 0.01029. [  9 s]\n",
      "Epoch: 10 - 00060/00100 - Loss: 0.00733. [ 14 s]\n",
      "Epoch: 10 - 00080/00100 - Loss: 0.00892. [ 18 s]\n",
      "Epoch: 10 - 00100/00100 - Loss: 0.00735. [ 23 s]\n",
      "Epoch: 10 - loss(trn/val):0.01004/0.04468, acc(val):98.67%, lr=0.00010 [BEST]. [23s] @16 samples/s \n",
      "Epoch: 11 - 00020/00100 - Loss: 0.02998. [  5 s]\n",
      "Epoch: 11 - 00040/00100 - Loss: 0.01207. [  9 s]\n",
      "Epoch: 11 - 00060/00100 - Loss: 0.01158. [ 14 s]\n",
      "Epoch: 11 - 00080/00100 - Loss: 0.00779. [ 18 s]\n",
      "Epoch: 11 - 00100/00100 - Loss: 0.01234. [ 23 s]\n",
      "Epoch: 11 - loss(trn/val):0.00937/0.07076, acc(val):98.21%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 12 - 00020/00100 - Loss: 0.00849. [  5 s]\n",
      "Epoch: 12 - 00040/00100 - Loss: 0.01894. [  9 s]\n",
      "Epoch: 12 - 00060/00100 - Loss: 0.01086. [ 14 s]\n",
      "Epoch: 12 - 00080/00100 - Loss: 0.00951. [ 18 s]\n",
      "Epoch: 12 - 00100/00100 - Loss: 0.00964. [ 23 s]\n",
      "Epoch: 12 - loss(trn/val):0.00915/0.04199, acc(val):98.78%, lr=0.00010 [BEST]. [23s] @17 samples/s \n",
      "Epoch: 13 - 00020/00100 - Loss: 0.01019. [  5 s]\n",
      "Epoch: 13 - 00040/00100 - Loss: 0.01044. [  9 s]\n",
      "Epoch: 13 - 00060/00100 - Loss: 0.00683. [ 14 s]\n",
      "Epoch: 13 - 00080/00100 - Loss: 0.00924. [ 18 s]\n",
      "Epoch: 13 - 00100/00100 - Loss: 0.01495. [ 23 s]\n",
      "Epoch: 13 - loss(trn/val):0.01118/0.05589, acc(val):98.37%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 14 - 00020/00100 - Loss: 0.00834. [  5 s]\n",
      "Epoch: 14 - 00040/00100 - Loss: 0.00618. [  9 s]\n",
      "Epoch: 14 - 00060/00100 - Loss: 0.01221. [ 14 s]\n",
      "Epoch: 14 - 00080/00100 - Loss: 0.01627. [ 18 s]\n",
      "Epoch: 14 - 00100/00100 - Loss: 0.00894. [ 23 s]\n",
      "Epoch: 14 - loss(trn/val):0.01072/0.05822, acc(val):98.29%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 15 - 00020/00100 - Loss: 0.00645. [  5 s]\n",
      "Epoch: 15 - 00040/00100 - Loss: 0.00956. [  9 s]\n",
      "Epoch: 15 - 00060/00100 - Loss: 0.01059. [ 14 s]\n",
      "Epoch: 15 - 00080/00100 - Loss: 0.00594. [ 18 s]\n",
      "Epoch: 15 - 00100/00100 - Loss: 0.00599. [ 23 s]\n",
      "Epoch: 15 - loss(trn/val):0.00783/0.06966, acc(val):98.15%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 16 - 00020/00100 - Loss: 0.00696. [  5 s]\n",
      "Epoch: 16 - 00040/00100 - Loss: 0.01101. [  9 s]\n",
      "Epoch: 16 - 00060/00100 - Loss: 0.01319. [ 14 s]\n",
      "Epoch: 16 - 00080/00100 - Loss: 0.00734. [ 18 s]\n",
      "Epoch: 16 - 00100/00100 - Loss: 0.01059. [ 23 s]\n",
      "Epoch: 16 - loss(trn/val):0.00825/0.05245, acc(val):98.57%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 17 - 00020/00100 - Loss: 0.00547. [  5 s]\n",
      "Epoch: 17 - 00040/00100 - Loss: 0.00902. [  9 s]\n",
      "Epoch: 17 - 00060/00100 - Loss: 0.00896. [ 14 s]\n",
      "Epoch: 17 - 00080/00100 - Loss: 0.01227. [ 18 s]\n",
      "Epoch: 17 - 00100/00100 - Loss: 0.01411. [ 23 s]\n",
      "Epoch: 17 - loss(trn/val):0.00772/0.05585, acc(val):98.51%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 18 - 00020/00100 - Loss: 0.00507. [  5 s]\n",
      "Epoch: 18 - 00040/00100 - Loss: 0.00497. [  9 s]\n",
      "Epoch: 18 - 00060/00100 - Loss: 0.00996. [ 14 s]\n",
      "Epoch: 18 - 00080/00100 - Loss: 0.00772. [ 18 s]\n",
      "Epoch: 18 - 00100/00100 - Loss: 0.00643. [ 23 s]\n",
      "Epoch: 18 - loss(trn/val):0.00650/0.06630, acc(val):98.35%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 19 - 00020/00100 - Loss: 0.00589. [  5 s]\n",
      "Epoch: 19 - 00040/00100 - Loss: 0.00604. [  9 s]\n",
      "Epoch: 19 - 00060/00100 - Loss: 0.00777. [ 14 s]\n",
      "Epoch: 19 - 00080/00100 - Loss: 0.00835. [ 18 s]\n",
      "Epoch: 19 - 00100/00100 - Loss: 0.00736. [ 23 s]\n",
      "Epoch: 19 - loss(trn/val):0.00694/0.08668, acc(val):98.00%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 20 - 00020/00100 - Loss: 0.00531. [  5 s]\n",
      "Epoch: 20 - 00040/00100 - Loss: 0.00752. [  9 s]\n",
      "Epoch: 20 - 00060/00100 - Loss: 0.00660. [ 14 s]\n",
      "Epoch: 20 - 00080/00100 - Loss: 0.00499. [ 18 s]\n",
      "Epoch: 20 - 00100/00100 - Loss: 0.03492. [ 23 s]\n",
      "Epoch: 20 - loss(trn/val):0.01943/0.08144, acc(val):97.96%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 21 - 00020/00100 - Loss: 0.00729. [  5 s]\n",
      "Epoch: 21 - 00040/00100 - Loss: 0.00643. [  9 s]\n",
      "Epoch: 21 - 00060/00100 - Loss: 0.00458. [ 14 s]\n",
      "Epoch: 21 - 00080/00100 - Loss: 0.00479. [ 18 s]\n",
      "Epoch: 21 - 00100/00100 - Loss: 0.00901. [ 23 s]\n",
      "Epoch: 21 - loss(trn/val):0.00620/0.05325, acc(val):98.67%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 22 - 00020/00100 - Loss: 0.00895. [  5 s]\n",
      "Epoch: 22 - 00040/00100 - Loss: 0.00517. [  9 s]\n",
      "Epoch: 22 - 00060/00100 - Loss: 0.00529. [ 14 s]\n",
      "Epoch: 22 - 00080/00100 - Loss: 0.00609. [ 18 s]\n",
      "Epoch: 22 - 00100/00100 - Loss: 0.00502. [ 23 s]\n",
      "Epoch: 22 - loss(trn/val):0.00553/0.04458, acc(val):98.84%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 23 - 00020/00100 - Loss: 0.00485. [  5 s]\n",
      "Epoch: 23 - 00040/00100 - Loss: 0.00502. [  9 s]\n",
      "Epoch: 23 - 00060/00100 - Loss: 0.00699. [ 14 s]\n",
      "Epoch: 23 - 00080/00100 - Loss: 0.00584. [ 18 s]\n",
      "Epoch: 23 - 00100/00100 - Loss: 0.00750. [ 23 s]\n",
      "Epoch: 23 - loss(trn/val):0.00574/0.04353, acc(val):98.81%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 24 - 00020/00100 - Loss: 0.00612. [  5 s]\n",
      "Epoch: 24 - 00040/00100 - Loss: 0.00561. [  9 s]\n",
      "Epoch: 24 - 00060/00100 - Loss: 0.00504. [ 14 s]\n",
      "Epoch: 24 - 00080/00100 - Loss: 0.00617. [ 18 s]\n",
      "Epoch: 24 - 00100/00100 - Loss: 0.00348. [ 23 s]\n",
      "Epoch: 24 - loss(trn/val):0.00512/0.05253, acc(val):98.69%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 25 - 00020/00100 - Loss: 0.00494. [  5 s]\n",
      "Epoch: 25 - 00040/00100 - Loss: 0.00590. [  9 s]\n",
      "Epoch: 25 - 00060/00100 - Loss: 0.00598. [ 14 s]\n",
      "Epoch: 25 - 00080/00100 - Loss: 0.00587. [ 18 s]\n",
      "Epoch: 25 - 00100/00100 - Loss: 0.00976. [ 23 s]\n",
      "Epoch: 25 - loss(trn/val):0.00586/0.05916, acc(val):98.58%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 26 - 00020/00100 - Loss: 0.01051. [  5 s]\n",
      "Epoch: 26 - 00040/00100 - Loss: 0.00575. [  9 s]\n",
      "Epoch: 26 - 00060/00100 - Loss: 0.00494. [ 14 s]\n",
      "Epoch: 26 - 00080/00100 - Loss: 0.00429. [ 18 s]\n",
      "Epoch: 26 - 00100/00100 - Loss: 0.00487. [ 23 s]\n",
      "Epoch: 26 - loss(trn/val):0.00569/0.05965, acc(val):98.53%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 27 - 00020/00100 - Loss: 0.00734. [  5 s]\n",
      "Epoch: 27 - 00040/00100 - Loss: 0.00415. [  9 s]\n",
      "Epoch: 27 - 00060/00100 - Loss: 0.00838. [ 14 s]\n",
      "Epoch: 27 - 00080/00100 - Loss: 0.00533. [ 18 s]\n",
      "Epoch: 27 - 00100/00100 - Loss: 0.00953. [ 23 s]\n",
      "Epoch: 27 - loss(trn/val):0.00558/0.04961, acc(val):98.63%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 28 - 00020/00100 - Loss: 0.00605. [  5 s]\n",
      "Epoch: 28 - 00040/00100 - Loss: 0.00614. [  9 s]\n",
      "Epoch: 28 - 00060/00100 - Loss: 0.00674. [ 14 s]\n",
      "Epoch: 28 - 00080/00100 - Loss: 0.00376. [ 19 s]\n",
      "Epoch: 28 - 00100/00100 - Loss: 0.00428. [ 23 s]\n",
      "Epoch: 28 - loss(trn/val):0.00494/0.05860, acc(val):98.55%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 29 - 00020/00100 - Loss: 0.00403. [  5 s]\n",
      "Epoch: 29 - 00040/00100 - Loss: 0.00622. [  9 s]\n",
      "Epoch: 29 - 00060/00100 - Loss: 0.00977. [ 14 s]\n",
      "Epoch: 29 - 00080/00100 - Loss: 0.00673. [ 18 s]\n",
      "Epoch: 29 - 00100/00100 - Loss: 0.00485. [ 23 s]\n",
      "Epoch: 29 - loss(trn/val):0.00487/0.06128, acc(val):98.58%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 30 - 00020/00100 - Loss: 0.00869. [  5 s]\n",
      "Epoch: 30 - 00040/00100 - Loss: 0.00757. [  9 s]\n",
      "Epoch: 30 - 00060/00100 - Loss: 0.00464. [ 14 s]\n",
      "Epoch: 30 - 00080/00100 - Loss: 0.00856. [ 18 s]\n",
      "Epoch: 30 - 00100/00100 - Loss: 0.00497. [ 23 s]\n",
      "Epoch: 30 - loss(trn/val):0.00480/0.05661, acc(val):98.68%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 31 - 00020/00100 - Loss: 0.01286. [  5 s]\n",
      "Epoch: 31 - 00040/00100 - Loss: 0.00514. [  9 s]\n",
      "Epoch: 31 - 00060/00100 - Loss: 0.00452. [ 14 s]\n",
      "Epoch: 31 - 00080/00100 - Loss: 0.00484. [ 18 s]\n",
      "Epoch: 31 - 00100/00100 - Loss: 0.00389. [ 23 s]\n",
      "Epoch: 31 - loss(trn/val):0.00491/0.07045, acc(val):98.49%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 32 - 00020/00100 - Loss: 0.00373. [  5 s]\n",
      "Epoch: 32 - 00040/00100 - Loss: 0.00481. [  9 s]\n",
      "Epoch: 32 - 00060/00100 - Loss: 0.00483. [ 14 s]\n",
      "Epoch: 32 - 00080/00100 - Loss: 0.00441. [ 18 s]\n",
      "Epoch: 32 - 00100/00100 - Loss: 0.00616. [ 23 s]\n",
      "Epoch: 32 - loss(trn/val):0.00507/0.07851, acc(val):98.24%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 33 - 00020/00100 - Loss: 0.00567. [  5 s]\n",
      "Epoch: 33 - 00040/00100 - Loss: 0.00694. [  9 s]\n",
      "Epoch: 33 - 00060/00100 - Loss: 0.00428. [ 14 s]\n",
      "Epoch: 33 - 00080/00100 - Loss: 0.00571. [ 19 s]\n",
      "Epoch: 33 - 00100/00100 - Loss: 0.00826. [ 23 s]\n",
      "Epoch: 33 - loss(trn/val):0.00619/0.06817, acc(val):98.42%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 34 - 00020/00100 - Loss: 0.02457. [  5 s]\n",
      "Epoch: 34 - 00040/00100 - Loss: 0.00504. [  9 s]\n",
      "Epoch: 34 - 00060/00100 - Loss: 0.00453. [ 14 s]\n",
      "Epoch: 34 - 00080/00100 - Loss: 0.00406. [ 18 s]\n",
      "Epoch: 34 - 00100/00100 - Loss: 0.00353. [ 23 s]\n",
      "Epoch: 34 - loss(trn/val):0.00463/0.07509, acc(val):98.12%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 35 - 00020/00100 - Loss: 0.00369. [  5 s]\n",
      "Epoch: 35 - 00040/00100 - Loss: 0.00574. [  9 s]\n",
      "Epoch: 35 - 00060/00100 - Loss: 0.00346. [ 14 s]\n",
      "Epoch: 35 - 00080/00100 - Loss: 0.00455. [ 18 s]\n",
      "Epoch: 35 - 00100/00100 - Loss: 0.00487. [ 23 s]\n",
      "Epoch: 35 - loss(trn/val):0.00429/0.06631, acc(val):98.31%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 36 - 00020/00100 - Loss: 0.00534. [  5 s]\n",
      "Epoch: 36 - 00040/00100 - Loss: 0.00479. [  9 s]\n",
      "Epoch: 36 - 00060/00100 - Loss: 0.00381. [ 14 s]\n",
      "Epoch: 36 - 00080/00100 - Loss: 0.00331. [ 18 s]\n",
      "Epoch: 36 - 00100/00100 - Loss: 0.00397. [ 23 s]\n",
      "Epoch: 36 - loss(trn/val):0.00430/0.08098, acc(val):98.15%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 37 - 00020/00100 - Loss: 0.00459. [  5 s]\n",
      "Epoch: 37 - 00040/00100 - Loss: 0.00355. [  9 s]\n",
      "Epoch: 37 - 00060/00100 - Loss: 0.00495. [ 14 s]\n",
      "Epoch: 37 - 00080/00100 - Loss: 0.00356. [ 18 s]\n",
      "Epoch: 37 - 00100/00100 - Loss: 0.00377. [ 23 s]\n",
      "Epoch: 37 - loss(trn/val):0.00435/0.07398, acc(val):98.25%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 38 - 00020/00100 - Loss: 0.00401. [  5 s]\n",
      "Epoch: 38 - 00040/00100 - Loss: 0.00424. [  9 s]\n",
      "Epoch: 38 - 00060/00100 - Loss: 0.00449. [ 14 s]\n",
      "Epoch: 38 - 00080/00100 - Loss: 0.00285. [ 18 s]\n",
      "Epoch: 38 - 00100/00100 - Loss: 0.00427. [ 23 s]\n",
      "Epoch: 38 - loss(trn/val):0.00431/0.06114, acc(val):98.50%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 39 - 00020/00100 - Loss: 0.00440. [  5 s]\n",
      "Epoch: 39 - 00040/00100 - Loss: 0.00430. [  9 s]\n",
      "Epoch: 39 - 00060/00100 - Loss: 0.00615. [ 14 s]\n",
      "Epoch: 39 - 00080/00100 - Loss: 0.00367. [ 18 s]\n",
      "Epoch: 39 - 00100/00100 - Loss: 0.00456. [ 23 s]\n",
      "Epoch: 39 - loss(trn/val):0.00405/0.06620, acc(val):98.48%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 40 - 00020/00100 - Loss: 0.00352. [  5 s]\n",
      "Epoch: 40 - 00040/00100 - Loss: 0.00377. [  9 s]\n",
      "Epoch: 40 - 00060/00100 - Loss: 0.00398. [ 14 s]\n",
      "Epoch: 40 - 00080/00100 - Loss: 0.00384. [ 18 s]\n",
      "Epoch: 40 - 00100/00100 - Loss: 0.00630. [ 23 s]\n",
      "Epoch: 40 - loss(trn/val):0.00405/0.06855, acc(val):98.43%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 41 - 00020/00100 - Loss: 0.00571. [  5 s]\n",
      "Epoch: 41 - 00040/00100 - Loss: 0.00430. [  9 s]\n",
      "Epoch: 41 - 00060/00100 - Loss: 0.00401. [ 14 s]\n",
      "Epoch: 41 - 00080/00100 - Loss: 0.00357. [ 18 s]\n",
      "Epoch: 41 - 00100/00100 - Loss: 0.00313. [ 23 s]\n",
      "Epoch: 41 - loss(trn/val):0.00396/0.07196, acc(val):98.35%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 42 - 00020/00100 - Loss: 0.00536. [  5 s]\n",
      "Epoch: 42 - 00040/00100 - Loss: 0.00380. [  9 s]\n",
      "Epoch: 42 - 00060/00100 - Loss: 0.00427. [ 14 s]\n",
      "Epoch: 42 - 00080/00100 - Loss: 0.00449. [ 18 s]\n",
      "Epoch: 42 - 00100/00100 - Loss: 0.00445. [ 23 s]\n",
      "Epoch: 42 - loss(trn/val):0.00429/0.06663, acc(val):98.55%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 43 - 00020/00100 - Loss: 0.00641. [  5 s]\n",
      "Epoch: 43 - 00040/00100 - Loss: 0.00450. [  9 s]\n",
      "Epoch: 43 - 00060/00100 - Loss: 0.00564. [ 14 s]\n",
      "Epoch: 43 - 00080/00100 - Loss: 0.00426. [ 19 s]\n",
      "Epoch: 43 - 00100/00100 - Loss: 0.00292. [ 23 s]\n",
      "Epoch: 43 - loss(trn/val):0.00399/0.06865, acc(val):98.48%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 44 - 00020/00100 - Loss: 0.00669. [  5 s]\n",
      "Epoch: 44 - 00040/00100 - Loss: 0.00513. [  9 s]\n",
      "Epoch: 44 - 00060/00100 - Loss: 0.00455. [ 14 s]\n",
      "Epoch: 44 - 00080/00100 - Loss: 0.00787. [ 18 s]\n",
      "Epoch: 44 - 00100/00100 - Loss: 0.00526. [ 23 s]\n",
      "Epoch: 44 - loss(trn/val):0.00405/0.07725, acc(val):98.37%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 45 - 00020/00100 - Loss: 0.00288. [  5 s]\n",
      "Epoch: 45 - 00040/00100 - Loss: 0.00355. [  9 s]\n",
      "Epoch: 45 - 00060/00100 - Loss: 0.00385. [ 14 s]\n",
      "Epoch: 45 - 00080/00100 - Loss: 0.00459. [ 18 s]\n",
      "Epoch: 45 - 00100/00100 - Loss: 0.00516. [ 23 s]\n",
      "Epoch: 45 - loss(trn/val):0.00447/0.07654, acc(val):98.36%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 46 - 00020/00100 - Loss: 0.00406. [  5 s]\n",
      "Epoch: 46 - 00040/00100 - Loss: 0.00319. [  9 s]\n",
      "Epoch: 46 - 00060/00100 - Loss: 0.00389. [ 14 s]\n",
      "Epoch: 46 - 00080/00100 - Loss: 0.00326. [ 18 s]\n",
      "Epoch: 46 - 00100/00100 - Loss: 0.00427. [ 23 s]\n",
      "Epoch: 46 - loss(trn/val):0.00398/0.07738, acc(val):98.37%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 47 - 00020/00100 - Loss: 0.00298. [  5 s]\n",
      "Epoch: 47 - 00040/00100 - Loss: 0.01728. [  9 s]\n",
      "Epoch: 47 - 00060/00100 - Loss: 0.00716. [ 14 s]\n",
      "Epoch: 47 - 00080/00100 - Loss: 0.00331. [ 18 s]\n",
      "Epoch: 47 - 00100/00100 - Loss: 0.00427. [ 23 s]\n",
      "Epoch: 47 - loss(trn/val):0.00407/0.05039, acc(val):98.80%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 48 - 00020/00100 - Loss: 0.00389. [  5 s]\n",
      "Epoch: 48 - 00040/00100 - Loss: 0.00418. [  9 s]\n",
      "Epoch: 48 - 00060/00100 - Loss: 0.00383. [ 14 s]\n",
      "Epoch: 48 - 00080/00100 - Loss: 0.00253. [ 18 s]\n",
      "Epoch: 48 - 00100/00100 - Loss: 0.00401. [ 23 s]\n",
      "Epoch: 48 - loss(trn/val):0.00360/0.05343, acc(val):98.79%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 49 - 00020/00100 - Loss: 0.00309. [  5 s]\n",
      "Epoch: 49 - 00040/00100 - Loss: 0.00325. [  9 s]\n",
      "Epoch: 49 - 00060/00100 - Loss: 0.00441. [ 14 s]\n",
      "Epoch: 49 - 00080/00100 - Loss: 0.00429. [ 18 s]\n",
      "Epoch: 49 - 00100/00100 - Loss: 0.00387. [ 23 s]\n",
      "Epoch: 49 - loss(trn/val):0.00344/0.05189, acc(val):98.85%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 50 - 00020/00100 - Loss: 0.00534. [  5 s]\n",
      "Epoch: 50 - 00040/00100 - Loss: 0.00542. [  9 s]\n",
      "Epoch: 50 - 00060/00100 - Loss: 0.00401. [ 14 s]\n",
      "Epoch: 50 - 00080/00100 - Loss: 0.00322. [ 18 s]\n",
      "Epoch: 50 - 00100/00100 - Loss: 0.00394. [ 23 s]\n",
      "Epoch: 50 - loss(trn/val):0.00363/0.06208, acc(val):98.61%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 51 - 00020/00100 - Loss: 0.00345. [  5 s]\n",
      "Epoch: 51 - 00040/00100 - Loss: 0.00408. [  9 s]\n",
      "Epoch: 51 - 00060/00100 - Loss: 0.00384. [ 14 s]\n",
      "Epoch: 51 - 00080/00100 - Loss: 0.00442. [ 18 s]\n",
      "Epoch: 51 - 00100/00100 - Loss: 0.00334. [ 23 s]\n",
      "Epoch: 51 - loss(trn/val):0.00343/0.06583, acc(val):98.63%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 52 - 00020/00100 - Loss: 0.00495. [  5 s]\n",
      "Epoch: 52 - 00040/00100 - Loss: 0.00714. [  9 s]\n",
      "Epoch: 52 - 00060/00100 - Loss: 0.00313. [ 14 s]\n",
      "Epoch: 52 - 00080/00100 - Loss: 0.00274. [ 18 s]\n",
      "Epoch: 52 - 00100/00100 - Loss: 0.00364. [ 23 s]\n",
      "Epoch: 52 - loss(trn/val):0.00331/0.08301, acc(val):98.20%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 53 - 00020/00100 - Loss: 0.00330. [  5 s]\n",
      "Epoch: 53 - 00040/00100 - Loss: 0.00330. [  9 s]\n",
      "Epoch: 53 - 00060/00100 - Loss: 0.00310. [ 14 s]\n",
      "Epoch: 53 - 00080/00100 - Loss: 0.00334. [ 18 s]\n",
      "Epoch: 53 - 00100/00100 - Loss: 0.00353. [ 23 s]\n",
      "Epoch: 53 - loss(trn/val):0.00367/0.08220, acc(val):98.34%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 54 - 00020/00100 - Loss: 0.00294. [  5 s]\n",
      "Epoch: 54 - 00040/00100 - Loss: 0.00312. [  9 s]\n",
      "Epoch: 54 - 00060/00100 - Loss: 0.00363. [ 14 s]\n",
      "Epoch: 54 - 00080/00100 - Loss: 0.00342. [ 18 s]\n",
      "Epoch: 54 - 00100/00100 - Loss: 0.00312. [ 23 s]\n",
      "Epoch: 54 - loss(trn/val):0.00352/0.07786, acc(val):98.34%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 55 - 00020/00100 - Loss: 0.00408. [  5 s]\n",
      "Epoch: 55 - 00040/00100 - Loss: 0.00329. [  9 s]\n",
      "Epoch: 55 - 00060/00100 - Loss: 0.00274. [ 14 s]\n",
      "Epoch: 55 - 00080/00100 - Loss: 0.00401. [ 18 s]\n",
      "Epoch: 55 - 00100/00100 - Loss: 0.00577. [ 23 s]\n",
      "Epoch: 55 - loss(trn/val):0.00415/0.11074, acc(val):97.89%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 56 - 00020/00100 - Loss: 0.00278. [  5 s]\n",
      "Epoch: 56 - 00040/00100 - Loss: 0.00274. [  9 s]\n",
      "Epoch: 56 - 00060/00100 - Loss: 0.00627. [ 14 s]\n",
      "Epoch: 56 - 00080/00100 - Loss: 0.00275. [ 18 s]\n",
      "Epoch: 56 - 00100/00100 - Loss: 0.00452. [ 23 s]\n",
      "Epoch: 56 - loss(trn/val):0.00427/0.08123, acc(val):98.34%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 57 - 00020/00100 - Loss: 0.00358. [  5 s]\n",
      "Epoch: 57 - 00040/00100 - Loss: 0.00362. [  9 s]\n",
      "Epoch: 57 - 00060/00100 - Loss: 0.00316. [ 14 s]\n",
      "Epoch: 57 - 00080/00100 - Loss: 0.00314. [ 18 s]\n",
      "Epoch: 57 - 00100/00100 - Loss: 0.00502. [ 23 s]\n",
      "Epoch: 57 - loss(trn/val):0.00360/0.08842, acc(val):98.17%, lr=0.00010. [23s] @17 samples/s \n",
      "Epoch: 58 - 00020/00100 - Loss: 0.00416. [  5 s]\n",
      "Epoch: 58 - 00040/00100 - Loss: 0.00245. [  9 s]\n",
      "Epoch: 58 - 00060/00100 - Loss: 0.00367. [ 14 s]\n",
      "Epoch: 58 - 00080/00100 - Loss: 0.00348. [ 18 s]\n",
      "Epoch: 58 - 00100/00100 - Loss: 0.00292. [ 23 s]\n",
      "Epoch: 58 - loss(trn/val):0.00322/0.09861, acc(val):98.11%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 59 - 00020/00100 - Loss: 0.00343. [  5 s]\n",
      "Epoch: 59 - 00040/00100 - Loss: 0.00318. [  9 s]\n",
      "Epoch: 59 - 00060/00100 - Loss: 0.00332. [ 14 s]\n",
      "Epoch: 59 - 00080/00100 - Loss: 0.00271. [ 18 s]\n",
      "Epoch: 59 - 00100/00100 - Loss: 0.00299. [ 23 s]\n",
      "Epoch: 59 - loss(trn/val):0.00316/0.10121, acc(val):98.10%, lr=0.00010. [23s] @16 samples/s \n",
      "Epoch: 60 - 00020/00100 - Loss: 0.00274. [  5 s]\n",
      "Epoch: 60 - 00040/00100 - Loss: 0.00376. [  9 s]\n",
      "Epoch: 60 - 00060/00100 - Loss: 0.00410. [ 14 s]\n",
      "Epoch: 60 - 00080/00100 - Loss: 0.00359. [ 18 s]\n",
      "Epoch: 60 - 00100/00100 - Loss: 0.00258. [ 23 s]\n",
      "Epoch: 60 - loss(trn/val):0.00303/0.08055, acc(val):98.33%, lr=0.00010. [23s] @16 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.0420 \n",
      "iou=0.9735 \n",
      "acc=0.9878 \n",
      "sensitivity=0.9869 \n",
      "specificity=0.9886 \n",
      "precision=0.9856 \n",
      "f1=0.9861\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, additional_logger = logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea63364",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb752d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.041987 at epoch 12\n",
      "Minimum training loss: 0.003033 at epoch 60\n",
      "Maximum validation IoU: 0.974600 at epoch 22\n",
      "Maximum training IoU: 0.997140 at epoch 60\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_Enc_Dec_Tamp_RMS_lr_e-4\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795110d",
   "metadata": {},
   "source": [
    "# Training on \"Misc\" Dataset only\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab4ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_Enc_Dec_Misc_RMS_lr_e-4'\n",
    "directory = \"../data/training_states/ResNet34_Enc_Dec\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648a6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "res_model = ResNet(resnet34)\n",
    "model = FCN8s(res_model, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc913c",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21cd80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188 images in training dataset\n",
      "212 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.MISC_TRAINING\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=[], data_list_misc=['training'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=[], data_list_misc=['validation'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9f192",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b969c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "FCN8s(\n",
      "  (pretrained_net): ResNet(\n",
      "    (resnet): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (intermediate): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (deconv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Trainable params:   23513705\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training              2188    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.2 mean:0.0 std:1.0 / Labels:min:0.0 max:1.0 mean:0.3 std:0.5\n",
      "Validation             212    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.2 mean:-0.1 std:0.9 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00547 - Loss: 0.59318. [  5 s]\n",
      "Epoch: 1 - 00040/00547 - Loss: 0.56012. [  9 s]\n",
      "Epoch: 1 - 00060/00547 - Loss: 0.73172. [ 14 s]\n",
      "Epoch: 1 - 00080/00547 - Loss: 0.57641. [ 18 s]\n",
      "Epoch: 1 - 00100/00547 - Loss: 0.44252. [ 23 s]\n",
      "Epoch: 1 - 00120/00547 - Loss: 0.28269. [ 27 s]\n",
      "Epoch: 1 - 00140/00547 - Loss: 0.36113. [ 32 s]\n",
      "Epoch: 1 - 00160/00547 - Loss: 0.31585. [ 37 s]\n",
      "Epoch: 1 - 00180/00547 - Loss: 0.30306. [ 41 s]\n",
      "Epoch: 1 - 00200/00547 - Loss: 0.43162. [ 46 s]\n",
      "Epoch: 1 - 00220/00547 - Loss: 0.83646. [ 51 s]\n",
      "Epoch: 1 - 00240/00547 - Loss: 0.43501. [ 55 s]\n",
      "Epoch: 1 - 00260/00547 - Loss: 0.30508. [ 60 s]\n",
      "Epoch: 1 - 00280/00547 - Loss: 0.33716. [ 65 s]\n",
      "Epoch: 1 - 00300/00547 - Loss: 0.60586. [ 69 s]\n",
      "Epoch: 1 - 00320/00547 - Loss: 0.30397. [ 74 s]\n",
      "Epoch: 1 - 00340/00547 - Loss: 0.40389. [ 79 s]\n",
      "Epoch: 1 - 00360/00547 - Loss: 0.37333. [ 83 s]\n",
      "Epoch: 1 - 00380/00547 - Loss: 0.32749. [ 88 s]\n",
      "Epoch: 1 - 00400/00547 - Loss: 0.38388. [ 93 s]\n",
      "Epoch: 1 - 00420/00547 - Loss: 0.61052. [ 97 s]\n",
      "Epoch: 1 - 00440/00547 - Loss: 0.42312. [102 s]\n",
      "Epoch: 1 - 00460/00547 - Loss: 0.30629. [106 s]\n",
      "Epoch: 1 - 00480/00547 - Loss: 0.34283. [111 s]\n",
      "Epoch: 1 - 00500/00547 - Loss: 0.21478. [116 s]\n",
      "Epoch: 1 - 00520/00547 - Loss: 0.34628. [120 s]\n",
      "Epoch: 1 - 00540/00547 - Loss: 0.27879. [125 s]\n",
      "Epoch: 1 - loss(trn/val):0.30975/0.31192, acc(val):89.33%, lr=0.00010 [BEST]. [127s] @17 samples/s \n",
      "Epoch: 2 - 00020/00547 - Loss: 0.27451. [  4 s]\n",
      "Epoch: 2 - 00040/00547 - Loss: 0.65052. [  9 s]\n",
      "Epoch: 2 - 00060/00547 - Loss: 0.42099. [ 14 s]\n",
      "Epoch: 2 - 00080/00547 - Loss: 0.44007. [ 18 s]\n",
      "Epoch: 2 - 00100/00547 - Loss: 0.22550. [ 23 s]\n",
      "Epoch: 2 - 00120/00547 - Loss: 0.28505. [ 27 s]\n",
      "Epoch: 2 - 00140/00547 - Loss: 0.22888. [ 32 s]\n",
      "Epoch: 2 - 00160/00547 - Loss: 0.28935. [ 36 s]\n",
      "Epoch: 2 - 00180/00547 - Loss: 0.42043. [ 41 s]\n",
      "Epoch: 2 - 00200/00547 - Loss: 0.34762. [ 46 s]\n",
      "Epoch: 2 - 00220/00547 - Loss: 0.25756. [ 50 s]\n",
      "Epoch: 2 - 00240/00547 - Loss: 0.21518. [ 55 s]\n",
      "Epoch: 2 - 00260/00547 - Loss: 0.21586. [ 60 s]\n",
      "Epoch: 2 - 00280/00547 - Loss: 0.30339. [ 64 s]\n",
      "Epoch: 2 - 00300/00547 - Loss: 0.14979. [ 69 s]\n",
      "Epoch: 2 - 00320/00547 - Loss: 0.25068. [ 73 s]\n",
      "Epoch: 2 - 00340/00547 - Loss: 0.24982. [ 78 s]\n",
      "Epoch: 2 - 00360/00547 - Loss: 0.23263. [ 83 s]\n",
      "Epoch: 2 - 00380/00547 - Loss: 0.24117. [ 87 s]\n",
      "Epoch: 2 - 00400/00547 - Loss: 0.11804. [ 92 s]\n",
      "Epoch: 2 - 00420/00547 - Loss: 0.28777. [ 97 s]\n",
      "Epoch: 2 - 00440/00547 - Loss: 0.20446. [101 s]\n",
      "Epoch: 2 - 00460/00547 - Loss: 0.23342. [106 s]\n",
      "Epoch: 2 - 00480/00547 - Loss: 0.33720. [111 s]\n",
      "Epoch: 2 - 00500/00547 - Loss: 0.20542. [115 s]\n",
      "Epoch: 2 - 00520/00547 - Loss: 0.23548. [120 s]\n",
      "Epoch: 2 - 00540/00547 - Loss: 0.22210. [124 s]\n",
      "Epoch: 2 - loss(trn/val):0.23832/0.30798, acc(val):87.81%, lr=0.00010 [BEST]. [126s] @17 samples/s \n",
      "Epoch: 3 - 00020/00547 - Loss: 0.44494. [  4 s]\n",
      "Epoch: 3 - 00040/00547 - Loss: 0.12339. [  9 s]\n",
      "Epoch: 3 - 00060/00547 - Loss: 0.31955. [ 14 s]\n",
      "Epoch: 3 - 00080/00547 - Loss: 0.37074. [ 18 s]\n",
      "Epoch: 3 - 00100/00547 - Loss: 0.12495. [ 23 s]\n",
      "Epoch: 3 - 00120/00547 - Loss: 0.19438. [ 27 s]\n",
      "Epoch: 3 - 00140/00547 - Loss: 0.20799. [ 32 s]\n",
      "Epoch: 3 - 00160/00547 - Loss: 0.18405. [ 36 s]\n",
      "Epoch: 3 - 00180/00547 - Loss: 0.33103. [ 41 s]\n",
      "Epoch: 3 - 00200/00547 - Loss: 0.23293. [ 46 s]\n",
      "Epoch: 3 - 00220/00547 - Loss: 0.46483. [ 50 s]\n",
      "Epoch: 3 - 00240/00547 - Loss: 0.26818. [ 55 s]\n",
      "Epoch: 3 - 00260/00547 - Loss: 0.50227. [ 60 s]\n",
      "Epoch: 3 - 00280/00547 - Loss: 0.32422. [ 64 s]\n",
      "Epoch: 3 - 00300/00547 - Loss: 0.31848. [ 69 s]\n",
      "Epoch: 3 - 00320/00547 - Loss: 0.18181. [ 74 s]\n",
      "Epoch: 3 - 00340/00547 - Loss: 0.17859. [ 78 s]\n",
      "Epoch: 3 - 00360/00547 - Loss: 0.24159. [ 83 s]\n",
      "Epoch: 3 - 00380/00547 - Loss: 0.22732. [ 87 s]\n",
      "Epoch: 3 - 00400/00547 - Loss: 0.23646. [ 92 s]\n",
      "Epoch: 3 - 00420/00547 - Loss: 0.39075. [ 97 s]\n",
      "Epoch: 3 - 00440/00547 - Loss: 0.26062. [101 s]\n",
      "Epoch: 3 - 00460/00547 - Loss: 0.21399. [106 s]\n",
      "Epoch: 3 - 00480/00547 - Loss: 0.18165. [110 s]\n",
      "Epoch: 3 - 00500/00547 - Loss: 0.21175. [115 s]\n",
      "Epoch: 3 - 00520/00547 - Loss: 0.40407. [120 s]\n",
      "Epoch: 3 - 00540/00547 - Loss: 0.39184. [124 s]\n",
      "Epoch: 3 - loss(trn/val):0.17686/0.24898, acc(val):89.66%, lr=0.00010 [BEST]. [126s] @17 samples/s \n",
      "Epoch: 4 - 00020/00547 - Loss: 0.24138. [  4 s]\n",
      "Epoch: 4 - 00040/00547 - Loss: 0.22341. [  9 s]\n",
      "Epoch: 4 - 00060/00547 - Loss: 0.19727. [ 14 s]\n",
      "Epoch: 4 - 00080/00547 - Loss: 0.16699. [ 18 s]\n",
      "Epoch: 4 - 00100/00547 - Loss: 0.18028. [ 23 s]\n",
      "Epoch: 4 - 00120/00547 - Loss: 0.16593. [ 27 s]\n",
      "Epoch: 4 - 00140/00547 - Loss: 0.17325. [ 32 s]\n",
      "Epoch: 4 - 00160/00547 - Loss: 0.17000. [ 36 s]\n",
      "Epoch: 4 - 00180/00547 - Loss: 0.26702. [ 41 s]\n",
      "Epoch: 4 - 00200/00547 - Loss: 0.16310. [ 46 s]\n",
      "Epoch: 4 - 00220/00547 - Loss: 0.13062. [ 50 s]\n",
      "Epoch: 4 - 00240/00547 - Loss: 0.44235. [ 55 s]\n",
      "Epoch: 4 - 00260/00547 - Loss: 0.13453. [ 60 s]\n",
      "Epoch: 4 - 00280/00547 - Loss: 0.31512. [ 64 s]\n",
      "Epoch: 4 - 00300/00547 - Loss: 0.27394. [ 69 s]\n",
      "Epoch: 4 - 00320/00547 - Loss: 0.18589. [ 73 s]\n",
      "Epoch: 4 - 00340/00547 - Loss: 0.21641. [ 78 s]\n",
      "Epoch: 4 - 00360/00547 - Loss: 0.13534. [ 83 s]\n",
      "Epoch: 4 - 00380/00547 - Loss: 0.13094. [ 87 s]\n",
      "Epoch: 4 - 00400/00547 - Loss: 0.46878. [ 92 s]\n",
      "Epoch: 4 - 00420/00547 - Loss: 0.10625. [ 97 s]\n",
      "Epoch: 4 - 00440/00547 - Loss: 0.16727. [101 s]\n",
      "Epoch: 4 - 00460/00547 - Loss: 0.16459. [106 s]\n",
      "Epoch: 4 - 00480/00547 - Loss: 0.15453. [110 s]\n",
      "Epoch: 4 - 00500/00547 - Loss: 0.14478. [115 s]\n",
      "Epoch: 4 - 00520/00547 - Loss: 0.21272. [120 s]\n",
      "Epoch: 4 - 00540/00547 - Loss: 0.20266. [124 s]\n",
      "Epoch: 4 - loss(trn/val):0.15202/0.25054, acc(val):89.47%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 5 - 00020/00547 - Loss: 0.23502. [  4 s]\n",
      "Epoch: 5 - 00040/00547 - Loss: 0.07835. [  9 s]\n",
      "Epoch: 5 - 00060/00547 - Loss: 0.17728. [ 14 s]\n",
      "Epoch: 5 - 00080/00547 - Loss: 0.16929. [ 18 s]\n",
      "Epoch: 5 - 00100/00547 - Loss: 0.17475. [ 23 s]\n",
      "Epoch: 5 - 00120/00547 - Loss: 0.17081. [ 27 s]\n",
      "Epoch: 5 - 00140/00547 - Loss: 0.12358. [ 32 s]\n",
      "Epoch: 5 - 00160/00547 - Loss: 0.10805. [ 36 s]\n",
      "Epoch: 5 - 00180/00547 - Loss: 0.15095. [ 41 s]\n",
      "Epoch: 5 - 00200/00547 - Loss: 0.15269. [ 46 s]\n",
      "Epoch: 5 - 00220/00547 - Loss: 0.17924. [ 51 s]\n",
      "Epoch: 5 - 00240/00547 - Loss: 0.22264. [ 55 s]\n",
      "Epoch: 5 - 00260/00547 - Loss: 0.14377. [ 60 s]\n",
      "Epoch: 5 - 00280/00547 - Loss: 0.26280. [ 64 s]\n",
      "Epoch: 5 - 00300/00547 - Loss: 0.24503. [ 69 s]\n",
      "Epoch: 5 - 00320/00547 - Loss: 0.13181. [ 74 s]\n",
      "Epoch: 5 - 00340/00547 - Loss: 0.16388. [ 78 s]\n",
      "Epoch: 5 - 00360/00547 - Loss: 0.09016. [ 83 s]\n",
      "Epoch: 5 - 00380/00547 - Loss: 0.12178. [ 88 s]\n",
      "Epoch: 5 - 00400/00547 - Loss: 0.15265. [ 92 s]\n",
      "Epoch: 5 - 00420/00547 - Loss: 0.28670. [ 97 s]\n",
      "Epoch: 5 - 00440/00547 - Loss: 0.09395. [102 s]\n",
      "Epoch: 5 - 00460/00547 - Loss: 0.11320. [106 s]\n",
      "Epoch: 5 - 00480/00547 - Loss: 0.12333. [111 s]\n",
      "Epoch: 5 - 00500/00547 - Loss: 0.23905. [115 s]\n",
      "Epoch: 5 - 00520/00547 - Loss: 0.39025. [120 s]\n",
      "Epoch: 5 - 00540/00547 - Loss: 0.24843. [125 s]\n",
      "Epoch: 5 - loss(trn/val):0.13112/0.19618, acc(val):92.30%, lr=0.00010 [BEST]. [126s] @17 samples/s \n",
      "Epoch: 6 - 00020/00547 - Loss: 0.17088. [  5 s]\n",
      "Epoch: 6 - 00040/00547 - Loss: 0.22611. [  9 s]\n",
      "Epoch: 6 - 00060/00547 - Loss: 0.10567. [ 14 s]\n",
      "Epoch: 6 - 00080/00547 - Loss: 1.30731. [ 18 s]\n",
      "Epoch: 6 - 00100/00547 - Loss: 0.13365. [ 23 s]\n",
      "Epoch: 6 - 00120/00547 - Loss: 0.17670. [ 27 s]\n",
      "Epoch: 6 - 00140/00547 - Loss: 0.12891. [ 32 s]\n",
      "Epoch: 6 - 00160/00547 - Loss: 0.10178. [ 37 s]\n",
      "Epoch: 6 - 00180/00547 - Loss: 0.09812. [ 41 s]\n",
      "Epoch: 6 - 00200/00547 - Loss: 0.15571. [ 46 s]\n",
      "Epoch: 6 - 00220/00547 - Loss: 0.08186. [ 51 s]\n",
      "Epoch: 6 - 00240/00547 - Loss: 0.12208. [ 55 s]\n",
      "Epoch: 6 - 00260/00547 - Loss: 0.15022. [ 60 s]\n",
      "Epoch: 6 - 00280/00547 - Loss: 0.25959. [ 64 s]\n",
      "Epoch: 6 - 00300/00547 - Loss: 0.15228. [ 69 s]\n",
      "Epoch: 6 - 00320/00547 - Loss: 0.12019. [ 74 s]\n",
      "Epoch: 6 - 00340/00547 - Loss: 0.12012. [ 78 s]\n",
      "Epoch: 6 - 00360/00547 - Loss: 0.09097. [ 83 s]\n",
      "Epoch: 6 - 00380/00547 - Loss: 0.19155. [ 88 s]\n",
      "Epoch: 6 - 00400/00547 - Loss: 0.07903. [ 92 s]\n",
      "Epoch: 6 - 00420/00547 - Loss: 0.21073. [ 97 s]\n",
      "Epoch: 6 - 00440/00547 - Loss: 0.08346. [102 s]\n",
      "Epoch: 6 - 00460/00547 - Loss: 0.47300. [106 s]\n",
      "Epoch: 6 - 00480/00547 - Loss: 0.11499. [111 s]\n",
      "Epoch: 6 - 00500/00547 - Loss: 0.23103. [115 s]\n",
      "Epoch: 6 - 00520/00547 - Loss: 0.14141. [120 s]\n",
      "Epoch: 6 - 00540/00547 - Loss: 0.13051. [125 s]\n",
      "Epoch: 6 - loss(trn/val):0.11588/0.21180, acc(val):91.14%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 7 - 00020/00547 - Loss: 0.10989. [  4 s]\n",
      "Epoch: 7 - 00040/00547 - Loss: 0.15028. [  9 s]\n",
      "Epoch: 7 - 00060/00547 - Loss: 0.11381. [ 14 s]\n",
      "Epoch: 7 - 00080/00547 - Loss: 0.14526. [ 18 s]\n",
      "Epoch: 7 - 00100/00547 - Loss: 0.11444. [ 23 s]\n",
      "Epoch: 7 - 00120/00547 - Loss: 0.15155. [ 27 s]\n",
      "Epoch: 7 - 00140/00547 - Loss: 0.19369. [ 32 s]\n",
      "Epoch: 7 - 00160/00547 - Loss: 0.11411. [ 37 s]\n",
      "Epoch: 7 - 00180/00547 - Loss: 0.24637. [ 41 s]\n",
      "Epoch: 7 - 00200/00547 - Loss: 0.07029. [ 46 s]\n",
      "Epoch: 7 - 00220/00547 - Loss: 0.09178. [ 51 s]\n",
      "Epoch: 7 - 00240/00547 - Loss: 0.16530. [ 55 s]\n",
      "Epoch: 7 - 00260/00547 - Loss: 0.09162. [ 60 s]\n",
      "Epoch: 7 - 00280/00547 - Loss: 0.10274. [ 65 s]\n",
      "Epoch: 7 - 00300/00547 - Loss: 0.09547. [ 69 s]\n",
      "Epoch: 7 - 00320/00547 - Loss: 0.15591. [ 74 s]\n",
      "Epoch: 7 - 00340/00547 - Loss: 0.10260. [ 79 s]\n",
      "Epoch: 7 - 00360/00547 - Loss: 0.16520. [ 83 s]\n",
      "Epoch: 7 - 00380/00547 - Loss: 0.16803. [ 88 s]\n",
      "Epoch: 7 - 00400/00547 - Loss: 0.12876. [ 93 s]\n",
      "Epoch: 7 - 00420/00547 - Loss: 0.06759. [ 97 s]\n",
      "Epoch: 7 - 00440/00547 - Loss: 0.13316. [102 s]\n",
      "Epoch: 7 - 00460/00547 - Loss: 0.20184. [107 s]\n",
      "Epoch: 7 - 00480/00547 - Loss: 0.19210. [111 s]\n",
      "Epoch: 7 - 00500/00547 - Loss: 0.11718. [116 s]\n",
      "Epoch: 7 - 00520/00547 - Loss: 0.12065. [121 s]\n",
      "Epoch: 7 - 00540/00547 - Loss: 0.10745. [125 s]\n",
      "Epoch: 7 - loss(trn/val):0.10778/0.24315, acc(val):90.35%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 8 - 00020/00547 - Loss: 0.19238. [  5 s]\n",
      "Epoch: 8 - 00040/00547 - Loss: 0.15529. [  9 s]\n",
      "Epoch: 8 - 00060/00547 - Loss: 0.07776. [ 14 s]\n",
      "Epoch: 8 - 00080/00547 - Loss: 0.08570. [ 18 s]\n",
      "Epoch: 8 - 00100/00547 - Loss: 0.15243. [ 23 s]\n",
      "Epoch: 8 - 00120/00547 - Loss: 0.09816. [ 28 s]\n",
      "Epoch: 8 - 00140/00547 - Loss: 0.19094. [ 32 s]\n",
      "Epoch: 8 - 00160/00547 - Loss: 0.08446. [ 37 s]\n",
      "Epoch: 8 - 00180/00547 - Loss: 0.08725. [ 42 s]\n",
      "Epoch: 8 - 00200/00547 - Loss: 0.14422. [ 46 s]\n",
      "Epoch: 8 - 00220/00547 - Loss: 0.16591. [ 51 s]\n",
      "Epoch: 8 - 00240/00547 - Loss: 0.11552. [ 55 s]\n",
      "Epoch: 8 - 00260/00547 - Loss: 0.33912. [ 60 s]\n",
      "Epoch: 8 - 00280/00547 - Loss: 0.12487. [ 65 s]\n",
      "Epoch: 8 - 00300/00547 - Loss: 0.05373. [ 69 s]\n",
      "Epoch: 8 - 00320/00547 - Loss: 0.09190. [ 74 s]\n",
      "Epoch: 8 - 00340/00547 - Loss: 0.09634. [ 79 s]\n",
      "Epoch: 8 - 00360/00547 - Loss: 0.08907. [ 83 s]\n",
      "Epoch: 8 - 00380/00547 - Loss: 0.08746. [ 88 s]\n",
      "Epoch: 8 - 00400/00547 - Loss: 0.13088. [ 93 s]\n",
      "Epoch: 8 - 00420/00547 - Loss: 0.08667. [ 98 s]\n",
      "Epoch: 8 - 00440/00547 - Loss: 0.10006. [102 s]\n",
      "Epoch: 8 - 00460/00547 - Loss: 0.26471. [107 s]\n",
      "Epoch: 8 - 00480/00547 - Loss: 0.10315. [112 s]\n",
      "Epoch: 8 - 00500/00547 - Loss: 0.12446. [116 s]\n",
      "Epoch: 8 - 00520/00547 - Loss: 0.09297. [121 s]\n",
      "Epoch: 8 - 00540/00547 - Loss: 0.12282. [126 s]\n",
      "Epoch: 8 - loss(trn/val):0.08707/0.21581, acc(val):91.65%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 9 - 00020/00547 - Loss: 0.05267. [  5 s]\n",
      "Epoch: 9 - 00040/00547 - Loss: 0.06120. [  9 s]\n",
      "Epoch: 9 - 00060/00547 - Loss: 0.22840. [ 14 s]\n",
      "Epoch: 9 - 00080/00547 - Loss: 0.26871. [ 18 s]\n",
      "Epoch: 9 - 00100/00547 - Loss: 0.12914. [ 23 s]\n",
      "Epoch: 9 - 00120/00547 - Loss: 0.07738. [ 28 s]\n",
      "Epoch: 9 - 00140/00547 - Loss: 0.11357. [ 32 s]\n",
      "Epoch: 9 - 00160/00547 - Loss: 0.09447. [ 37 s]\n",
      "Epoch: 9 - 00180/00547 - Loss: 0.10100. [ 41 s]\n",
      "Epoch: 9 - 00200/00547 - Loss: 0.09395. [ 46 s]\n",
      "Epoch: 9 - 00220/00547 - Loss: 0.06822. [ 51 s]\n",
      "Epoch: 9 - 00240/00547 - Loss: 0.05694. [ 55 s]\n",
      "Epoch: 9 - 00260/00547 - Loss: 0.16344. [ 60 s]\n",
      "Epoch: 9 - 00280/00547 - Loss: 0.05568. [ 65 s]\n",
      "Epoch: 9 - 00300/00547 - Loss: 0.14498. [ 69 s]\n",
      "Epoch: 9 - 00320/00547 - Loss: 0.18668. [ 74 s]\n",
      "Epoch: 9 - 00340/00547 - Loss: 0.08020. [ 79 s]\n",
      "Epoch: 9 - 00360/00547 - Loss: 0.17365. [ 83 s]\n",
      "Epoch: 9 - 00380/00547 - Loss: 0.13533. [ 88 s]\n",
      "Epoch: 9 - 00400/00547 - Loss: 0.13095. [ 93 s]\n",
      "Epoch: 9 - 00420/00547 - Loss: 0.16289. [ 97 s]\n",
      "Epoch: 9 - 00440/00547 - Loss: 0.08909. [102 s]\n",
      "Epoch: 9 - 00460/00547 - Loss: 0.13181. [107 s]\n",
      "Epoch: 9 - 00480/00547 - Loss: 0.09549. [111 s]\n",
      "Epoch: 9 - 00500/00547 - Loss: 0.18573. [116 s]\n",
      "Epoch: 9 - 00520/00547 - Loss: 0.08292. [121 s]\n",
      "Epoch: 9 - 00540/00547 - Loss: 0.06894. [125 s]\n",
      "Epoch: 9 - loss(trn/val):0.08303/0.19736, acc(val):92.31%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 10 - 00020/00547 - Loss: 0.13864. [  4 s]\n",
      "Epoch: 10 - 00040/00547 - Loss: 0.05270. [  9 s]\n",
      "Epoch: 10 - 00060/00547 - Loss: 0.08765. [ 14 s]\n",
      "Epoch: 10 - 00080/00547 - Loss: 0.20126. [ 18 s]\n",
      "Epoch: 10 - 00100/00547 - Loss: 0.04828. [ 23 s]\n",
      "Epoch: 10 - 00120/00547 - Loss: 0.05533. [ 27 s]\n",
      "Epoch: 10 - 00140/00547 - Loss: 0.07211. [ 32 s]\n",
      "Epoch: 10 - 00160/00547 - Loss: 0.15532. [ 37 s]\n",
      "Epoch: 10 - 00180/00547 - Loss: 0.10364. [ 41 s]\n",
      "Epoch: 10 - 00200/00547 - Loss: 0.18113. [ 46 s]\n",
      "Epoch: 10 - 00220/00547 - Loss: 0.04674. [ 51 s]\n",
      "Epoch: 10 - 00240/00547 - Loss: 0.06781. [ 55 s]\n",
      "Epoch: 10 - 00260/00547 - Loss: 0.05275. [ 60 s]\n",
      "Epoch: 10 - 00280/00547 - Loss: 0.06576. [ 64 s]\n",
      "Epoch: 10 - 00300/00547 - Loss: 0.14418. [ 69 s]\n",
      "Epoch: 10 - 00320/00547 - Loss: 0.09812. [ 74 s]\n",
      "Epoch: 10 - 00340/00547 - Loss: 0.06049. [ 78 s]\n",
      "Epoch: 10 - 00360/00547 - Loss: 0.12392. [ 83 s]\n",
      "Epoch: 10 - 00380/00547 - Loss: 0.07244. [ 88 s]\n",
      "Epoch: 10 - 00400/00547 - Loss: 0.07747. [ 92 s]\n",
      "Epoch: 10 - 00420/00547 - Loss: 0.16055. [ 97 s]\n",
      "Epoch: 10 - 00440/00547 - Loss: 0.08091. [102 s]\n",
      "Epoch: 10 - 00460/00547 - Loss: 0.07266. [106 s]\n",
      "Epoch: 10 - 00480/00547 - Loss: 0.15221. [111 s]\n",
      "Epoch: 10 - 00500/00547 - Loss: 0.06152. [116 s]\n",
      "Epoch: 10 - 00520/00547 - Loss: 0.06080. [120 s]\n",
      "Epoch: 10 - 00540/00547 - Loss: 0.06270. [125 s]\n",
      "Epoch: 10 - loss(trn/val):0.08167/0.20481, acc(val):91.82%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 11 - 00020/00547 - Loss: 0.06147. [  5 s]\n",
      "Epoch: 11 - 00040/00547 - Loss: 0.08303. [  9 s]\n",
      "Epoch: 11 - 00060/00547 - Loss: 0.06558. [ 14 s]\n",
      "Epoch: 11 - 00080/00547 - Loss: 0.10923. [ 18 s]\n",
      "Epoch: 11 - 00100/00547 - Loss: 0.06657. [ 23 s]\n",
      "Epoch: 11 - 00120/00547 - Loss: 0.03262. [ 27 s]\n",
      "Epoch: 11 - 00140/00547 - Loss: 0.12886. [ 32 s]\n",
      "Epoch: 11 - 00160/00547 - Loss: 0.04790. [ 37 s]\n",
      "Epoch: 11 - 00180/00547 - Loss: 0.08020. [ 41 s]\n",
      "Epoch: 11 - 00200/00547 - Loss: 0.13375. [ 46 s]\n",
      "Epoch: 11 - 00220/00547 - Loss: 0.08156. [ 51 s]\n",
      "Epoch: 11 - 00240/00547 - Loss: 0.19658. [ 55 s]\n",
      "Epoch: 11 - 00260/00547 - Loss: 0.14104. [ 60 s]\n",
      "Epoch: 11 - 00280/00547 - Loss: 0.09718. [ 65 s]\n",
      "Epoch: 11 - 00300/00547 - Loss: 0.05373. [ 69 s]\n",
      "Epoch: 11 - 00320/00547 - Loss: 0.09731. [ 74 s]\n",
      "Epoch: 11 - 00340/00547 - Loss: 0.08117. [ 78 s]\n",
      "Epoch: 11 - 00360/00547 - Loss: 0.09389. [ 83 s]\n",
      "Epoch: 11 - 00380/00547 - Loss: 0.03836. [ 88 s]\n",
      "Epoch: 11 - 00400/00547 - Loss: 0.06982. [ 92 s]\n",
      "Epoch: 11 - 00420/00547 - Loss: 0.22583. [ 97 s]\n",
      "Epoch: 11 - 00440/00547 - Loss: 0.06646. [102 s]\n",
      "Epoch: 11 - 00460/00547 - Loss: 0.09326. [106 s]\n",
      "Epoch: 11 - 00480/00547 - Loss: 0.03808. [111 s]\n",
      "Epoch: 11 - 00500/00547 - Loss: 0.07248. [116 s]\n",
      "Epoch: 11 - 00520/00547 - Loss: 0.06038. [120 s]\n",
      "Epoch: 11 - 00540/00547 - Loss: 0.13027. [125 s]\n",
      "Epoch: 11 - loss(trn/val):0.07159/0.19784, acc(val):92.60%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 12 - 00020/00547 - Loss: 0.09735. [  4 s]\n",
      "Epoch: 12 - 00040/00547 - Loss: 0.19721. [  9 s]\n",
      "Epoch: 12 - 00060/00547 - Loss: 0.12149. [ 14 s]\n",
      "Epoch: 12 - 00080/00547 - Loss: 0.07850. [ 18 s]\n",
      "Epoch: 12 - 00100/00547 - Loss: 0.05721. [ 23 s]\n",
      "Epoch: 12 - 00120/00547 - Loss: 0.05663. [ 27 s]\n",
      "Epoch: 12 - 00140/00547 - Loss: 0.11589. [ 32 s]\n",
      "Epoch: 12 - 00160/00547 - Loss: 0.06855. [ 37 s]\n",
      "Epoch: 12 - 00180/00547 - Loss: 0.04997. [ 41 s]\n",
      "Epoch: 12 - 00200/00547 - Loss: 0.05565. [ 46 s]\n",
      "Epoch: 12 - 00220/00547 - Loss: 0.09828. [ 50 s]\n",
      "Epoch: 12 - 00240/00547 - Loss: 0.03272. [ 55 s]\n",
      "Epoch: 12 - 00260/00547 - Loss: 0.05216. [ 60 s]\n",
      "Epoch: 12 - 00280/00547 - Loss: 0.16326. [ 64 s]\n",
      "Epoch: 12 - 00300/00547 - Loss: 0.11262. [ 69 s]\n",
      "Epoch: 12 - 00320/00547 - Loss: 0.08317. [ 74 s]\n",
      "Epoch: 12 - 00340/00547 - Loss: 0.18071. [ 78 s]\n",
      "Epoch: 12 - 00360/00547 - Loss: 0.13838. [ 83 s]\n",
      "Epoch: 12 - 00380/00547 - Loss: 0.07583. [ 88 s]\n",
      "Epoch: 12 - 00400/00547 - Loss: 0.03976. [ 92 s]\n",
      "Epoch: 12 - 00420/00547 - Loss: 0.07169. [ 97 s]\n",
      "Epoch: 12 - 00440/00547 - Loss: 0.41634. [101 s]\n",
      "Epoch: 12 - 00460/00547 - Loss: 0.10226. [106 s]\n",
      "Epoch: 12 - 00480/00547 - Loss: 0.07046. [111 s]\n",
      "Epoch: 12 - 00500/00547 - Loss: 0.09571. [115 s]\n",
      "Epoch: 12 - 00520/00547 - Loss: 0.11285. [120 s]\n",
      "Epoch: 12 - 00540/00547 - Loss: 0.06429. [125 s]\n",
      "Epoch: 12 - loss(trn/val):0.06535/0.21686, acc(val):91.84%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 13 - 00020/00547 - Loss: 0.03857. [  4 s]\n",
      "Epoch: 13 - 00040/00547 - Loss: 0.05031. [  9 s]\n",
      "Epoch: 13 - 00060/00547 - Loss: 0.05175. [ 14 s]\n",
      "Epoch: 13 - 00080/00547 - Loss: 0.10060. [ 18 s]\n",
      "Epoch: 13 - 00100/00547 - Loss: 0.08787. [ 23 s]\n",
      "Epoch: 13 - 00120/00547 - Loss: 0.05323. [ 27 s]\n",
      "Epoch: 13 - 00140/00547 - Loss: 0.10490. [ 32 s]\n",
      "Epoch: 13 - 00160/00547 - Loss: 0.04906. [ 37 s]\n",
      "Epoch: 13 - 00180/00547 - Loss: 0.04298. [ 41 s]\n",
      "Epoch: 13 - 00200/00547 - Loss: 0.12423. [ 46 s]\n",
      "Epoch: 13 - 00220/00547 - Loss: 0.30463. [ 51 s]\n",
      "Epoch: 13 - 00240/00547 - Loss: 0.15519. [ 55 s]\n",
      "Epoch: 13 - 00260/00547 - Loss: 0.07893. [ 60 s]\n",
      "Epoch: 13 - 00280/00547 - Loss: 0.09799. [ 64 s]\n",
      "Epoch: 13 - 00300/00547 - Loss: 0.09673. [ 69 s]\n",
      "Epoch: 13 - 00320/00547 - Loss: 0.11445. [ 74 s]\n",
      "Epoch: 13 - 00340/00547 - Loss: 0.09158. [ 78 s]\n",
      "Epoch: 13 - 00360/00547 - Loss: 0.06165. [ 83 s]\n",
      "Epoch: 13 - 00380/00547 - Loss: 0.13660. [ 88 s]\n",
      "Epoch: 13 - 00400/00547 - Loss: 0.06810. [ 92 s]\n",
      "Epoch: 13 - 00420/00547 - Loss: 0.04044. [ 97 s]\n",
      "Epoch: 13 - 00440/00547 - Loss: 0.04879. [102 s]\n",
      "Epoch: 13 - 00460/00547 - Loss: 0.08725. [106 s]\n",
      "Epoch: 13 - 00480/00547 - Loss: 0.05813. [111 s]\n",
      "Epoch: 13 - 00500/00547 - Loss: 0.08026. [115 s]\n",
      "Epoch: 13 - 00520/00547 - Loss: 0.14996. [120 s]\n",
      "Epoch: 13 - 00540/00547 - Loss: 0.06497. [125 s]\n",
      "Epoch: 13 - loss(trn/val):0.07273/0.20346, acc(val):92.34%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 14 - 00020/00547 - Loss: 0.08608. [  5 s]\n",
      "Epoch: 14 - 00040/00547 - Loss: 0.12687. [  9 s]\n",
      "Epoch: 14 - 00060/00547 - Loss: 0.09344. [ 14 s]\n",
      "Epoch: 14 - 00080/00547 - Loss: 0.05835. [ 18 s]\n",
      "Epoch: 14 - 00100/00547 - Loss: 0.10033. [ 23 s]\n",
      "Epoch: 14 - 00120/00547 - Loss: 0.09659. [ 27 s]\n",
      "Epoch: 14 - 00140/00547 - Loss: 0.06586. [ 32 s]\n",
      "Epoch: 14 - 00160/00547 - Loss: 0.06001. [ 37 s]\n",
      "Epoch: 14 - 00180/00547 - Loss: 0.04870. [ 41 s]\n",
      "Epoch: 14 - 00200/00547 - Loss: 0.06039. [ 46 s]\n",
      "Epoch: 14 - 00220/00547 - Loss: 0.14989. [ 50 s]\n",
      "Epoch: 14 - 00240/00547 - Loss: 0.11769. [ 55 s]\n",
      "Epoch: 14 - 00260/00547 - Loss: 0.05832. [ 60 s]\n",
      "Epoch: 14 - 00280/00547 - Loss: 0.08912. [ 64 s]\n",
      "Epoch: 14 - 00300/00547 - Loss: 0.11982. [ 69 s]\n",
      "Epoch: 14 - 00320/00547 - Loss: 0.07110. [ 74 s]\n",
      "Epoch: 14 - 00340/00547 - Loss: 0.10982. [ 78 s]\n",
      "Epoch: 14 - 00360/00547 - Loss: 0.11470. [ 83 s]\n",
      "Epoch: 14 - 00380/00547 - Loss: 0.04854. [ 88 s]\n",
      "Epoch: 14 - 00400/00547 - Loss: 0.17482. [ 92 s]\n",
      "Epoch: 14 - 00420/00547 - Loss: 0.09571. [ 97 s]\n",
      "Epoch: 14 - 00440/00547 - Loss: 0.10215. [102 s]\n",
      "Epoch: 14 - 00460/00547 - Loss: 0.05719. [106 s]\n",
      "Epoch: 14 - 00480/00547 - Loss: 0.07645. [111 s]\n",
      "Epoch: 14 - 00500/00547 - Loss: 0.11279. [116 s]\n",
      "Epoch: 14 - 00520/00547 - Loss: 0.14113. [120 s]\n",
      "Epoch: 14 - 00540/00547 - Loss: 0.07268. [125 s]\n",
      "Epoch: 14 - loss(trn/val):0.06192/0.21832, acc(val):92.28%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 15 - 00020/00547 - Loss: 0.02280. [  5 s]\n",
      "Epoch: 15 - 00040/00547 - Loss: 0.07152. [  9 s]\n",
      "Epoch: 15 - 00060/00547 - Loss: 0.08809. [ 14 s]\n",
      "Epoch: 15 - 00080/00547 - Loss: 0.03253. [ 18 s]\n",
      "Epoch: 15 - 00100/00547 - Loss: 0.08318. [ 23 s]\n",
      "Epoch: 15 - 00120/00547 - Loss: 0.06859. [ 27 s]\n",
      "Epoch: 15 - 00140/00547 - Loss: 0.13312. [ 32 s]\n",
      "Epoch: 15 - 00160/00547 - Loss: 0.08033. [ 37 s]\n",
      "Epoch: 15 - 00180/00547 - Loss: 0.06817. [ 41 s]\n",
      "Epoch: 15 - 00200/00547 - Loss: 0.10108. [ 46 s]\n",
      "Epoch: 15 - 00220/00547 - Loss: 0.09713. [ 51 s]\n",
      "Epoch: 15 - 00240/00547 - Loss: 0.09476. [ 55 s]\n",
      "Epoch: 15 - 00260/00547 - Loss: 0.05343. [ 60 s]\n",
      "Epoch: 15 - 00280/00547 - Loss: 0.08258. [ 65 s]\n",
      "Epoch: 15 - 00300/00547 - Loss: 0.03343. [ 69 s]\n",
      "Epoch: 15 - 00320/00547 - Loss: 0.06366. [ 74 s]\n",
      "Epoch: 15 - 00340/00547 - Loss: 0.06061. [ 78 s]\n",
      "Epoch: 15 - 00360/00547 - Loss: 0.06000. [ 83 s]\n",
      "Epoch: 15 - 00380/00547 - Loss: 0.05838. [ 88 s]\n",
      "Epoch: 15 - 00400/00547 - Loss: 0.12236. [ 92 s]\n",
      "Epoch: 15 - 00420/00547 - Loss: 0.06059. [ 97 s]\n",
      "Epoch: 15 - 00440/00547 - Loss: 0.04694. [102 s]\n",
      "Epoch: 15 - 00460/00547 - Loss: 0.07090. [106 s]\n",
      "Epoch: 15 - 00480/00547 - Loss: 0.07679. [111 s]\n",
      "Epoch: 15 - 00500/00547 - Loss: 0.09615. [116 s]\n",
      "Epoch: 15 - 00520/00547 - Loss: 0.06587. [120 s]\n",
      "Epoch: 15 - 00540/00547 - Loss: 0.04045. [125 s]\n",
      "Epoch: 15 - loss(trn/val):0.05652/0.23308, acc(val):91.85%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 16 - 00020/00547 - Loss: 0.04210. [  5 s]\n",
      "Epoch: 16 - 00040/00547 - Loss: 0.10096. [  9 s]\n",
      "Epoch: 16 - 00060/00547 - Loss: 0.09636. [ 14 s]\n",
      "Epoch: 16 - 00080/00547 - Loss: 0.06235. [ 18 s]\n",
      "Epoch: 16 - 00100/00547 - Loss: 0.08012. [ 23 s]\n",
      "Epoch: 16 - 00120/00547 - Loss: 0.04504. [ 27 s]\n",
      "Epoch: 16 - 00140/00547 - Loss: 0.17339. [ 32 s]\n",
      "Epoch: 16 - 00160/00547 - Loss: 0.08860. [ 37 s]\n",
      "Epoch: 16 - 00180/00547 - Loss: 0.04306. [ 41 s]\n",
      "Epoch: 16 - 00200/00547 - Loss: 0.05249. [ 46 s]\n",
      "Epoch: 16 - 00220/00547 - Loss: 0.06773. [ 51 s]\n",
      "Epoch: 16 - 00240/00547 - Loss: 0.06625. [ 55 s]\n",
      "Epoch: 16 - 00260/00547 - Loss: 0.04772. [ 60 s]\n",
      "Epoch: 16 - 00280/00547 - Loss: 0.06295. [ 64 s]\n",
      "Epoch: 16 - 00300/00547 - Loss: 0.03702. [ 69 s]\n",
      "Epoch: 16 - 00320/00547 - Loss: 0.06218. [ 74 s]\n",
      "Epoch: 16 - 00340/00547 - Loss: 0.03194. [ 78 s]\n",
      "Epoch: 16 - 00360/00547 - Loss: 0.03171. [ 83 s]\n",
      "Epoch: 16 - 00380/00547 - Loss: 0.04371. [ 88 s]\n",
      "Epoch: 16 - 00400/00547 - Loss: 0.08212. [ 92 s]\n",
      "Epoch: 16 - 00420/00547 - Loss: 0.10919. [ 97 s]\n",
      "Epoch: 16 - 00440/00547 - Loss: 0.07060. [102 s]\n",
      "Epoch: 16 - 00460/00547 - Loss: 0.05415. [106 s]\n",
      "Epoch: 16 - 00480/00547 - Loss: 0.11988. [111 s]\n",
      "Epoch: 16 - 00500/00547 - Loss: 0.05035. [116 s]\n",
      "Epoch: 16 - 00520/00547 - Loss: 0.05570. [120 s]\n",
      "Epoch: 16 - 00540/00547 - Loss: 0.05759. [125 s]\n",
      "Epoch: 16 - loss(trn/val):0.05544/0.23127, acc(val):92.23%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 17 - 00020/00547 - Loss: 0.07066. [  4 s]\n",
      "Epoch: 17 - 00040/00547 - Loss: 0.04571. [  9 s]\n",
      "Epoch: 17 - 00060/00547 - Loss: 0.05351. [ 14 s]\n",
      "Epoch: 17 - 00080/00547 - Loss: 0.05468. [ 18 s]\n",
      "Epoch: 17 - 00100/00547 - Loss: 0.07715. [ 23 s]\n",
      "Epoch: 17 - 00120/00547 - Loss: 0.03083. [ 27 s]\n",
      "Epoch: 17 - 00140/00547 - Loss: 0.03981. [ 32 s]\n",
      "Epoch: 17 - 00160/00547 - Loss: 0.07126. [ 37 s]\n",
      "Epoch: 17 - 00180/00547 - Loss: 0.07080. [ 41 s]\n",
      "Epoch: 17 - 00200/00547 - Loss: 0.05269. [ 46 s]\n",
      "Epoch: 17 - 00220/00547 - Loss: 0.05884. [ 51 s]\n",
      "Epoch: 17 - 00240/00547 - Loss: 0.04797. [ 55 s]\n",
      "Epoch: 17 - 00260/00547 - Loss: 0.04729. [ 60 s]\n",
      "Epoch: 17 - 00280/00547 - Loss: 0.04759. [ 65 s]\n",
      "Epoch: 17 - 00300/00547 - Loss: 0.08295. [ 69 s]\n",
      "Epoch: 17 - 00320/00547 - Loss: 0.07931. [ 74 s]\n",
      "Epoch: 17 - 00340/00547 - Loss: 0.05259. [ 78 s]\n",
      "Epoch: 17 - 00360/00547 - Loss: 0.03182. [ 83 s]\n",
      "Epoch: 17 - 00380/00547 - Loss: 0.08120. [ 88 s]\n",
      "Epoch: 17 - 00400/00547 - Loss: 0.08883. [ 92 s]\n",
      "Epoch: 17 - 00420/00547 - Loss: 0.08843. [ 97 s]\n",
      "Epoch: 17 - 00440/00547 - Loss: 0.04783. [102 s]\n",
      "Epoch: 17 - 00460/00547 - Loss: 0.04292. [106 s]\n",
      "Epoch: 17 - 00480/00547 - Loss: 0.09145. [111 s]\n",
      "Epoch: 17 - 00500/00547 - Loss: 0.04411. [116 s]\n",
      "Epoch: 17 - 00520/00547 - Loss: 0.07948. [120 s]\n",
      "Epoch: 17 - 00540/00547 - Loss: 0.06679. [125 s]\n",
      "Epoch: 17 - loss(trn/val):0.05233/0.18249, acc(val):93.10%, lr=0.00010 [BEST]. [126s] @17 samples/s \n",
      "Epoch: 18 - 00020/00547 - Loss: 0.17999. [  4 s]\n",
      "Epoch: 18 - 00040/00547 - Loss: 0.04763. [  9 s]\n",
      "Epoch: 18 - 00060/00547 - Loss: 0.04442. [ 14 s]\n",
      "Epoch: 18 - 00080/00547 - Loss: 0.05190. [ 18 s]\n",
      "Epoch: 18 - 00100/00547 - Loss: 0.15875. [ 23 s]\n",
      "Epoch: 18 - 00120/00547 - Loss: 0.03588. [ 27 s]\n",
      "Epoch: 18 - 00140/00547 - Loss: 0.21447. [ 32 s]\n",
      "Epoch: 18 - 00160/00547 - Loss: 0.04757. [ 36 s]\n",
      "Epoch: 18 - 00180/00547 - Loss: 0.05135. [ 41 s]\n",
      "Epoch: 18 - 00200/00547 - Loss: 0.04729. [ 46 s]\n",
      "Epoch: 18 - 00220/00547 - Loss: 0.05363. [ 50 s]\n",
      "Epoch: 18 - 00240/00547 - Loss: 0.04510. [ 55 s]\n",
      "Epoch: 18 - 00260/00547 - Loss: 0.06379. [ 60 s]\n",
      "Epoch: 18 - 00280/00547 - Loss: 0.05364. [ 64 s]\n",
      "Epoch: 18 - 00300/00547 - Loss: 0.03407. [ 69 s]\n",
      "Epoch: 18 - 00320/00547 - Loss: 0.04202. [ 73 s]\n",
      "Epoch: 18 - 00340/00547 - Loss: 0.03353. [ 78 s]\n",
      "Epoch: 18 - 00360/00547 - Loss: 0.03547. [ 83 s]\n",
      "Epoch: 18 - 00380/00547 - Loss: 0.03581. [ 87 s]\n",
      "Epoch: 18 - 00400/00547 - Loss: 0.04351. [ 92 s]\n",
      "Epoch: 18 - 00420/00547 - Loss: 0.05700. [ 97 s]\n",
      "Epoch: 18 - 00440/00547 - Loss: 0.09889. [101 s]\n",
      "Epoch: 18 - 00460/00547 - Loss: 0.05429. [106 s]\n",
      "Epoch: 18 - 00480/00547 - Loss: 0.14165. [111 s]\n",
      "Epoch: 18 - 00500/00547 - Loss: 0.09229. [115 s]\n",
      "Epoch: 18 - 00520/00547 - Loss: 0.02808. [120 s]\n",
      "Epoch: 18 - 00540/00547 - Loss: 0.04516. [124 s]\n",
      "Epoch: 18 - loss(trn/val):0.04943/0.28133, acc(val):90.18%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 19 - 00020/00547 - Loss: 0.02778. [  4 s]\n",
      "Epoch: 19 - 00040/00547 - Loss: 0.06755. [  9 s]\n",
      "Epoch: 19 - 00060/00547 - Loss: 0.02126. [ 14 s]\n",
      "Epoch: 19 - 00080/00547 - Loss: 0.03260. [ 18 s]\n",
      "Epoch: 19 - 00100/00547 - Loss: 0.04038. [ 23 s]\n",
      "Epoch: 19 - 00120/00547 - Loss: 0.04722. [ 27 s]\n",
      "Epoch: 19 - 00140/00547 - Loss: 0.08672. [ 32 s]\n",
      "Epoch: 19 - 00160/00547 - Loss: 0.06689. [ 37 s]\n",
      "Epoch: 19 - 00180/00547 - Loss: 0.06149. [ 41 s]\n",
      "Epoch: 19 - 00200/00547 - Loss: 0.04647. [ 46 s]\n",
      "Epoch: 19 - 00220/00547 - Loss: 0.04946. [ 51 s]\n",
      "Epoch: 19 - 00240/00547 - Loss: 0.12281. [ 55 s]\n",
      "Epoch: 19 - 00260/00547 - Loss: 0.04146. [ 60 s]\n",
      "Epoch: 19 - 00280/00547 - Loss: 0.05600. [ 64 s]\n",
      "Epoch: 19 - 00300/00547 - Loss: 0.05157. [ 69 s]\n",
      "Epoch: 19 - 00320/00547 - Loss: 0.06471. [ 74 s]\n",
      "Epoch: 19 - 00340/00547 - Loss: 0.08689. [ 78 s]\n",
      "Epoch: 19 - 00360/00547 - Loss: 0.04915. [ 83 s]\n",
      "Epoch: 19 - 00380/00547 - Loss: 0.03479. [ 88 s]\n",
      "Epoch: 19 - 00400/00547 - Loss: 0.04518. [ 92 s]\n",
      "Epoch: 19 - 00420/00547 - Loss: 0.07731. [ 97 s]\n",
      "Epoch: 19 - 00440/00547 - Loss: 0.04040. [101 s]\n",
      "Epoch: 19 - 00460/00547 - Loss: 0.02815. [106 s]\n",
      "Epoch: 19 - 00480/00547 - Loss: 0.05725. [111 s]\n",
      "Epoch: 19 - 00500/00547 - Loss: 0.09479. [115 s]\n",
      "Epoch: 19 - 00520/00547 - Loss: 0.08273. [120 s]\n",
      "Epoch: 19 - 00540/00547 - Loss: 0.07077. [125 s]\n",
      "Epoch: 19 - loss(trn/val):0.05029/0.21482, acc(val):92.36%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 20 - 00020/00547 - Loss: 0.11822. [  4 s]\n",
      "Epoch: 20 - 00040/00547 - Loss: 0.03471. [  9 s]\n",
      "Epoch: 20 - 00060/00547 - Loss: 0.07708. [ 14 s]\n",
      "Epoch: 20 - 00080/00547 - Loss: 0.03632. [ 18 s]\n",
      "Epoch: 20 - 00100/00547 - Loss: 0.04860. [ 23 s]\n",
      "Epoch: 20 - 00120/00547 - Loss: 0.06751. [ 27 s]\n",
      "Epoch: 20 - 00140/00547 - Loss: 0.03173. [ 32 s]\n",
      "Epoch: 20 - 00160/00547 - Loss: 0.04965. [ 37 s]\n",
      "Epoch: 20 - 00180/00547 - Loss: 0.03524. [ 41 s]\n",
      "Epoch: 20 - 00200/00547 - Loss: 0.05906. [ 46 s]\n",
      "Epoch: 20 - 00220/00547 - Loss: 0.09363. [ 51 s]\n",
      "Epoch: 20 - 00240/00547 - Loss: 0.03134. [ 55 s]\n",
      "Epoch: 20 - 00260/00547 - Loss: 0.03371. [ 60 s]\n",
      "Epoch: 20 - 00280/00547 - Loss: 0.03240. [ 64 s]\n",
      "Epoch: 20 - 00300/00547 - Loss: 0.03961. [ 69 s]\n",
      "Epoch: 20 - 00320/00547 - Loss: 0.03633. [ 74 s]\n",
      "Epoch: 20 - 00340/00547 - Loss: 0.04481. [ 78 s]\n",
      "Epoch: 20 - 00360/00547 - Loss: 0.05493. [ 83 s]\n",
      "Epoch: 20 - 00380/00547 - Loss: 0.04624. [ 88 s]\n",
      "Epoch: 20 - 00400/00547 - Loss: 0.09465. [ 92 s]\n",
      "Epoch: 20 - 00420/00547 - Loss: 0.03556. [ 97 s]\n",
      "Epoch: 20 - 00440/00547 - Loss: 0.02660. [101 s]\n",
      "Epoch: 20 - 00460/00547 - Loss: 0.06577. [106 s]\n",
      "Epoch: 20 - 00480/00547 - Loss: 0.04302. [111 s]\n",
      "Epoch: 20 - 00500/00547 - Loss: 0.04390. [115 s]\n",
      "Epoch: 20 - 00520/00547 - Loss: 0.04989. [120 s]\n",
      "Epoch: 20 - 00540/00547 - Loss: 0.04326. [125 s]\n",
      "Epoch: 20 - loss(trn/val):0.04621/0.23366, acc(val):92.05%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 21 - 00020/00547 - Loss: 0.02951. [  4 s]\n",
      "Epoch: 21 - 00040/00547 - Loss: 0.02507. [  9 s]\n",
      "Epoch: 21 - 00060/00547 - Loss: 0.04290. [ 14 s]\n",
      "Epoch: 21 - 00080/00547 - Loss: 0.03818. [ 18 s]\n",
      "Epoch: 21 - 00100/00547 - Loss: 0.04536. [ 23 s]\n",
      "Epoch: 21 - 00120/00547 - Loss: 0.02743. [ 27 s]\n",
      "Epoch: 21 - 00140/00547 - Loss: 0.02865. [ 32 s]\n",
      "Epoch: 21 - 00160/00547 - Loss: 0.08595. [ 37 s]\n",
      "Epoch: 21 - 00180/00547 - Loss: 0.04825. [ 41 s]\n",
      "Epoch: 21 - 00200/00547 - Loss: 0.04777. [ 46 s]\n",
      "Epoch: 21 - 00220/00547 - Loss: 0.03533. [ 50 s]\n",
      "Epoch: 21 - 00240/00547 - Loss: 0.02934. [ 55 s]\n",
      "Epoch: 21 - 00260/00547 - Loss: 0.02847. [ 60 s]\n",
      "Epoch: 21 - 00280/00547 - Loss: 0.09140. [ 64 s]\n",
      "Epoch: 21 - 00300/00547 - Loss: 0.02968. [ 69 s]\n",
      "Epoch: 21 - 00320/00547 - Loss: 0.06584. [ 74 s]\n",
      "Epoch: 21 - 00340/00547 - Loss: 0.05573. [ 78 s]\n",
      "Epoch: 21 - 00360/00547 - Loss: 0.02503. [ 83 s]\n",
      "Epoch: 21 - 00380/00547 - Loss: 0.02220. [ 87 s]\n",
      "Epoch: 21 - 00400/00547 - Loss: 0.06790. [ 92 s]\n",
      "Epoch: 21 - 00420/00547 - Loss: 0.05254. [ 97 s]\n",
      "Epoch: 21 - 00440/00547 - Loss: 0.04547. [101 s]\n",
      "Epoch: 21 - 00460/00547 - Loss: 0.03248. [106 s]\n",
      "Epoch: 21 - 00480/00547 - Loss: 0.05164. [110 s]\n",
      "Epoch: 21 - 00500/00547 - Loss: 0.05099. [115 s]\n",
      "Epoch: 21 - 00520/00547 - Loss: 0.01510. [120 s]\n",
      "Epoch: 21 - 00540/00547 - Loss: 0.04665. [124 s]\n",
      "Epoch: 21 - loss(trn/val):0.04849/0.24969, acc(val):91.88%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 22 - 00020/00547 - Loss: 0.06108. [  5 s]\n",
      "Epoch: 22 - 00040/00547 - Loss: 0.06297. [  9 s]\n",
      "Epoch: 22 - 00060/00547 - Loss: 0.03899. [ 14 s]\n",
      "Epoch: 22 - 00080/00547 - Loss: 0.04976. [ 18 s]\n",
      "Epoch: 22 - 00100/00547 - Loss: 0.20537. [ 23 s]\n",
      "Epoch: 22 - 00120/00547 - Loss: 0.04001. [ 27 s]\n",
      "Epoch: 22 - 00140/00547 - Loss: 0.05519. [ 32 s]\n",
      "Epoch: 22 - 00160/00547 - Loss: 0.07294. [ 37 s]\n",
      "Epoch: 22 - 00180/00547 - Loss: 0.03316. [ 41 s]\n",
      "Epoch: 22 - 00200/00547 - Loss: 0.05506. [ 46 s]\n",
      "Epoch: 22 - 00220/00547 - Loss: 0.07522. [ 50 s]\n",
      "Epoch: 22 - 00240/00547 - Loss: 0.06092. [ 55 s]\n",
      "Epoch: 22 - 00260/00547 - Loss: 0.03386. [ 60 s]\n",
      "Epoch: 22 - 00280/00547 - Loss: 0.01866. [ 64 s]\n",
      "Epoch: 22 - 00300/00547 - Loss: 0.05862. [ 69 s]\n",
      "Epoch: 22 - 00320/00547 - Loss: 0.05656. [ 74 s]\n",
      "Epoch: 22 - 00340/00547 - Loss: 0.04106. [ 78 s]\n",
      "Epoch: 22 - 00360/00547 - Loss: 0.04667. [ 83 s]\n",
      "Epoch: 22 - 00380/00547 - Loss: 0.02640. [ 87 s]\n",
      "Epoch: 22 - 00400/00547 - Loss: 0.08135. [ 92 s]\n",
      "Epoch: 22 - 00420/00547 - Loss: 0.02030. [ 97 s]\n",
      "Epoch: 22 - 00440/00547 - Loss: 0.07226. [101 s]\n",
      "Epoch: 22 - 00460/00547 - Loss: 0.03540. [106 s]\n",
      "Epoch: 22 - 00480/00547 - Loss: 0.05926. [111 s]\n",
      "Epoch: 22 - 00500/00547 - Loss: 0.08462. [115 s]\n",
      "Epoch: 22 - 00520/00547 - Loss: 0.05953. [120 s]\n",
      "Epoch: 22 - 00540/00547 - Loss: 0.02823. [124 s]\n",
      "Epoch: 22 - loss(trn/val):0.04412/0.22563, acc(val):92.09%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 23 - 00020/00547 - Loss: 0.04728. [  4 s]\n",
      "Epoch: 23 - 00040/00547 - Loss: 0.03071. [  9 s]\n",
      "Epoch: 23 - 00060/00547 - Loss: 0.02856. [ 14 s]\n",
      "Epoch: 23 - 00080/00547 - Loss: 0.02535. [ 18 s]\n",
      "Epoch: 23 - 00100/00547 - Loss: 0.07813. [ 23 s]\n",
      "Epoch: 23 - 00120/00547 - Loss: 0.11487. [ 27 s]\n",
      "Epoch: 23 - 00140/00547 - Loss: 0.04064. [ 32 s]\n",
      "Epoch: 23 - 00160/00547 - Loss: 0.02845. [ 37 s]\n",
      "Epoch: 23 - 00180/00547 - Loss: 0.02690. [ 41 s]\n",
      "Epoch: 23 - 00200/00547 - Loss: 0.08847. [ 46 s]\n",
      "Epoch: 23 - 00220/00547 - Loss: 0.04729. [ 50 s]\n",
      "Epoch: 23 - 00240/00547 - Loss: 0.03623. [ 55 s]\n",
      "Epoch: 23 - 00260/00547 - Loss: 0.04219. [ 60 s]\n",
      "Epoch: 23 - 00280/00547 - Loss: 0.04538. [ 64 s]\n",
      "Epoch: 23 - 00300/00547 - Loss: 0.05845. [ 69 s]\n",
      "Epoch: 23 - 00320/00547 - Loss: 0.05482. [ 74 s]\n",
      "Epoch: 23 - 00340/00547 - Loss: 0.03102. [ 78 s]\n",
      "Epoch: 23 - 00360/00547 - Loss: 0.04593. [ 83 s]\n",
      "Epoch: 23 - 00380/00547 - Loss: 0.06209. [ 88 s]\n",
      "Epoch: 23 - 00400/00547 - Loss: 0.05324. [ 92 s]\n",
      "Epoch: 23 - 00420/00547 - Loss: 0.03213. [ 97 s]\n",
      "Epoch: 23 - 00440/00547 - Loss: 0.04347. [101 s]\n",
      "Epoch: 23 - 00460/00547 - Loss: 0.02576. [106 s]\n",
      "Epoch: 23 - 00480/00547 - Loss: 0.09570. [111 s]\n",
      "Epoch: 23 - 00500/00547 - Loss: 0.05089. [115 s]\n",
      "Epoch: 23 - 00520/00547 - Loss: 0.06717. [120 s]\n",
      "Epoch: 23 - 00540/00547 - Loss: 0.02599. [125 s]\n",
      "Epoch: 23 - loss(trn/val):0.04862/0.35981, acc(val):90.53%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 24 - 00020/00547 - Loss: 0.05611. [  4 s]\n",
      "Epoch: 24 - 00040/00547 - Loss: 0.07903. [  9 s]\n",
      "Epoch: 24 - 00060/00547 - Loss: 0.06977. [ 14 s]\n",
      "Epoch: 24 - 00080/00547 - Loss: 0.06516. [ 18 s]\n",
      "Epoch: 24 - 00100/00547 - Loss: 0.08439. [ 23 s]\n",
      "Epoch: 24 - 00120/00547 - Loss: 0.07805. [ 27 s]\n",
      "Epoch: 24 - 00140/00547 - Loss: 0.05589. [ 32 s]\n",
      "Epoch: 24 - 00160/00547 - Loss: 0.03628. [ 36 s]\n",
      "Epoch: 24 - 00180/00547 - Loss: 0.11938. [ 41 s]\n",
      "Epoch: 24 - 00200/00547 - Loss: 0.08035. [ 46 s]\n",
      "Epoch: 24 - 00220/00547 - Loss: 0.04197. [ 50 s]\n",
      "Epoch: 24 - 00240/00547 - Loss: 0.03088. [ 55 s]\n",
      "Epoch: 24 - 00260/00547 - Loss: 0.03594. [ 60 s]\n",
      "Epoch: 24 - 00280/00547 - Loss: 0.03105. [ 64 s]\n",
      "Epoch: 24 - 00300/00547 - Loss: 0.06100. [ 69 s]\n",
      "Epoch: 24 - 00320/00547 - Loss: 0.02129. [ 73 s]\n",
      "Epoch: 24 - 00340/00547 - Loss: 0.05250. [ 78 s]\n",
      "Epoch: 24 - 00360/00547 - Loss: 0.08614. [ 83 s]\n",
      "Epoch: 24 - 00380/00547 - Loss: 0.03388. [ 87 s]\n",
      "Epoch: 24 - 00400/00547 - Loss: 0.02981. [ 92 s]\n",
      "Epoch: 24 - 00420/00547 - Loss: 0.04661. [ 97 s]\n",
      "Epoch: 24 - 00440/00547 - Loss: 0.02542. [101 s]\n",
      "Epoch: 24 - 00460/00547 - Loss: 0.04296. [106 s]\n",
      "Epoch: 24 - 00480/00547 - Loss: 0.10583. [110 s]\n",
      "Epoch: 24 - 00500/00547 - Loss: 0.14126. [115 s]\n",
      "Epoch: 24 - 00520/00547 - Loss: 0.05263. [120 s]\n",
      "Epoch: 24 - 00540/00547 - Loss: 0.03883. [124 s]\n",
      "Epoch: 24 - loss(trn/val):0.04690/0.28410, acc(val):91.29%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 25 - 00020/00547 - Loss: 0.03430. [  4 s]\n",
      "Epoch: 25 - 00040/00547 - Loss: 0.03062. [  9 s]\n",
      "Epoch: 25 - 00060/00547 - Loss: 0.04539. [ 14 s]\n",
      "Epoch: 25 - 00080/00547 - Loss: 0.06833. [ 18 s]\n",
      "Epoch: 25 - 00100/00547 - Loss: 0.04132. [ 23 s]\n",
      "Epoch: 25 - 00120/00547 - Loss: 0.02578. [ 27 s]\n",
      "Epoch: 25 - 00140/00547 - Loss: 0.04058. [ 32 s]\n",
      "Epoch: 25 - 00160/00547 - Loss: 0.04512. [ 37 s]\n",
      "Epoch: 25 - 00180/00547 - Loss: 0.04092. [ 41 s]\n",
      "Epoch: 25 - 00200/00547 - Loss: 0.05206. [ 46 s]\n",
      "Epoch: 25 - 00220/00547 - Loss: 0.03234. [ 50 s]\n",
      "Epoch: 25 - 00240/00547 - Loss: 0.02890. [ 55 s]\n",
      "Epoch: 25 - 00260/00547 - Loss: 0.06251. [ 60 s]\n",
      "Epoch: 25 - 00280/00547 - Loss: 0.02909. [ 64 s]\n",
      "Epoch: 25 - 00300/00547 - Loss: 0.06698. [ 69 s]\n",
      "Epoch: 25 - 00320/00547 - Loss: 0.04478. [ 74 s]\n",
      "Epoch: 25 - 00340/00547 - Loss: 0.09121. [ 78 s]\n",
      "Epoch: 25 - 00360/00547 - Loss: 0.04798. [ 83 s]\n",
      "Epoch: 25 - 00380/00547 - Loss: 0.03447. [ 88 s]\n",
      "Epoch: 25 - 00400/00547 - Loss: 0.06270. [ 92 s]\n",
      "Epoch: 25 - 00420/00547 - Loss: 0.03859. [ 97 s]\n",
      "Epoch: 25 - 00440/00547 - Loss: 0.02990. [101 s]\n",
      "Epoch: 25 - 00460/00547 - Loss: 0.05556. [106 s]\n",
      "Epoch: 25 - 00480/00547 - Loss: 0.08817. [111 s]\n",
      "Epoch: 25 - 00500/00547 - Loss: 0.03919. [115 s]\n",
      "Epoch: 25 - 00520/00547 - Loss: 0.05147. [120 s]\n",
      "Epoch: 25 - 00540/00547 - Loss: 0.04958. [125 s]\n",
      "Epoch: 25 - loss(trn/val):0.03615/0.34859, acc(val):90.99%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 26 - 00020/00547 - Loss: 0.04761. [  4 s]\n",
      "Epoch: 26 - 00040/00547 - Loss: 0.03881. [  9 s]\n",
      "Epoch: 26 - 00060/00547 - Loss: 0.02861. [ 14 s]\n",
      "Epoch: 26 - 00080/00547 - Loss: 0.02560. [ 18 s]\n",
      "Epoch: 26 - 00100/00547 - Loss: 0.08189. [ 23 s]\n",
      "Epoch: 26 - 00120/00547 - Loss: 0.02723. [ 27 s]\n",
      "Epoch: 26 - 00140/00547 - Loss: 0.04087. [ 32 s]\n",
      "Epoch: 26 - 00160/00547 - Loss: 0.02718. [ 37 s]\n",
      "Epoch: 26 - 00180/00547 - Loss: 0.05104. [ 41 s]\n",
      "Epoch: 26 - 00200/00547 - Loss: 0.02497. [ 46 s]\n",
      "Epoch: 26 - 00220/00547 - Loss: 0.04767. [ 51 s]\n",
      "Epoch: 26 - 00240/00547 - Loss: 0.03624. [ 55 s]\n",
      "Epoch: 26 - 00260/00547 - Loss: 0.04354. [ 60 s]\n",
      "Epoch: 26 - 00280/00547 - Loss: 0.04687. [ 64 s]\n",
      "Epoch: 26 - 00300/00547 - Loss: 0.02800. [ 69 s]\n",
      "Epoch: 26 - 00320/00547 - Loss: 0.02395. [ 74 s]\n",
      "Epoch: 26 - 00340/00547 - Loss: 0.07749. [ 78 s]\n",
      "Epoch: 26 - 00360/00547 - Loss: 0.04907. [ 83 s]\n",
      "Epoch: 26 - 00380/00547 - Loss: 0.03324. [ 88 s]\n",
      "Epoch: 26 - 00400/00547 - Loss: 0.03343. [ 92 s]\n",
      "Epoch: 26 - 00420/00547 - Loss: 0.05977. [ 97 s]\n",
      "Epoch: 26 - 00440/00547 - Loss: 0.04150. [101 s]\n",
      "Epoch: 26 - 00460/00547 - Loss: 0.02701. [106 s]\n",
      "Epoch: 26 - 00480/00547 - Loss: 0.06875. [111 s]\n",
      "Epoch: 26 - 00500/00547 - Loss: 0.04350. [115 s]\n",
      "Epoch: 26 - 00520/00547 - Loss: 0.04831. [120 s]\n",
      "Epoch: 26 - 00540/00547 - Loss: 0.02516. [125 s]\n",
      "Epoch: 26 - loss(trn/val):0.03965/0.35287, acc(val):89.80%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 27 - 00020/00547 - Loss: 0.03997. [  5 s]\n",
      "Epoch: 27 - 00040/00547 - Loss: 0.04601. [  9 s]\n",
      "Epoch: 27 - 00060/00547 - Loss: 0.04676. [ 14 s]\n",
      "Epoch: 27 - 00080/00547 - Loss: 0.03097. [ 18 s]\n",
      "Epoch: 27 - 00100/00547 - Loss: 0.03802. [ 23 s]\n",
      "Epoch: 27 - 00120/00547 - Loss: 0.03475. [ 28 s]\n",
      "Epoch: 27 - 00140/00547 - Loss: 0.03611. [ 32 s]\n",
      "Epoch: 27 - 00160/00547 - Loss: 0.05496. [ 37 s]\n",
      "Epoch: 27 - 00180/00547 - Loss: 0.05813. [ 42 s]\n",
      "Epoch: 27 - 00200/00547 - Loss: 0.04495. [ 46 s]\n",
      "Epoch: 27 - 00220/00547 - Loss: 0.06070. [ 51 s]\n",
      "Epoch: 27 - 00240/00547 - Loss: 0.03885. [ 56 s]\n",
      "Epoch: 27 - 00260/00547 - Loss: 0.10135. [ 60 s]\n",
      "Epoch: 27 - 00280/00547 - Loss: 0.04738. [ 65 s]\n",
      "Epoch: 27 - 00300/00547 - Loss: 0.03201. [ 69 s]\n",
      "Epoch: 27 - 00320/00547 - Loss: 0.03229. [ 74 s]\n",
      "Epoch: 27 - 00340/00547 - Loss: 0.02342. [ 79 s]\n",
      "Epoch: 27 - 00360/00547 - Loss: 0.02418. [ 84 s]\n",
      "Epoch: 27 - 00380/00547 - Loss: 0.03235. [ 88 s]\n",
      "Epoch: 27 - 00400/00547 - Loss: 0.01842. [ 93 s]\n",
      "Epoch: 27 - 00420/00547 - Loss: 0.02897. [ 97 s]\n",
      "Epoch: 27 - 00440/00547 - Loss: 0.03044. [102 s]\n",
      "Epoch: 27 - 00460/00547 - Loss: 0.03357. [107 s]\n",
      "Epoch: 27 - 00480/00547 - Loss: 0.04282. [111 s]\n",
      "Epoch: 27 - 00500/00547 - Loss: 0.04306. [116 s]\n",
      "Epoch: 27 - 00520/00547 - Loss: 0.06640. [121 s]\n",
      "Epoch: 27 - 00540/00547 - Loss: 0.05615. [125 s]\n",
      "Epoch: 27 - loss(trn/val):0.04217/0.35571, acc(val):88.98%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 28 - 00020/00547 - Loss: 0.02967. [  5 s]\n",
      "Epoch: 28 - 00040/00547 - Loss: 0.02658. [  9 s]\n",
      "Epoch: 28 - 00060/00547 - Loss: 0.06539. [ 14 s]\n",
      "Epoch: 28 - 00080/00547 - Loss: 0.04129. [ 19 s]\n",
      "Epoch: 28 - 00100/00547 - Loss: 0.02642. [ 23 s]\n",
      "Epoch: 28 - 00120/00547 - Loss: 0.04322. [ 28 s]\n",
      "Epoch: 28 - 00140/00547 - Loss: 0.02607. [ 32 s]\n",
      "Epoch: 28 - 00160/00547 - Loss: 0.01914. [ 37 s]\n",
      "Epoch: 28 - 00180/00547 - Loss: 0.03396. [ 42 s]\n",
      "Epoch: 28 - 00200/00547 - Loss: 0.05910. [ 46 s]\n",
      "Epoch: 28 - 00220/00547 - Loss: 0.02507. [ 51 s]\n",
      "Epoch: 28 - 00240/00547 - Loss: 0.09606. [ 56 s]\n",
      "Epoch: 28 - 00260/00547 - Loss: 0.03990. [ 60 s]\n",
      "Epoch: 28 - 00280/00547 - Loss: 0.09737. [ 65 s]\n",
      "Epoch: 28 - 00300/00547 - Loss: 0.03442. [ 70 s]\n",
      "Epoch: 28 - 00320/00547 - Loss: 0.03716. [ 74 s]\n",
      "Epoch: 28 - 00340/00547 - Loss: 0.11783. [ 79 s]\n",
      "Epoch: 28 - 00360/00547 - Loss: 0.02319. [ 84 s]\n",
      "Epoch: 28 - 00380/00547 - Loss: 0.04908. [ 88 s]\n",
      "Epoch: 28 - 00400/00547 - Loss: 0.02571. [ 93 s]\n",
      "Epoch: 28 - 00420/00547 - Loss: 0.02481. [ 98 s]\n",
      "Epoch: 28 - 00440/00547 - Loss: 0.02469. [102 s]\n",
      "Epoch: 28 - 00460/00547 - Loss: 0.07170. [107 s]\n",
      "Epoch: 28 - 00480/00547 - Loss: 0.06148. [112 s]\n",
      "Epoch: 28 - 00500/00547 - Loss: 0.07583. [116 s]\n",
      "Epoch: 28 - 00520/00547 - Loss: 0.05852. [121 s]\n",
      "Epoch: 28 - 00540/00547 - Loss: 0.11162. [126 s]\n",
      "Epoch: 28 - loss(trn/val):0.03769/0.45097, acc(val):87.21%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 29 - 00020/00547 - Loss: 0.06158. [  5 s]\n",
      "Epoch: 29 - 00040/00547 - Loss: 0.03944. [  9 s]\n",
      "Epoch: 29 - 00060/00547 - Loss: 0.04037. [ 14 s]\n",
      "Epoch: 29 - 00080/00547 - Loss: 0.03701. [ 18 s]\n",
      "Epoch: 29 - 00100/00547 - Loss: 0.05548. [ 23 s]\n",
      "Epoch: 29 - 00120/00547 - Loss: 0.05304. [ 28 s]\n",
      "Epoch: 29 - 00140/00547 - Loss: 0.02393. [ 32 s]\n",
      "Epoch: 29 - 00160/00547 - Loss: 0.04040. [ 37 s]\n",
      "Epoch: 29 - 00180/00547 - Loss: 0.05325. [ 42 s]\n",
      "Epoch: 29 - 00200/00547 - Loss: 0.04007. [ 46 s]\n",
      "Epoch: 29 - 00220/00547 - Loss: 0.02841. [ 51 s]\n",
      "Epoch: 29 - 00240/00547 - Loss: 0.02529. [ 56 s]\n",
      "Epoch: 29 - 00260/00547 - Loss: 0.02789. [ 60 s]\n",
      "Epoch: 29 - 00280/00547 - Loss: 0.02744. [ 65 s]\n",
      "Epoch: 29 - 00300/00547 - Loss: 0.04336. [ 70 s]\n",
      "Epoch: 29 - 00320/00547 - Loss: 0.04944. [ 74 s]\n",
      "Epoch: 29 - 00340/00547 - Loss: 0.08009. [ 79 s]\n",
      "Epoch: 29 - 00360/00547 - Loss: 0.04644. [ 84 s]\n",
      "Epoch: 29 - 00380/00547 - Loss: 0.08987. [ 88 s]\n",
      "Epoch: 29 - 00400/00547 - Loss: 0.03094. [ 93 s]\n",
      "Epoch: 29 - 00420/00547 - Loss: 0.01851. [ 98 s]\n",
      "Epoch: 29 - 00440/00547 - Loss: 0.03134. [102 s]\n",
      "Epoch: 29 - 00460/00547 - Loss: 0.04399. [107 s]\n",
      "Epoch: 29 - 00480/00547 - Loss: 0.02619. [112 s]\n",
      "Epoch: 29 - 00500/00547 - Loss: 0.03873. [116 s]\n",
      "Epoch: 29 - 00520/00547 - Loss: 0.03367. [121 s]\n",
      "Epoch: 29 - 00540/00547 - Loss: 0.03817. [126 s]\n",
      "Epoch: 29 - loss(trn/val):0.03631/0.27138, acc(val):92.00%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 30 - 00020/00547 - Loss: 0.04439. [  5 s]\n",
      "Epoch: 30 - 00040/00547 - Loss: 0.01961. [  9 s]\n",
      "Epoch: 30 - 00060/00547 - Loss: 0.04138. [ 14 s]\n",
      "Epoch: 30 - 00080/00547 - Loss: 0.04813. [ 18 s]\n",
      "Epoch: 30 - 00100/00547 - Loss: 0.06334. [ 23 s]\n",
      "Epoch: 30 - 00120/00547 - Loss: 0.02312. [ 27 s]\n",
      "Epoch: 30 - 00140/00547 - Loss: 0.03981. [ 32 s]\n",
      "Epoch: 30 - 00160/00547 - Loss: 0.06252. [ 37 s]\n",
      "Epoch: 30 - 00180/00547 - Loss: 0.03888. [ 41 s]\n",
      "Epoch: 30 - 00200/00547 - Loss: 0.02334. [ 46 s]\n",
      "Epoch: 30 - 00220/00547 - Loss: 0.02447. [ 50 s]\n",
      "Epoch: 30 - 00240/00547 - Loss: 0.10879. [ 55 s]\n",
      "Epoch: 30 - 00260/00547 - Loss: 0.03811. [ 60 s]\n",
      "Epoch: 30 - 00280/00547 - Loss: 0.04086. [ 64 s]\n",
      "Epoch: 30 - 00300/00547 - Loss: 0.02042. [ 69 s]\n",
      "Epoch: 30 - 00320/00547 - Loss: 0.03230. [ 74 s]\n",
      "Epoch: 30 - 00340/00547 - Loss: 0.02068. [ 78 s]\n",
      "Epoch: 30 - 00360/00547 - Loss: 0.03485. [ 83 s]\n",
      "Epoch: 30 - 00380/00547 - Loss: 0.04000. [ 88 s]\n",
      "Epoch: 30 - 00400/00547 - Loss: 0.04874. [ 92 s]\n",
      "Epoch: 30 - 00420/00547 - Loss: 0.04219. [ 97 s]\n",
      "Epoch: 30 - 00440/00547 - Loss: 0.03401. [101 s]\n",
      "Epoch: 30 - 00460/00547 - Loss: 0.05198. [106 s]\n",
      "Epoch: 30 - 00480/00547 - Loss: 0.03144. [111 s]\n",
      "Epoch: 30 - 00500/00547 - Loss: 0.02322. [115 s]\n",
      "Epoch: 30 - 00520/00547 - Loss: 0.04853. [120 s]\n",
      "Epoch: 30 - 00540/00547 - Loss: 0.02315. [124 s]\n",
      "Epoch: 30 - loss(trn/val):0.03292/0.26819, acc(val):92.62%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 31 - 00020/00547 - Loss: 0.04222. [  4 s]\n",
      "Epoch: 31 - 00040/00547 - Loss: 0.02860. [  9 s]\n",
      "Epoch: 31 - 00060/00547 - Loss: 0.03530. [ 14 s]\n",
      "Epoch: 31 - 00080/00547 - Loss: 0.01693. [ 18 s]\n",
      "Epoch: 31 - 00100/00547 - Loss: 0.01942. [ 23 s]\n",
      "Epoch: 31 - 00120/00547 - Loss: 0.02808. [ 27 s]\n",
      "Epoch: 31 - 00140/00547 - Loss: 0.05584. [ 32 s]\n",
      "Epoch: 31 - 00160/00547 - Loss: 0.02434. [ 37 s]\n",
      "Epoch: 31 - 00180/00547 - Loss: 0.04114. [ 41 s]\n",
      "Epoch: 31 - 00200/00547 - Loss: 0.02101. [ 46 s]\n",
      "Epoch: 31 - 00220/00547 - Loss: 0.02663. [ 50 s]\n",
      "Epoch: 31 - 00240/00547 - Loss: 0.04525. [ 55 s]\n",
      "Epoch: 31 - 00260/00547 - Loss: 0.02851. [ 60 s]\n",
      "Epoch: 31 - 00280/00547 - Loss: 0.02667. [ 64 s]\n",
      "Epoch: 31 - 00300/00547 - Loss: 0.05641. [ 69 s]\n",
      "Epoch: 31 - 00320/00547 - Loss: 0.04452. [ 74 s]\n",
      "Epoch: 31 - 00340/00547 - Loss: 0.04393. [ 78 s]\n",
      "Epoch: 31 - 00360/00547 - Loss: 0.05820. [ 83 s]\n",
      "Epoch: 31 - 00380/00547 - Loss: 0.04042. [ 87 s]\n",
      "Epoch: 31 - 00400/00547 - Loss: 0.03893. [ 92 s]\n",
      "Epoch: 31 - 00420/00547 - Loss: 0.02556. [ 97 s]\n",
      "Epoch: 31 - 00440/00547 - Loss: 0.04912. [101 s]\n",
      "Epoch: 31 - 00460/00547 - Loss: 0.08955. [106 s]\n",
      "Epoch: 31 - 00480/00547 - Loss: 0.08254. [110 s]\n",
      "Epoch: 31 - 00500/00547 - Loss: 0.06020. [115 s]\n",
      "Epoch: 31 - 00520/00547 - Loss: 0.03167. [120 s]\n",
      "Epoch: 31 - 00540/00547 - Loss: 0.02870. [124 s]\n",
      "Epoch: 31 - loss(trn/val):0.04121/0.33116, acc(val):91.10%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 32 - 00020/00547 - Loss: 0.04059. [  4 s]\n",
      "Epoch: 32 - 00040/00547 - Loss: 0.02639. [  9 s]\n",
      "Epoch: 32 - 00060/00547 - Loss: 0.02369. [ 14 s]\n",
      "Epoch: 32 - 00080/00547 - Loss: 0.02578. [ 18 s]\n",
      "Epoch: 32 - 00100/00547 - Loss: 0.03706. [ 23 s]\n",
      "Epoch: 32 - 00120/00547 - Loss: 0.03047. [ 27 s]\n",
      "Epoch: 32 - 00140/00547 - Loss: 0.04733. [ 32 s]\n",
      "Epoch: 32 - 00160/00547 - Loss: 0.05224. [ 37 s]\n",
      "Epoch: 32 - 00180/00547 - Loss: 0.04113. [ 41 s]\n",
      "Epoch: 32 - 00200/00547 - Loss: 0.04482. [ 46 s]\n",
      "Epoch: 32 - 00220/00547 - Loss: 0.03363. [ 50 s]\n",
      "Epoch: 32 - 00240/00547 - Loss: 0.04201. [ 55 s]\n",
      "Epoch: 32 - 00260/00547 - Loss: 0.05104. [ 60 s]\n",
      "Epoch: 32 - 00280/00547 - Loss: 0.04041. [ 64 s]\n",
      "Epoch: 32 - 00300/00547 - Loss: 0.04535. [ 69 s]\n",
      "Epoch: 32 - 00320/00547 - Loss: 0.04018. [ 74 s]\n",
      "Epoch: 32 - 00340/00547 - Loss: 0.04381. [ 78 s]\n",
      "Epoch: 32 - 00360/00547 - Loss: 0.03529. [ 83 s]\n",
      "Epoch: 32 - 00380/00547 - Loss: 0.07089. [ 88 s]\n",
      "Epoch: 32 - 00400/00547 - Loss: 0.02146. [ 92 s]\n",
      "Epoch: 32 - 00420/00547 - Loss: 0.05041. [ 97 s]\n",
      "Epoch: 32 - 00440/00547 - Loss: 0.03131. [101 s]\n",
      "Epoch: 32 - 00460/00547 - Loss: 0.07155. [106 s]\n",
      "Epoch: 32 - 00480/00547 - Loss: 0.02134. [111 s]\n",
      "Epoch: 32 - 00500/00547 - Loss: 0.04915. [115 s]\n",
      "Epoch: 32 - 00520/00547 - Loss: 0.03340. [120 s]\n",
      "Epoch: 32 - 00540/00547 - Loss: 0.04179. [124 s]\n",
      "Epoch: 32 - loss(trn/val):0.03182/0.23461, acc(val):92.95%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 33 - 00020/00547 - Loss: 0.06920. [  4 s]\n",
      "Epoch: 33 - 00040/00547 - Loss: 0.03250. [  9 s]\n",
      "Epoch: 33 - 00060/00547 - Loss: 0.03067. [ 14 s]\n",
      "Epoch: 33 - 00080/00547 - Loss: 0.02966. [ 18 s]\n",
      "Epoch: 33 - 00100/00547 - Loss: 0.04056. [ 23 s]\n",
      "Epoch: 33 - 00120/00547 - Loss: 0.03971. [ 27 s]\n",
      "Epoch: 33 - 00140/00547 - Loss: 0.05073. [ 32 s]\n",
      "Epoch: 33 - 00160/00547 - Loss: 0.02842. [ 37 s]\n",
      "Epoch: 33 - 00180/00547 - Loss: 0.03540. [ 41 s]\n",
      "Epoch: 33 - 00200/00547 - Loss: 0.03636. [ 46 s]\n",
      "Epoch: 33 - 00220/00547 - Loss: 0.04917. [ 51 s]\n",
      "Epoch: 33 - 00240/00547 - Loss: 0.03855. [ 55 s]\n",
      "Epoch: 33 - 00260/00547 - Loss: 0.03214. [ 60 s]\n",
      "Epoch: 33 - 00280/00547 - Loss: 0.03238. [ 64 s]\n",
      "Epoch: 33 - 00300/00547 - Loss: 0.05100. [ 69 s]\n",
      "Epoch: 33 - 00320/00547 - Loss: 0.04768. [ 74 s]\n",
      "Epoch: 33 - 00340/00547 - Loss: 0.04771. [ 78 s]\n",
      "Epoch: 33 - 00360/00547 - Loss: 0.01914. [ 83 s]\n",
      "Epoch: 33 - 00380/00547 - Loss: 0.02110. [ 88 s]\n",
      "Epoch: 33 - 00400/00547 - Loss: 0.06065. [ 92 s]\n",
      "Epoch: 33 - 00420/00547 - Loss: 0.03558. [ 97 s]\n",
      "Epoch: 33 - 00440/00547 - Loss: 0.02251. [102 s]\n",
      "Epoch: 33 - 00460/00547 - Loss: 0.03269. [106 s]\n",
      "Epoch: 33 - 00480/00547 - Loss: 0.01721. [111 s]\n",
      "Epoch: 33 - 00500/00547 - Loss: 0.03055. [116 s]\n",
      "Epoch: 33 - 00520/00547 - Loss: 0.03011. [120 s]\n",
      "Epoch: 33 - 00540/00547 - Loss: 0.03863. [125 s]\n",
      "Epoch: 33 - loss(trn/val):0.03054/0.28240, acc(val):92.27%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 34 - 00020/00547 - Loss: 0.01931. [  4 s]\n",
      "Epoch: 34 - 00040/00547 - Loss: 0.02996. [  9 s]\n",
      "Epoch: 34 - 00060/00547 - Loss: 0.03933. [ 14 s]\n",
      "Epoch: 34 - 00080/00547 - Loss: 0.02073. [ 18 s]\n",
      "Epoch: 34 - 00100/00547 - Loss: 0.05545. [ 23 s]\n",
      "Epoch: 34 - 00120/00547 - Loss: 0.01223. [ 28 s]\n",
      "Epoch: 34 - 00140/00547 - Loss: 0.03472. [ 32 s]\n",
      "Epoch: 34 - 00160/00547 - Loss: 0.02027. [ 37 s]\n",
      "Epoch: 34 - 00180/00547 - Loss: 0.04979. [ 41 s]\n",
      "Epoch: 34 - 00200/00547 - Loss: 0.03834. [ 46 s]\n",
      "Epoch: 34 - 00220/00547 - Loss: 0.09095. [ 51 s]\n",
      "Epoch: 34 - 00240/00547 - Loss: 0.02481. [ 55 s]\n",
      "Epoch: 34 - 00260/00547 - Loss: 0.03702. [ 60 s]\n",
      "Epoch: 34 - 00280/00547 - Loss: 0.05852. [ 65 s]\n",
      "Epoch: 34 - 00300/00547 - Loss: 0.02623. [ 69 s]\n",
      "Epoch: 34 - 00320/00547 - Loss: 0.05128. [ 74 s]\n",
      "Epoch: 34 - 00340/00547 - Loss: 0.04499. [ 79 s]\n",
      "Epoch: 34 - 00360/00547 - Loss: 0.05549. [ 83 s]\n",
      "Epoch: 34 - 00380/00547 - Loss: 0.07705. [ 88 s]\n",
      "Epoch: 34 - 00400/00547 - Loss: 0.03706. [ 93 s]\n",
      "Epoch: 34 - 00420/00547 - Loss: 0.02896. [ 97 s]\n",
      "Epoch: 34 - 00440/00547 - Loss: 0.02658. [102 s]\n",
      "Epoch: 34 - 00460/00547 - Loss: 0.02692. [107 s]\n",
      "Epoch: 34 - 00480/00547 - Loss: 0.03990. [111 s]\n",
      "Epoch: 34 - 00500/00547 - Loss: 0.03102. [116 s]\n",
      "Epoch: 34 - 00520/00547 - Loss: 0.02215. [121 s]\n",
      "Epoch: 34 - 00540/00547 - Loss: 0.05440. [125 s]\n",
      "Epoch: 34 - loss(trn/val):0.02908/0.30345, acc(val):91.79%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 35 - 00020/00547 - Loss: 0.04134. [  5 s]\n",
      "Epoch: 35 - 00040/00547 - Loss: 0.03325. [  9 s]\n",
      "Epoch: 35 - 00060/00547 - Loss: 0.04432. [ 14 s]\n",
      "Epoch: 35 - 00080/00547 - Loss: 0.03251. [ 18 s]\n",
      "Epoch: 35 - 00100/00547 - Loss: 0.03172. [ 23 s]\n",
      "Epoch: 35 - 00120/00547 - Loss: 0.03475. [ 28 s]\n",
      "Epoch: 35 - 00140/00547 - Loss: 0.01667. [ 32 s]\n",
      "Epoch: 35 - 00160/00547 - Loss: 0.04999. [ 37 s]\n",
      "Epoch: 35 - 00180/00547 - Loss: 0.01649. [ 41 s]\n",
      "Epoch: 35 - 00200/00547 - Loss: 0.01989. [ 46 s]\n",
      "Epoch: 35 - 00220/00547 - Loss: 0.02437. [ 51 s]\n",
      "Epoch: 35 - 00240/00547 - Loss: 0.04281. [ 55 s]\n",
      "Epoch: 35 - 00260/00547 - Loss: 0.01824. [ 60 s]\n",
      "Epoch: 35 - 00280/00547 - Loss: 0.02185. [ 65 s]\n",
      "Epoch: 35 - 00300/00547 - Loss: 0.01991. [ 69 s]\n",
      "Epoch: 35 - 00320/00547 - Loss: 0.02026. [ 74 s]\n",
      "Epoch: 35 - 00340/00547 - Loss: 0.03882. [ 79 s]\n",
      "Epoch: 35 - 00360/00547 - Loss: 0.02522. [ 83 s]\n",
      "Epoch: 35 - 00380/00547 - Loss: 0.04158. [ 88 s]\n",
      "Epoch: 35 - 00400/00547 - Loss: 0.04043. [ 93 s]\n",
      "Epoch: 35 - 00420/00547 - Loss: 0.01883. [ 97 s]\n",
      "Epoch: 35 - 00440/00547 - Loss: 0.06597. [102 s]\n",
      "Epoch: 35 - 00460/00547 - Loss: 0.04276. [107 s]\n",
      "Epoch: 35 - 00480/00547 - Loss: 0.08142. [111 s]\n",
      "Epoch: 35 - 00500/00547 - Loss: 0.02608. [116 s]\n",
      "Epoch: 35 - 00520/00547 - Loss: 0.04854. [121 s]\n",
      "Epoch: 35 - 00540/00547 - Loss: 0.06349. [125 s]\n",
      "Epoch: 35 - loss(trn/val):0.03497/0.26141, acc(val):91.88%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 36 - 00020/00547 - Loss: 0.03690. [  5 s]\n",
      "Epoch: 36 - 00040/00547 - Loss: 0.02126. [  9 s]\n",
      "Epoch: 36 - 00060/00547 - Loss: 0.04946. [ 14 s]\n",
      "Epoch: 36 - 00080/00547 - Loss: 0.03917. [ 18 s]\n",
      "Epoch: 36 - 00100/00547 - Loss: 0.01733. [ 23 s]\n",
      "Epoch: 36 - 00120/00547 - Loss: 0.01864. [ 28 s]\n",
      "Epoch: 36 - 00140/00547 - Loss: 0.03746. [ 32 s]\n",
      "Epoch: 36 - 00160/00547 - Loss: 0.03341. [ 37 s]\n",
      "Epoch: 36 - 00180/00547 - Loss: 0.02170. [ 42 s]\n",
      "Epoch: 36 - 00200/00547 - Loss: 0.05760. [ 46 s]\n",
      "Epoch: 36 - 00220/00547 - Loss: 0.03617. [ 51 s]\n",
      "Epoch: 36 - 00240/00547 - Loss: 0.03151. [ 56 s]\n",
      "Epoch: 36 - 00260/00547 - Loss: 0.02101. [ 60 s]\n",
      "Epoch: 36 - 00280/00547 - Loss: 0.02748. [ 65 s]\n",
      "Epoch: 36 - 00300/00547 - Loss: 0.03186. [ 70 s]\n",
      "Epoch: 36 - 00320/00547 - Loss: 0.04687. [ 74 s]\n",
      "Epoch: 36 - 00340/00547 - Loss: 0.04693. [ 79 s]\n",
      "Epoch: 36 - 00360/00547 - Loss: 0.04901. [ 84 s]\n",
      "Epoch: 36 - 00380/00547 - Loss: 0.02054. [ 88 s]\n",
      "Epoch: 36 - 00400/00547 - Loss: 0.03087. [ 93 s]\n",
      "Epoch: 36 - 00420/00547 - Loss: 0.04555. [ 98 s]\n",
      "Epoch: 36 - 00440/00547 - Loss: 0.02729. [102 s]\n",
      "Epoch: 36 - 00460/00547 - Loss: 0.02694. [107 s]\n",
      "Epoch: 36 - 00480/00547 - Loss: 0.01853. [112 s]\n",
      "Epoch: 36 - 00500/00547 - Loss: 0.02839. [116 s]\n",
      "Epoch: 36 - 00520/00547 - Loss: 0.02138. [121 s]\n",
      "Epoch: 36 - 00540/00547 - Loss: 0.05972. [126 s]\n",
      "Epoch: 36 - loss(trn/val):0.03769/0.27909, acc(val):90.52%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 37 - 00020/00547 - Loss: 0.02604. [  5 s]\n",
      "Epoch: 37 - 00040/00547 - Loss: 0.02621. [  9 s]\n",
      "Epoch: 37 - 00060/00547 - Loss: 0.04579. [ 14 s]\n",
      "Epoch: 37 - 00080/00547 - Loss: 0.02349. [ 18 s]\n",
      "Epoch: 37 - 00100/00547 - Loss: 0.01309. [ 23 s]\n",
      "Epoch: 37 - 00120/00547 - Loss: 0.03637. [ 27 s]\n",
      "Epoch: 37 - 00140/00547 - Loss: 0.02785. [ 32 s]\n",
      "Epoch: 37 - 00160/00547 - Loss: 0.02543. [ 37 s]\n",
      "Epoch: 37 - 00180/00547 - Loss: 0.02152. [ 41 s]\n",
      "Epoch: 37 - 00200/00547 - Loss: 0.04459. [ 46 s]\n",
      "Epoch: 37 - 00220/00547 - Loss: 0.02628. [ 51 s]\n",
      "Epoch: 37 - 00240/00547 - Loss: 0.01835. [ 55 s]\n",
      "Epoch: 37 - 00260/00547 - Loss: 0.02879. [ 60 s]\n",
      "Epoch: 37 - 00280/00547 - Loss: 0.01818. [ 64 s]\n",
      "Epoch: 37 - 00300/00547 - Loss: 0.01973. [ 69 s]\n",
      "Epoch: 37 - 00320/00547 - Loss: 0.04159. [ 74 s]\n",
      "Epoch: 37 - 00340/00547 - Loss: 0.03335. [ 78 s]\n",
      "Epoch: 37 - 00360/00547 - Loss: 0.01702. [ 83 s]\n",
      "Epoch: 37 - 00380/00547 - Loss: 0.02321. [ 88 s]\n",
      "Epoch: 37 - 00400/00547 - Loss: 0.01566. [ 92 s]\n",
      "Epoch: 37 - 00420/00547 - Loss: 0.02462. [ 97 s]\n",
      "Epoch: 37 - 00440/00547 - Loss: 0.02349. [102 s]\n",
      "Epoch: 37 - 00460/00547 - Loss: 0.03708. [106 s]\n",
      "Epoch: 37 - 00480/00547 - Loss: 0.05169. [111 s]\n",
      "Epoch: 37 - 00500/00547 - Loss: 0.02131. [115 s]\n",
      "Epoch: 37 - 00520/00547 - Loss: 0.03216. [120 s]\n",
      "Epoch: 37 - 00540/00547 - Loss: 0.03979. [125 s]\n",
      "Epoch: 37 - loss(trn/val):0.02791/0.25021, acc(val):92.99%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 38 - 00020/00547 - Loss: 0.01838. [  4 s]\n",
      "Epoch: 38 - 00040/00547 - Loss: 0.03060. [  9 s]\n",
      "Epoch: 38 - 00060/00547 - Loss: 0.04152. [ 14 s]\n",
      "Epoch: 38 - 00080/00547 - Loss: 0.02189. [ 18 s]\n",
      "Epoch: 38 - 00100/00547 - Loss: 0.02763. [ 23 s]\n",
      "Epoch: 38 - 00120/00547 - Loss: 0.03864. [ 27 s]\n",
      "Epoch: 38 - 00140/00547 - Loss: 0.03728. [ 32 s]\n",
      "Epoch: 38 - 00160/00547 - Loss: 0.03143. [ 37 s]\n",
      "Epoch: 38 - 00180/00547 - Loss: 0.02882. [ 41 s]\n",
      "Epoch: 38 - 00200/00547 - Loss: 0.03717. [ 46 s]\n",
      "Epoch: 38 - 00220/00547 - Loss: 0.02495. [ 51 s]\n",
      "Epoch: 38 - 00240/00547 - Loss: 0.04171. [ 55 s]\n",
      "Epoch: 38 - 00260/00547 - Loss: 0.01813. [ 60 s]\n",
      "Epoch: 38 - 00280/00547 - Loss: 0.02507. [ 64 s]\n",
      "Epoch: 38 - 00300/00547 - Loss: 0.03483. [ 69 s]\n",
      "Epoch: 38 - 00320/00547 - Loss: 0.03478. [ 74 s]\n",
      "Epoch: 38 - 00340/00547 - Loss: 0.01922. [ 78 s]\n",
      "Epoch: 38 - 00360/00547 - Loss: 0.02275. [ 83 s]\n",
      "Epoch: 38 - 00380/00547 - Loss: 0.02053. [ 88 s]\n",
      "Epoch: 38 - 00400/00547 - Loss: 0.04291. [ 92 s]\n",
      "Epoch: 38 - 00420/00547 - Loss: 0.05977. [ 97 s]\n",
      "Epoch: 38 - 00440/00547 - Loss: 0.01897. [102 s]\n",
      "Epoch: 38 - 00460/00547 - Loss: 0.01989. [106 s]\n",
      "Epoch: 38 - 00480/00547 - Loss: 0.04353. [111 s]\n",
      "Epoch: 38 - 00500/00547 - Loss: 0.02763. [116 s]\n",
      "Epoch: 38 - 00520/00547 - Loss: 0.05148. [120 s]\n",
      "Epoch: 38 - 00540/00547 - Loss: 0.02887. [125 s]\n",
      "Epoch: 38 - loss(trn/val):0.03407/0.27852, acc(val):91.45%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 39 - 00020/00547 - Loss: 0.02668. [  4 s]\n",
      "Epoch: 39 - 00040/00547 - Loss: 0.03761. [  9 s]\n",
      "Epoch: 39 - 00060/00547 - Loss: 0.02928. [ 14 s]\n",
      "Epoch: 39 - 00080/00547 - Loss: 0.04547. [ 18 s]\n",
      "Epoch: 39 - 00100/00547 - Loss: 0.03359. [ 23 s]\n",
      "Epoch: 39 - 00120/00547 - Loss: 0.02596. [ 27 s]\n",
      "Epoch: 39 - 00140/00547 - Loss: 0.04014. [ 32 s]\n",
      "Epoch: 39 - 00160/00547 - Loss: 0.02498. [ 37 s]\n",
      "Epoch: 39 - 00180/00547 - Loss: 0.02591. [ 41 s]\n",
      "Epoch: 39 - 00200/00547 - Loss: 0.02855. [ 46 s]\n",
      "Epoch: 39 - 00220/00547 - Loss: 0.04993. [ 51 s]\n",
      "Epoch: 39 - 00240/00547 - Loss: 0.12710. [ 55 s]\n",
      "Epoch: 39 - 00260/00547 - Loss: 0.05129. [ 60 s]\n",
      "Epoch: 39 - 00280/00547 - Loss: 0.01142. [ 64 s]\n",
      "Epoch: 39 - 00300/00547 - Loss: 0.03425. [ 69 s]\n",
      "Epoch: 39 - 00320/00547 - Loss: 0.02637. [ 74 s]\n",
      "Epoch: 39 - 00340/00547 - Loss: 0.02594. [ 78 s]\n",
      "Epoch: 39 - 00360/00547 - Loss: 0.02214. [ 83 s]\n",
      "Epoch: 39 - 00380/00547 - Loss: 0.02887. [ 88 s]\n",
      "Epoch: 39 - 00400/00547 - Loss: 0.02759. [ 92 s]\n",
      "Epoch: 39 - 00420/00547 - Loss: 0.01927. [ 97 s]\n",
      "Epoch: 39 - 00440/00547 - Loss: 0.02820. [102 s]\n",
      "Epoch: 39 - 00460/00547 - Loss: 0.04067. [106 s]\n",
      "Epoch: 39 - 00480/00547 - Loss: 0.04561. [111 s]\n",
      "Epoch: 39 - 00500/00547 - Loss: 0.03721. [115 s]\n",
      "Epoch: 39 - 00520/00547 - Loss: 0.05107. [120 s]\n",
      "Epoch: 39 - 00540/00547 - Loss: 0.02558. [125 s]\n",
      "Epoch: 39 - loss(trn/val):0.02661/0.30067, acc(val):91.67%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 40 - 00020/00547 - Loss: 0.01423. [  4 s]\n",
      "Epoch: 40 - 00040/00547 - Loss: 0.01265. [  9 s]\n",
      "Epoch: 40 - 00060/00547 - Loss: 0.01616. [ 14 s]\n",
      "Epoch: 40 - 00080/00547 - Loss: 0.03344. [ 18 s]\n",
      "Epoch: 40 - 00100/00547 - Loss: 0.04259. [ 23 s]\n",
      "Epoch: 40 - 00120/00547 - Loss: 0.02083. [ 27 s]\n",
      "Epoch: 40 - 00140/00547 - Loss: 0.03976. [ 32 s]\n",
      "Epoch: 40 - 00160/00547 - Loss: 0.01803. [ 37 s]\n",
      "Epoch: 40 - 00180/00547 - Loss: 0.02908. [ 41 s]\n",
      "Epoch: 40 - 00200/00547 - Loss: 0.02182. [ 46 s]\n",
      "Epoch: 40 - 00220/00547 - Loss: 0.03368. [ 51 s]\n",
      "Epoch: 40 - 00240/00547 - Loss: 0.06421. [ 55 s]\n",
      "Epoch: 40 - 00260/00547 - Loss: 0.01752. [ 60 s]\n",
      "Epoch: 40 - 00280/00547 - Loss: 0.02494. [ 65 s]\n",
      "Epoch: 40 - 00300/00547 - Loss: 0.02160. [ 69 s]\n",
      "Epoch: 40 - 00320/00547 - Loss: 0.03022. [ 74 s]\n",
      "Epoch: 40 - 00340/00547 - Loss: 0.03996. [ 79 s]\n",
      "Epoch: 40 - 00360/00547 - Loss: 0.02883. [ 84 s]\n",
      "Epoch: 40 - 00380/00547 - Loss: 0.02224. [ 88 s]\n",
      "Epoch: 40 - 00400/00547 - Loss: 0.03553. [ 93 s]\n",
      "Epoch: 40 - 00420/00547 - Loss: 0.04230. [ 98 s]\n",
      "Epoch: 40 - 00440/00547 - Loss: 0.02133. [102 s]\n",
      "Epoch: 40 - 00460/00547 - Loss: 0.02105. [107 s]\n",
      "Epoch: 40 - 00480/00547 - Loss: 0.05318. [112 s]\n",
      "Epoch: 40 - 00500/00547 - Loss: 0.02150. [116 s]\n",
      "Epoch: 40 - 00520/00547 - Loss: 0.02387. [121 s]\n",
      "Epoch: 40 - 00540/00547 - Loss: 0.03448. [126 s]\n",
      "Epoch: 40 - loss(trn/val):0.02620/0.35355, acc(val):90.81%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 41 - 00020/00547 - Loss: 0.03108. [  5 s]\n",
      "Epoch: 41 - 00040/00547 - Loss: 0.03958. [  9 s]\n",
      "Epoch: 41 - 00060/00547 - Loss: 0.04095. [ 14 s]\n",
      "Epoch: 41 - 00080/00547 - Loss: 0.02648. [ 18 s]\n",
      "Epoch: 41 - 00100/00547 - Loss: 0.02522. [ 23 s]\n",
      "Epoch: 41 - 00120/00547 - Loss: 0.01570. [ 28 s]\n",
      "Epoch: 41 - 00140/00547 - Loss: 0.02309. [ 32 s]\n",
      "Epoch: 41 - 00160/00547 - Loss: 0.02433. [ 37 s]\n",
      "Epoch: 41 - 00180/00547 - Loss: 0.02702. [ 42 s]\n",
      "Epoch: 41 - 00200/00547 - Loss: 0.02795. [ 46 s]\n",
      "Epoch: 41 - 00220/00547 - Loss: 0.05327. [ 51 s]\n",
      "Epoch: 41 - 00240/00547 - Loss: 0.03513. [ 56 s]\n",
      "Epoch: 41 - 00260/00547 - Loss: 0.02754. [ 60 s]\n",
      "Epoch: 41 - 00280/00547 - Loss: 0.06526. [ 65 s]\n",
      "Epoch: 41 - 00300/00547 - Loss: 0.02503. [ 70 s]\n",
      "Epoch: 41 - 00320/00547 - Loss: 0.04706. [ 74 s]\n",
      "Epoch: 41 - 00340/00547 - Loss: 0.02172. [ 79 s]\n",
      "Epoch: 41 - 00360/00547 - Loss: 0.06581. [ 84 s]\n",
      "Epoch: 41 - 00380/00547 - Loss: 0.03839. [ 88 s]\n",
      "Epoch: 41 - 00400/00547 - Loss: 0.04736. [ 93 s]\n",
      "Epoch: 41 - 00420/00547 - Loss: 0.03683. [ 98 s]\n",
      "Epoch: 41 - 00440/00547 - Loss: 0.03760. [103 s]\n",
      "Epoch: 41 - 00460/00547 - Loss: 0.03263. [107 s]\n",
      "Epoch: 41 - 00480/00547 - Loss: 0.03207. [112 s]\n",
      "Epoch: 41 - 00500/00547 - Loss: 0.04081. [117 s]\n",
      "Epoch: 41 - 00520/00547 - Loss: 0.02616. [121 s]\n",
      "Epoch: 41 - 00540/00547 - Loss: 0.03150. [126 s]\n",
      "Epoch: 41 - loss(trn/val):0.02576/0.27454, acc(val):92.36%, lr=0.00010. [128s] @17 samples/s \n",
      "Epoch: 42 - 00020/00547 - Loss: 0.03931. [  5 s]\n",
      "Epoch: 42 - 00040/00547 - Loss: 0.01858. [  9 s]\n",
      "Epoch: 42 - 00060/00547 - Loss: 0.02367. [ 14 s]\n",
      "Epoch: 42 - 00080/00547 - Loss: 0.02043. [ 18 s]\n",
      "Epoch: 42 - 00100/00547 - Loss: 0.02866. [ 23 s]\n",
      "Epoch: 42 - 00120/00547 - Loss: 0.01895. [ 28 s]\n",
      "Epoch: 42 - 00140/00547 - Loss: 0.02683. [ 32 s]\n",
      "Epoch: 42 - 00160/00547 - Loss: 0.02546. [ 37 s]\n",
      "Epoch: 42 - 00180/00547 - Loss: 0.01835. [ 42 s]\n",
      "Epoch: 42 - 00200/00547 - Loss: 0.03592. [ 46 s]\n",
      "Epoch: 42 - 00220/00547 - Loss: 0.02672. [ 51 s]\n",
      "Epoch: 42 - 00240/00547 - Loss: 0.04251. [ 55 s]\n",
      "Epoch: 42 - 00260/00547 - Loss: 0.03233. [ 60 s]\n",
      "Epoch: 42 - 00280/00547 - Loss: 0.18278. [ 65 s]\n",
      "Epoch: 42 - 00300/00547 - Loss: 0.01559. [ 70 s]\n",
      "Epoch: 42 - 00320/00547 - Loss: 0.03465. [ 74 s]\n",
      "Epoch: 42 - 00340/00547 - Loss: 0.03652. [ 79 s]\n",
      "Epoch: 42 - 00360/00547 - Loss: 0.03368. [ 84 s]\n",
      "Epoch: 42 - 00380/00547 - Loss: 0.02048. [ 88 s]\n",
      "Epoch: 42 - 00400/00547 - Loss: 0.02632. [ 93 s]\n",
      "Epoch: 42 - 00420/00547 - Loss: 0.03211. [ 98 s]\n",
      "Epoch: 42 - 00440/00547 - Loss: 0.02717. [102 s]\n",
      "Epoch: 42 - 00460/00547 - Loss: 0.03043. [107 s]\n",
      "Epoch: 42 - 00480/00547 - Loss: 0.04010. [112 s]\n",
      "Epoch: 42 - 00500/00547 - Loss: 0.03779. [116 s]\n",
      "Epoch: 42 - 00520/00547 - Loss: 0.02316. [121 s]\n",
      "Epoch: 42 - 00540/00547 - Loss: 0.02277. [126 s]\n",
      "Epoch: 42 - loss(trn/val):0.02383/0.35923, acc(val):90.99%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 43 - 00020/00547 - Loss: 0.02911. [  5 s]\n",
      "Epoch: 43 - 00040/00547 - Loss: 0.02953. [  9 s]\n",
      "Epoch: 43 - 00060/00547 - Loss: 0.01950. [ 14 s]\n",
      "Epoch: 43 - 00080/00547 - Loss: 0.02098. [ 19 s]\n",
      "Epoch: 43 - 00100/00547 - Loss: 0.01502. [ 23 s]\n",
      "Epoch: 43 - 00120/00547 - Loss: 0.01884. [ 28 s]\n",
      "Epoch: 43 - 00140/00547 - Loss: 0.01418. [ 32 s]\n",
      "Epoch: 43 - 00160/00547 - Loss: 0.03389. [ 37 s]\n",
      "Epoch: 43 - 00180/00547 - Loss: 0.02187. [ 42 s]\n",
      "Epoch: 43 - 00200/00547 - Loss: 0.02565. [ 46 s]\n",
      "Epoch: 43 - 00220/00547 - Loss: 0.03012. [ 51 s]\n",
      "Epoch: 43 - 00240/00547 - Loss: 0.02985. [ 56 s]\n",
      "Epoch: 43 - 00260/00547 - Loss: 0.02084. [ 61 s]\n",
      "Epoch: 43 - 00280/00547 - Loss: 0.04080. [ 65 s]\n",
      "Epoch: 43 - 00300/00547 - Loss: 0.02245. [ 70 s]\n",
      "Epoch: 43 - 00320/00547 - Loss: 0.01814. [ 75 s]\n",
      "Epoch: 43 - 00340/00547 - Loss: 0.07293. [ 79 s]\n",
      "Epoch: 43 - 00360/00547 - Loss: 0.04884. [ 84 s]\n",
      "Epoch: 43 - 00380/00547 - Loss: 0.04656. [ 89 s]\n",
      "Epoch: 43 - 00400/00547 - Loss: 0.02621. [ 93 s]\n",
      "Epoch: 43 - 00420/00547 - Loss: 0.03725. [ 98 s]\n",
      "Epoch: 43 - 00440/00547 - Loss: 0.03882. [103 s]\n",
      "Epoch: 43 - 00460/00547 - Loss: 0.02998. [107 s]\n",
      "Epoch: 43 - 00480/00547 - Loss: 0.02466. [112 s]\n",
      "Epoch: 43 - 00500/00547 - Loss: 0.03434. [117 s]\n",
      "Epoch: 43 - 00520/00547 - Loss: 0.01957. [121 s]\n",
      "Epoch: 43 - 00540/00547 - Loss: 0.03833. [126 s]\n",
      "Epoch: 43 - loss(trn/val):0.03999/0.29471, acc(val):90.69%, lr=0.00010. [128s] @17 samples/s \n",
      "Epoch: 44 - 00020/00547 - Loss: 0.02543. [  5 s]\n",
      "Epoch: 44 - 00040/00547 - Loss: 0.03963. [  9 s]\n",
      "Epoch: 44 - 00060/00547 - Loss: 0.03056. [ 14 s]\n",
      "Epoch: 44 - 00080/00547 - Loss: 0.02288. [ 18 s]\n",
      "Epoch: 44 - 00100/00547 - Loss: 0.03565. [ 23 s]\n",
      "Epoch: 44 - 00120/00547 - Loss: 0.01967. [ 28 s]\n",
      "Epoch: 44 - 00140/00547 - Loss: 0.03234. [ 32 s]\n",
      "Epoch: 44 - 00160/00547 - Loss: 0.04668. [ 37 s]\n",
      "Epoch: 44 - 00180/00547 - Loss: 0.01519. [ 42 s]\n",
      "Epoch: 44 - 00200/00547 - Loss: 0.02557. [ 46 s]\n",
      "Epoch: 44 - 00220/00547 - Loss: 0.02102. [ 51 s]\n",
      "Epoch: 44 - 00240/00547 - Loss: 0.02779. [ 55 s]\n",
      "Epoch: 44 - 00260/00547 - Loss: 0.03510. [ 60 s]\n",
      "Epoch: 44 - 00280/00547 - Loss: 0.02791. [ 65 s]\n",
      "Epoch: 44 - 00300/00547 - Loss: 0.02010. [ 69 s]\n",
      "Epoch: 44 - 00320/00547 - Loss: 0.03246. [ 74 s]\n",
      "Epoch: 44 - 00340/00547 - Loss: 0.02833. [ 79 s]\n",
      "Epoch: 44 - 00360/00547 - Loss: 0.04996. [ 83 s]\n",
      "Epoch: 44 - 00380/00547 - Loss: 0.02294. [ 88 s]\n",
      "Epoch: 44 - 00400/00547 - Loss: 0.02625. [ 93 s]\n",
      "Epoch: 44 - 00420/00547 - Loss: 0.02830. [ 97 s]\n",
      "Epoch: 44 - 00440/00547 - Loss: 0.02864. [102 s]\n",
      "Epoch: 44 - 00460/00547 - Loss: 0.03753. [107 s]\n",
      "Epoch: 44 - 00480/00547 - Loss: 0.06045. [111 s]\n",
      "Epoch: 44 - 00500/00547 - Loss: 0.03405. [116 s]\n",
      "Epoch: 44 - 00520/00547 - Loss: 0.02849. [121 s]\n",
      "Epoch: 44 - 00540/00547 - Loss: 0.01393. [125 s]\n",
      "Epoch: 44 - loss(trn/val):0.02541/0.31592, acc(val):91.93%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 45 - 00020/00547 - Loss: 0.02178. [  5 s]\n",
      "Epoch: 45 - 00040/00547 - Loss: 0.04173. [  9 s]\n",
      "Epoch: 45 - 00060/00547 - Loss: 0.01737. [ 14 s]\n",
      "Epoch: 45 - 00080/00547 - Loss: 0.02471. [ 18 s]\n",
      "Epoch: 45 - 00100/00547 - Loss: 0.02587. [ 23 s]\n",
      "Epoch: 45 - 00120/00547 - Loss: 0.02605. [ 28 s]\n",
      "Epoch: 45 - 00140/00547 - Loss: 0.02903. [ 32 s]\n",
      "Epoch: 45 - 00160/00547 - Loss: 0.01829. [ 37 s]\n",
      "Epoch: 45 - 00180/00547 - Loss: 0.01166. [ 42 s]\n",
      "Epoch: 45 - 00200/00547 - Loss: 0.02547. [ 46 s]\n",
      "Epoch: 45 - 00220/00547 - Loss: 0.02609. [ 51 s]\n",
      "Epoch: 45 - 00240/00547 - Loss: 0.02457. [ 56 s]\n",
      "Epoch: 45 - 00260/00547 - Loss: 0.01762. [ 60 s]\n",
      "Epoch: 45 - 00280/00547 - Loss: 0.03990. [ 65 s]\n",
      "Epoch: 45 - 00300/00547 - Loss: 0.02562. [ 70 s]\n",
      "Epoch: 45 - 00320/00547 - Loss: 0.03611. [ 74 s]\n",
      "Epoch: 45 - 00340/00547 - Loss: 0.04200. [ 79 s]\n",
      "Epoch: 45 - 00360/00547 - Loss: 0.02635. [ 84 s]\n",
      "Epoch: 45 - 00380/00547 - Loss: 0.03413. [ 88 s]\n",
      "Epoch: 45 - 00400/00547 - Loss: 0.05161. [ 93 s]\n",
      "Epoch: 45 - 00420/00547 - Loss: 0.02795. [ 98 s]\n",
      "Epoch: 45 - 00440/00547 - Loss: 0.02707. [102 s]\n",
      "Epoch: 45 - 00460/00547 - Loss: 0.01832. [107 s]\n",
      "Epoch: 45 - 00480/00547 - Loss: 0.02049. [112 s]\n",
      "Epoch: 45 - 00500/00547 - Loss: 0.02167. [116 s]\n",
      "Epoch: 45 - 00520/00547 - Loss: 0.02591. [121 s]\n",
      "Epoch: 45 - 00540/00547 - Loss: 0.02211. [126 s]\n",
      "Epoch: 45 - loss(trn/val):0.02526/0.29918, acc(val):91.80%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 46 - 00020/00547 - Loss: 0.04612. [  4 s]\n",
      "Epoch: 46 - 00040/00547 - Loss: 0.09534. [  9 s]\n",
      "Epoch: 46 - 00060/00547 - Loss: 0.02054. [ 14 s]\n",
      "Epoch: 46 - 00080/00547 - Loss: 0.02241. [ 18 s]\n",
      "Epoch: 46 - 00100/00547 - Loss: 0.03084. [ 23 s]\n",
      "Epoch: 46 - 00120/00547 - Loss: 0.01854. [ 28 s]\n",
      "Epoch: 46 - 00140/00547 - Loss: 0.02642. [ 32 s]\n",
      "Epoch: 46 - 00160/00547 - Loss: 0.03251. [ 37 s]\n",
      "Epoch: 46 - 00180/00547 - Loss: 0.06096. [ 41 s]\n",
      "Epoch: 46 - 00200/00547 - Loss: 0.02516. [ 46 s]\n",
      "Epoch: 46 - 00220/00547 - Loss: 0.02785. [ 51 s]\n",
      "Epoch: 46 - 00240/00547 - Loss: 0.05156. [ 55 s]\n",
      "Epoch: 46 - 00260/00547 - Loss: 0.02491. [ 60 s]\n",
      "Epoch: 46 - 00280/00547 - Loss: 0.03578. [ 65 s]\n",
      "Epoch: 46 - 00300/00547 - Loss: 0.06654. [ 69 s]\n",
      "Epoch: 46 - 00320/00547 - Loss: 0.02421. [ 74 s]\n",
      "Epoch: 46 - 00340/00547 - Loss: 0.01306. [ 79 s]\n",
      "Epoch: 46 - 00360/00547 - Loss: 0.03816. [ 83 s]\n",
      "Epoch: 46 - 00380/00547 - Loss: 0.01737. [ 88 s]\n",
      "Epoch: 46 - 00400/00547 - Loss: 0.03400. [ 93 s]\n",
      "Epoch: 46 - 00420/00547 - Loss: 0.02484. [ 97 s]\n",
      "Epoch: 46 - 00440/00547 - Loss: 0.03601. [102 s]\n",
      "Epoch: 46 - 00460/00547 - Loss: 0.00784. [107 s]\n",
      "Epoch: 46 - 00480/00547 - Loss: 0.01816. [111 s]\n",
      "Epoch: 46 - 00500/00547 - Loss: 0.01671. [116 s]\n",
      "Epoch: 46 - 00520/00547 - Loss: 0.02682. [121 s]\n",
      "Epoch: 46 - 00540/00547 - Loss: 0.03827. [125 s]\n",
      "Epoch: 46 - loss(trn/val):0.02348/0.26316, acc(val):92.90%, lr=0.00010. [127s] @17 samples/s \n",
      "Epoch: 47 - 00020/00547 - Loss: 0.01845. [  4 s]\n",
      "Epoch: 47 - 00040/00547 - Loss: 0.01937. [  9 s]\n",
      "Epoch: 47 - 00060/00547 - Loss: 0.01697. [ 14 s]\n",
      "Epoch: 47 - 00080/00547 - Loss: 0.01512. [ 18 s]\n",
      "Epoch: 47 - 00100/00547 - Loss: 0.01248. [ 23 s]\n",
      "Epoch: 47 - 00120/00547 - Loss: 0.02858. [ 27 s]\n",
      "Epoch: 47 - 00140/00547 - Loss: 0.02221. [ 32 s]\n",
      "Epoch: 47 - 00160/00547 - Loss: 0.01881. [ 37 s]\n",
      "Epoch: 47 - 00180/00547 - Loss: 0.02851. [ 41 s]\n",
      "Epoch: 47 - 00200/00547 - Loss: 0.01793. [ 46 s]\n",
      "Epoch: 47 - 00220/00547 - Loss: 0.03586. [ 51 s]\n",
      "Epoch: 47 - 00240/00547 - Loss: 0.02787. [ 55 s]\n",
      "Epoch: 47 - 00260/00547 - Loss: 0.01647. [ 60 s]\n",
      "Epoch: 47 - 00280/00547 - Loss: 0.02171. [ 64 s]\n",
      "Epoch: 47 - 00300/00547 - Loss: 0.01219. [ 69 s]\n",
      "Epoch: 47 - 00320/00547 - Loss: 0.03829. [ 74 s]\n",
      "Epoch: 47 - 00340/00547 - Loss: 0.03363. [ 78 s]\n",
      "Epoch: 47 - 00360/00547 - Loss: 0.01482. [ 83 s]\n",
      "Epoch: 47 - 00380/00547 - Loss: 0.01426. [ 88 s]\n",
      "Epoch: 47 - 00400/00547 - Loss: 0.04014. [ 92 s]\n",
      "Epoch: 47 - 00420/00547 - Loss: 0.02005. [ 97 s]\n",
      "Epoch: 47 - 00440/00547 - Loss: 0.03955. [102 s]\n",
      "Epoch: 47 - 00460/00547 - Loss: 0.01390. [106 s]\n",
      "Epoch: 47 - 00480/00547 - Loss: 0.02432. [111 s]\n",
      "Epoch: 47 - 00500/00547 - Loss: 0.03195. [116 s]\n",
      "Epoch: 47 - 00520/00547 - Loss: 0.02316. [120 s]\n",
      "Epoch: 47 - 00540/00547 - Loss: 0.02417. [125 s]\n",
      "Epoch: 47 - loss(trn/val):0.02261/0.33472, acc(val):91.13%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 48 - 00020/00547 - Loss: 0.02021. [  4 s]\n",
      "Epoch: 48 - 00040/00547 - Loss: 0.01670. [  9 s]\n",
      "Epoch: 48 - 00060/00547 - Loss: 0.02031. [ 14 s]\n",
      "Epoch: 48 - 00080/00547 - Loss: 0.02796. [ 18 s]\n",
      "Epoch: 48 - 00100/00547 - Loss: 0.02419. [ 23 s]\n",
      "Epoch: 48 - 00120/00547 - Loss: 0.01799. [ 27 s]\n",
      "Epoch: 48 - 00140/00547 - Loss: 0.03470. [ 32 s]\n",
      "Epoch: 48 - 00160/00547 - Loss: 0.01015. [ 37 s]\n",
      "Epoch: 48 - 00180/00547 - Loss: 0.03322. [ 41 s]\n",
      "Epoch: 48 - 00200/00547 - Loss: 0.02924. [ 46 s]\n",
      "Epoch: 48 - 00220/00547 - Loss: 0.07934. [ 50 s]\n",
      "Epoch: 48 - 00240/00547 - Loss: 0.04272. [ 55 s]\n",
      "Epoch: 48 - 00260/00547 - Loss: 0.02419. [ 60 s]\n",
      "Epoch: 48 - 00280/00547 - Loss: 0.01162. [ 64 s]\n",
      "Epoch: 48 - 00300/00547 - Loss: 0.05168. [ 69 s]\n",
      "Epoch: 48 - 00320/00547 - Loss: 0.04327. [ 74 s]\n",
      "Epoch: 48 - 00340/00547 - Loss: 0.04880. [ 78 s]\n",
      "Epoch: 48 - 00360/00547 - Loss: 0.03006. [ 83 s]\n",
      "Epoch: 48 - 00380/00547 - Loss: 0.03576. [ 88 s]\n",
      "Epoch: 48 - 00400/00547 - Loss: 0.03189. [ 92 s]\n",
      "Epoch: 48 - 00420/00547 - Loss: 0.01926. [ 97 s]\n",
      "Epoch: 48 - 00440/00547 - Loss: 0.02822. [101 s]\n",
      "Epoch: 48 - 00460/00547 - Loss: 0.01295. [106 s]\n",
      "Epoch: 48 - 00480/00547 - Loss: 0.02231. [111 s]\n",
      "Epoch: 48 - 00500/00547 - Loss: 0.03359. [115 s]\n",
      "Epoch: 48 - 00520/00547 - Loss: 0.02580. [120 s]\n",
      "Epoch: 48 - 00540/00547 - Loss: 0.03014. [125 s]\n",
      "Epoch: 48 - loss(trn/val):0.02275/0.43495, acc(val):89.15%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 49 - 00020/00547 - Loss: 0.05373. [  4 s]\n",
      "Epoch: 49 - 00040/00547 - Loss: 0.04155. [  9 s]\n",
      "Epoch: 49 - 00060/00547 - Loss: 0.05093. [ 14 s]\n",
      "Epoch: 49 - 00080/00547 - Loss: 0.04609. [ 18 s]\n",
      "Epoch: 49 - 00100/00547 - Loss: 0.01680. [ 23 s]\n",
      "Epoch: 49 - 00120/00547 - Loss: 0.01888. [ 27 s]\n",
      "Epoch: 49 - 00140/00547 - Loss: 0.03641. [ 32 s]\n",
      "Epoch: 49 - 00160/00547 - Loss: 0.01714. [ 37 s]\n",
      "Epoch: 49 - 00180/00547 - Loss: 0.03081. [ 41 s]\n",
      "Epoch: 49 - 00200/00547 - Loss: 0.03108. [ 46 s]\n",
      "Epoch: 49 - 00220/00547 - Loss: 0.01230. [ 51 s]\n",
      "Epoch: 49 - 00240/00547 - Loss: 0.02064. [ 55 s]\n",
      "Epoch: 49 - 00260/00547 - Loss: 0.02496. [ 60 s]\n",
      "Epoch: 49 - 00280/00547 - Loss: 0.01079. [ 64 s]\n",
      "Epoch: 49 - 00300/00547 - Loss: 0.01879. [ 69 s]\n",
      "Epoch: 49 - 00320/00547 - Loss: 0.01838. [ 74 s]\n",
      "Epoch: 49 - 00340/00547 - Loss: 0.01904. [ 78 s]\n",
      "Epoch: 49 - 00360/00547 - Loss: 0.02172. [ 83 s]\n",
      "Epoch: 49 - 00380/00547 - Loss: 0.02161. [ 88 s]\n",
      "Epoch: 49 - 00400/00547 - Loss: 0.01854. [ 92 s]\n",
      "Epoch: 49 - 00420/00547 - Loss: 0.02369. [ 97 s]\n",
      "Epoch: 49 - 00440/00547 - Loss: 0.03545. [102 s]\n",
      "Epoch: 49 - 00460/00547 - Loss: 0.03439. [106 s]\n",
      "Epoch: 49 - 00480/00547 - Loss: 0.03961. [111 s]\n",
      "Epoch: 49 - 00500/00547 - Loss: 0.03186. [115 s]\n",
      "Epoch: 49 - 00520/00547 - Loss: 0.02164. [120 s]\n",
      "Epoch: 49 - 00540/00547 - Loss: 0.02034. [125 s]\n",
      "Epoch: 49 - loss(trn/val):0.02347/0.38847, acc(val):90.83%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 50 - 00020/00547 - Loss: 0.01908. [  5 s]\n",
      "Epoch: 50 - 00040/00547 - Loss: 0.02683. [  9 s]\n",
      "Epoch: 50 - 00060/00547 - Loss: 0.02132. [ 14 s]\n",
      "Epoch: 50 - 00080/00547 - Loss: 0.01606. [ 18 s]\n",
      "Epoch: 50 - 00100/00547 - Loss: 0.02861. [ 23 s]\n",
      "Epoch: 50 - 00120/00547 - Loss: 0.01560. [ 28 s]\n",
      "Epoch: 50 - 00140/00547 - Loss: 0.01290. [ 32 s]\n",
      "Epoch: 50 - 00160/00547 - Loss: 0.01899. [ 37 s]\n",
      "Epoch: 50 - 00180/00547 - Loss: 0.02014. [ 41 s]\n",
      "Epoch: 50 - 00200/00547 - Loss: 0.01463. [ 46 s]\n",
      "Epoch: 50 - 00220/00547 - Loss: 0.02795. [ 51 s]\n",
      "Epoch: 50 - 00240/00547 - Loss: 0.02498. [ 55 s]\n",
      "Epoch: 50 - 00260/00547 - Loss: 0.02351. [ 60 s]\n",
      "Epoch: 50 - 00280/00547 - Loss: 0.02165. [ 65 s]\n",
      "Epoch: 50 - 00300/00547 - Loss: 0.02412. [ 69 s]\n",
      "Epoch: 50 - 00320/00547 - Loss: 0.03092. [ 74 s]\n",
      "Epoch: 50 - 00340/00547 - Loss: 0.03670. [ 79 s]\n",
      "Epoch: 50 - 00360/00547 - Loss: 0.03496. [ 83 s]\n",
      "Epoch: 50 - 00380/00547 - Loss: 0.02254. [ 88 s]\n",
      "Epoch: 50 - 00400/00547 - Loss: 0.15352. [ 92 s]\n",
      "Epoch: 50 - 00420/00547 - Loss: 0.03943. [ 97 s]\n",
      "Epoch: 50 - 00440/00547 - Loss: 0.02164. [102 s]\n",
      "Epoch: 50 - 00460/00547 - Loss: 0.02619. [106 s]\n",
      "Epoch: 50 - 00480/00547 - Loss: 0.01441. [111 s]\n",
      "Epoch: 50 - 00500/00547 - Loss: 0.01674. [116 s]\n",
      "Epoch: 50 - 00520/00547 - Loss: 0.03059. [120 s]\n",
      "Epoch: 50 - 00540/00547 - Loss: 0.02664. [125 s]\n",
      "Epoch: 50 - loss(trn/val):0.02370/0.28859, acc(val):92.19%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 51 - 00020/00547 - Loss: 0.01521. [  4 s]\n",
      "Epoch: 51 - 00040/00547 - Loss: 0.02228. [  9 s]\n",
      "Epoch: 51 - 00060/00547 - Loss: 0.05119. [ 14 s]\n",
      "Epoch: 51 - 00080/00547 - Loss: 0.02074. [ 18 s]\n",
      "Epoch: 51 - 00100/00547 - Loss: 0.03422. [ 23 s]\n",
      "Epoch: 51 - 00120/00547 - Loss: 0.01557. [ 27 s]\n",
      "Epoch: 51 - 00140/00547 - Loss: 0.01694. [ 32 s]\n",
      "Epoch: 51 - 00160/00547 - Loss: 0.02504. [ 37 s]\n",
      "Epoch: 51 - 00180/00547 - Loss: 0.01912. [ 41 s]\n",
      "Epoch: 51 - 00200/00547 - Loss: 0.01719. [ 46 s]\n",
      "Epoch: 51 - 00220/00547 - Loss: 0.02142. [ 50 s]\n",
      "Epoch: 51 - 00240/00547 - Loss: 0.02293. [ 55 s]\n",
      "Epoch: 51 - 00260/00547 - Loss: 0.01520. [ 60 s]\n",
      "Epoch: 51 - 00280/00547 - Loss: 0.01398. [ 64 s]\n",
      "Epoch: 51 - 00300/00547 - Loss: 0.02525. [ 69 s]\n",
      "Epoch: 51 - 00320/00547 - Loss: 0.02935. [ 74 s]\n",
      "Epoch: 51 - 00340/00547 - Loss: 0.02855. [ 78 s]\n",
      "Epoch: 51 - 00360/00547 - Loss: 0.02675. [ 83 s]\n",
      "Epoch: 51 - 00380/00547 - Loss: 0.01520. [ 87 s]\n",
      "Epoch: 51 - 00400/00547 - Loss: 0.02866. [ 92 s]\n",
      "Epoch: 51 - 00420/00547 - Loss: 0.01831. [ 97 s]\n",
      "Epoch: 51 - 00440/00547 - Loss: 0.01361. [101 s]\n",
      "Epoch: 51 - 00460/00547 - Loss: 0.02964. [106 s]\n",
      "Epoch: 51 - 00480/00547 - Loss: 0.02280. [111 s]\n",
      "Epoch: 51 - 00500/00547 - Loss: 0.01491. [115 s]\n",
      "Epoch: 51 - 00520/00547 - Loss: 0.01401. [120 s]\n",
      "Epoch: 51 - 00540/00547 - Loss: 0.02805. [124 s]\n",
      "Epoch: 51 - loss(trn/val):0.02648/0.39273, acc(val):90.10%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 52 - 00020/00547 - Loss: 0.02790. [  4 s]\n",
      "Epoch: 52 - 00040/00547 - Loss: 0.04186. [  9 s]\n",
      "Epoch: 52 - 00060/00547 - Loss: 0.01556. [ 14 s]\n",
      "Epoch: 52 - 00080/00547 - Loss: 0.01578. [ 18 s]\n",
      "Epoch: 52 - 00100/00547 - Loss: 0.04259. [ 23 s]\n",
      "Epoch: 52 - 00120/00547 - Loss: 0.02218. [ 27 s]\n",
      "Epoch: 52 - 00140/00547 - Loss: 0.08956. [ 32 s]\n",
      "Epoch: 52 - 00160/00547 - Loss: 0.03957. [ 37 s]\n",
      "Epoch: 52 - 00180/00547 - Loss: 0.04327. [ 41 s]\n",
      "Epoch: 52 - 00200/00547 - Loss: 0.02669. [ 46 s]\n",
      "Epoch: 52 - 00220/00547 - Loss: 0.02009. [ 50 s]\n",
      "Epoch: 52 - 00240/00547 - Loss: 0.04655. [ 55 s]\n",
      "Epoch: 52 - 00260/00547 - Loss: 0.06697. [ 60 s]\n",
      "Epoch: 52 - 00280/00547 - Loss: 0.03247. [ 64 s]\n",
      "Epoch: 52 - 00300/00547 - Loss: 0.02914. [ 69 s]\n",
      "Epoch: 52 - 00320/00547 - Loss: 0.01935. [ 74 s]\n",
      "Epoch: 52 - 00340/00547 - Loss: 0.01920. [ 78 s]\n",
      "Epoch: 52 - 00360/00547 - Loss: 0.03282. [ 83 s]\n",
      "Epoch: 52 - 00380/00547 - Loss: 0.02480. [ 88 s]\n",
      "Epoch: 52 - 00400/00547 - Loss: 0.04550. [ 92 s]\n",
      "Epoch: 52 - 00420/00547 - Loss: 0.02956. [ 97 s]\n",
      "Epoch: 52 - 00440/00547 - Loss: 0.02486. [101 s]\n",
      "Epoch: 52 - 00460/00547 - Loss: 0.02248. [106 s]\n",
      "Epoch: 52 - 00480/00547 - Loss: 0.01881. [111 s]\n",
      "Epoch: 52 - 00500/00547 - Loss: 0.01771. [115 s]\n",
      "Epoch: 52 - 00520/00547 - Loss: 0.02834. [120 s]\n",
      "Epoch: 52 - 00540/00547 - Loss: 0.02436. [124 s]\n",
      "Epoch: 52 - loss(trn/val):0.02415/0.26683, acc(val):93.01%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 53 - 00020/00547 - Loss: 0.01918. [  4 s]\n",
      "Epoch: 53 - 00040/00547 - Loss: 0.03014. [  9 s]\n",
      "Epoch: 53 - 00060/00547 - Loss: 0.01921. [ 14 s]\n",
      "Epoch: 53 - 00080/00547 - Loss: 0.01112. [ 18 s]\n",
      "Epoch: 53 - 00100/00547 - Loss: 0.02480. [ 23 s]\n",
      "Epoch: 53 - 00120/00547 - Loss: 0.01725. [ 27 s]\n",
      "Epoch: 53 - 00140/00547 - Loss: 0.02846. [ 32 s]\n",
      "Epoch: 53 - 00160/00547 - Loss: 0.01753. [ 37 s]\n",
      "Epoch: 53 - 00180/00547 - Loss: 0.01895. [ 41 s]\n",
      "Epoch: 53 - 00200/00547 - Loss: 0.02488. [ 46 s]\n",
      "Epoch: 53 - 00220/00547 - Loss: 0.01308. [ 50 s]\n",
      "Epoch: 53 - 00240/00547 - Loss: 0.01294. [ 55 s]\n",
      "Epoch: 53 - 00260/00547 - Loss: 0.01643. [ 60 s]\n",
      "Epoch: 53 - 00280/00547 - Loss: 0.00986. [ 64 s]\n",
      "Epoch: 53 - 00300/00547 - Loss: 0.03161. [ 69 s]\n",
      "Epoch: 53 - 00320/00547 - Loss: 0.02006. [ 73 s]\n",
      "Epoch: 53 - 00340/00547 - Loss: 0.02129. [ 78 s]\n",
      "Epoch: 53 - 00360/00547 - Loss: 0.01883. [ 83 s]\n",
      "Epoch: 53 - 00380/00547 - Loss: 0.02230. [ 87 s]\n",
      "Epoch: 53 - 00400/00547 - Loss: 0.01658. [ 92 s]\n",
      "Epoch: 53 - 00420/00547 - Loss: 0.01327. [ 97 s]\n",
      "Epoch: 53 - 00440/00547 - Loss: 0.01269. [101 s]\n",
      "Epoch: 53 - 00460/00547 - Loss: 0.01258. [106 s]\n",
      "Epoch: 53 - 00480/00547 - Loss: 0.03215. [111 s]\n",
      "Epoch: 53 - 00500/00547 - Loss: 0.01554. [115 s]\n",
      "Epoch: 53 - 00520/00547 - Loss: 0.01714. [120 s]\n",
      "Epoch: 53 - 00540/00547 - Loss: 0.03503. [124 s]\n",
      "Epoch: 53 - loss(trn/val):0.03928/0.22803, acc(val):92.12%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 54 - 00020/00547 - Loss: 0.02727. [  5 s]\n",
      "Epoch: 54 - 00040/00547 - Loss: 0.02228. [  9 s]\n",
      "Epoch: 54 - 00060/00547 - Loss: 0.02225. [ 14 s]\n",
      "Epoch: 54 - 00080/00547 - Loss: 0.02667. [ 18 s]\n",
      "Epoch: 54 - 00100/00547 - Loss: 0.01987. [ 23 s]\n",
      "Epoch: 54 - 00120/00547 - Loss: 0.06279. [ 27 s]\n",
      "Epoch: 54 - 00140/00547 - Loss: 0.01850. [ 32 s]\n",
      "Epoch: 54 - 00160/00547 - Loss: 0.02442. [ 37 s]\n",
      "Epoch: 54 - 00180/00547 - Loss: 0.02236. [ 41 s]\n",
      "Epoch: 54 - 00200/00547 - Loss: 0.01473. [ 46 s]\n",
      "Epoch: 54 - 00220/00547 - Loss: 0.01766. [ 51 s]\n",
      "Epoch: 54 - 00240/00547 - Loss: 0.01908. [ 55 s]\n",
      "Epoch: 54 - 00260/00547 - Loss: 0.01705. [ 60 s]\n",
      "Epoch: 54 - 00280/00547 - Loss: 0.02250. [ 64 s]\n",
      "Epoch: 54 - 00300/00547 - Loss: 0.02479. [ 69 s]\n",
      "Epoch: 54 - 00320/00547 - Loss: 0.02165. [ 74 s]\n",
      "Epoch: 54 - 00340/00547 - Loss: 0.01685. [ 78 s]\n",
      "Epoch: 54 - 00360/00547 - Loss: 0.02738. [ 83 s]\n",
      "Epoch: 54 - 00380/00547 - Loss: 0.03269. [ 88 s]\n",
      "Epoch: 54 - 00400/00547 - Loss: 0.02000. [ 92 s]\n",
      "Epoch: 54 - 00420/00547 - Loss: 0.01543. [ 97 s]\n",
      "Epoch: 54 - 00440/00547 - Loss: 0.01809. [102 s]\n",
      "Epoch: 54 - 00460/00547 - Loss: 0.02371. [106 s]\n",
      "Epoch: 54 - 00480/00547 - Loss: 0.01329. [111 s]\n",
      "Epoch: 54 - 00500/00547 - Loss: 0.01646. [115 s]\n",
      "Epoch: 54 - 00520/00547 - Loss: 0.01934. [120 s]\n",
      "Epoch: 54 - 00540/00547 - Loss: 0.01929. [125 s]\n",
      "Epoch: 54 - loss(trn/val):0.02006/0.27574, acc(val):92.89%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 55 - 00020/00547 - Loss: 0.01916. [  4 s]\n",
      "Epoch: 55 - 00040/00547 - Loss: 0.02174. [  9 s]\n",
      "Epoch: 55 - 00060/00547 - Loss: 0.00931. [ 14 s]\n",
      "Epoch: 55 - 00080/00547 - Loss: 0.02075. [ 18 s]\n",
      "Epoch: 55 - 00100/00547 - Loss: 0.00967. [ 23 s]\n",
      "Epoch: 55 - 00120/00547 - Loss: 0.02022. [ 27 s]\n",
      "Epoch: 55 - 00140/00547 - Loss: 0.01908. [ 32 s]\n",
      "Epoch: 55 - 00160/00547 - Loss: 0.01547. [ 37 s]\n",
      "Epoch: 55 - 00180/00547 - Loss: 0.02157. [ 41 s]\n",
      "Epoch: 55 - 00200/00547 - Loss: 0.03091. [ 46 s]\n",
      "Epoch: 55 - 00220/00547 - Loss: 0.03058. [ 51 s]\n",
      "Epoch: 55 - 00240/00547 - Loss: 0.00999. [ 55 s]\n",
      "Epoch: 55 - 00260/00547 - Loss: 0.01983. [ 60 s]\n",
      "Epoch: 55 - 00280/00547 - Loss: 0.01600. [ 64 s]\n",
      "Epoch: 55 - 00300/00547 - Loss: 0.02907. [ 69 s]\n",
      "Epoch: 55 - 00320/00547 - Loss: 0.05777. [ 74 s]\n",
      "Epoch: 55 - 00340/00547 - Loss: 0.01713. [ 78 s]\n",
      "Epoch: 55 - 00360/00547 - Loss: 0.02560. [ 83 s]\n",
      "Epoch: 55 - 00380/00547 - Loss: 0.02989. [ 88 s]\n",
      "Epoch: 55 - 00400/00547 - Loss: 0.02567. [ 92 s]\n",
      "Epoch: 55 - 00420/00547 - Loss: 0.01239. [ 97 s]\n",
      "Epoch: 55 - 00440/00547 - Loss: 0.01297. [102 s]\n",
      "Epoch: 55 - 00460/00547 - Loss: 0.01954. [106 s]\n",
      "Epoch: 55 - 00480/00547 - Loss: 0.02575. [111 s]\n",
      "Epoch: 55 - 00500/00547 - Loss: 0.01543. [115 s]\n",
      "Epoch: 55 - 00520/00547 - Loss: 0.05703. [120 s]\n",
      "Epoch: 55 - 00540/00547 - Loss: 0.03677. [125 s]\n",
      "Epoch: 55 - loss(trn/val):0.02015/0.30651, acc(val):92.64%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 56 - 00020/00547 - Loss: 0.02494. [  4 s]\n",
      "Epoch: 56 - 00040/00547 - Loss: 0.01228. [  9 s]\n",
      "Epoch: 56 - 00060/00547 - Loss: 0.02129. [ 14 s]\n",
      "Epoch: 56 - 00080/00547 - Loss: 0.01360. [ 18 s]\n",
      "Epoch: 56 - 00100/00547 - Loss: 0.03732. [ 23 s]\n",
      "Epoch: 56 - 00120/00547 - Loss: 0.02172. [ 27 s]\n",
      "Epoch: 56 - 00140/00547 - Loss: 0.02897. [ 32 s]\n",
      "Epoch: 56 - 00160/00547 - Loss: 0.02198. [ 37 s]\n",
      "Epoch: 56 - 00180/00547 - Loss: 0.02131. [ 41 s]\n",
      "Epoch: 56 - 00200/00547 - Loss: 0.03477. [ 46 s]\n",
      "Epoch: 56 - 00220/00547 - Loss: 0.02216. [ 50 s]\n",
      "Epoch: 56 - 00240/00547 - Loss: 0.02408. [ 55 s]\n",
      "Epoch: 56 - 00260/00547 - Loss: 0.02259. [ 60 s]\n",
      "Epoch: 56 - 00280/00547 - Loss: 0.01118. [ 64 s]\n",
      "Epoch: 56 - 00300/00547 - Loss: 0.01514. [ 69 s]\n",
      "Epoch: 56 - 00320/00547 - Loss: 0.02475. [ 74 s]\n",
      "Epoch: 56 - 00340/00547 - Loss: 0.02694. [ 78 s]\n",
      "Epoch: 56 - 00360/00547 - Loss: 0.01351. [ 83 s]\n",
      "Epoch: 56 - 00380/00547 - Loss: 0.01942. [ 88 s]\n",
      "Epoch: 56 - 00400/00547 - Loss: 0.04739. [ 92 s]\n",
      "Epoch: 56 - 00420/00547 - Loss: 0.02344. [ 97 s]\n",
      "Epoch: 56 - 00440/00547 - Loss: 0.01800. [101 s]\n",
      "Epoch: 56 - 00460/00547 - Loss: 0.01233. [106 s]\n",
      "Epoch: 56 - 00480/00547 - Loss: 0.01171. [111 s]\n",
      "Epoch: 56 - 00500/00547 - Loss: 0.03093. [115 s]\n",
      "Epoch: 56 - 00520/00547 - Loss: 0.05622. [120 s]\n",
      "Epoch: 56 - 00540/00547 - Loss: 0.04110. [125 s]\n",
      "Epoch: 56 - loss(trn/val):0.02060/0.43360, acc(val):89.84%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 57 - 00020/00547 - Loss: 0.01744. [  4 s]\n",
      "Epoch: 57 - 00040/00547 - Loss: 0.01973. [  9 s]\n",
      "Epoch: 57 - 00060/00547 - Loss: 0.01937. [ 14 s]\n",
      "Epoch: 57 - 00080/00547 - Loss: 0.02222. [ 18 s]\n",
      "Epoch: 57 - 00100/00547 - Loss: 0.02009. [ 23 s]\n",
      "Epoch: 57 - 00120/00547 - Loss: 0.01517. [ 27 s]\n",
      "Epoch: 57 - 00140/00547 - Loss: 0.00922. [ 32 s]\n",
      "Epoch: 57 - 00160/00547 - Loss: 0.01491. [ 36 s]\n",
      "Epoch: 57 - 00180/00547 - Loss: 0.01382. [ 41 s]\n",
      "Epoch: 57 - 00200/00547 - Loss: 0.01296. [ 46 s]\n",
      "Epoch: 57 - 00220/00547 - Loss: 0.01777. [ 50 s]\n",
      "Epoch: 57 - 00240/00547 - Loss: 0.02281. [ 55 s]\n",
      "Epoch: 57 - 00260/00547 - Loss: 0.02114. [ 60 s]\n",
      "Epoch: 57 - 00280/00547 - Loss: 0.01915. [ 64 s]\n",
      "Epoch: 57 - 00300/00547 - Loss: 0.01800. [ 69 s]\n",
      "Epoch: 57 - 00320/00547 - Loss: 0.01609. [ 73 s]\n",
      "Epoch: 57 - 00340/00547 - Loss: 0.01346. [ 78 s]\n",
      "Epoch: 57 - 00360/00547 - Loss: 0.02040. [ 83 s]\n",
      "Epoch: 57 - 00380/00547 - Loss: 0.01844. [ 87 s]\n",
      "Epoch: 57 - 00400/00547 - Loss: 0.04423. [ 92 s]\n",
      "Epoch: 57 - 00420/00547 - Loss: 0.04452. [ 97 s]\n",
      "Epoch: 57 - 00440/00547 - Loss: 0.02199. [101 s]\n",
      "Epoch: 57 - 00460/00547 - Loss: 0.07386. [106 s]\n",
      "Epoch: 57 - 00480/00547 - Loss: 0.02587. [110 s]\n",
      "Epoch: 57 - 00500/00547 - Loss: 0.01655. [115 s]\n",
      "Epoch: 57 - 00520/00547 - Loss: 0.01515. [120 s]\n",
      "Epoch: 57 - 00540/00547 - Loss: 0.03025. [124 s]\n",
      "Epoch: 57 - loss(trn/val):0.02172/0.28070, acc(val):92.69%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 58 - 00020/00547 - Loss: 0.02298. [  4 s]\n",
      "Epoch: 58 - 00040/00547 - Loss: 0.04053. [  9 s]\n",
      "Epoch: 58 - 00060/00547 - Loss: 0.02336. [ 14 s]\n",
      "Epoch: 58 - 00080/00547 - Loss: 0.02416. [ 18 s]\n",
      "Epoch: 58 - 00100/00547 - Loss: 0.03398. [ 23 s]\n",
      "Epoch: 58 - 00120/00547 - Loss: 0.03348. [ 27 s]\n",
      "Epoch: 58 - 00140/00547 - Loss: 0.02493. [ 32 s]\n",
      "Epoch: 58 - 00160/00547 - Loss: 0.02108. [ 37 s]\n",
      "Epoch: 58 - 00180/00547 - Loss: 0.02397. [ 41 s]\n",
      "Epoch: 58 - 00200/00547 - Loss: 0.01588. [ 46 s]\n",
      "Epoch: 58 - 00220/00547 - Loss: 0.01952. [ 51 s]\n",
      "Epoch: 58 - 00240/00547 - Loss: 0.01811. [ 55 s]\n",
      "Epoch: 58 - 00260/00547 - Loss: 0.03964. [ 60 s]\n",
      "Epoch: 58 - 00280/00547 - Loss: 0.01787. [ 64 s]\n",
      "Epoch: 58 - 00300/00547 - Loss: 0.01683. [ 69 s]\n",
      "Epoch: 58 - 00320/00547 - Loss: 0.01850. [ 74 s]\n",
      "Epoch: 58 - 00340/00547 - Loss: 0.02829. [ 78 s]\n",
      "Epoch: 58 - 00360/00547 - Loss: 0.02442. [ 83 s]\n",
      "Epoch: 58 - 00380/00547 - Loss: 0.03330. [ 87 s]\n",
      "Epoch: 58 - 00400/00547 - Loss: 0.02014. [ 92 s]\n",
      "Epoch: 58 - 00420/00547 - Loss: 0.01628. [ 97 s]\n",
      "Epoch: 58 - 00440/00547 - Loss: 0.01995. [101 s]\n",
      "Epoch: 58 - 00460/00547 - Loss: 0.01762. [106 s]\n",
      "Epoch: 58 - 00480/00547 - Loss: 0.03342. [111 s]\n",
      "Epoch: 58 - 00500/00547 - Loss: 0.01611. [115 s]\n",
      "Epoch: 58 - 00520/00547 - Loss: 0.02602. [120 s]\n",
      "Epoch: 58 - 00540/00547 - Loss: 0.04019. [124 s]\n",
      "Epoch: 58 - loss(trn/val):0.02392/0.29636, acc(val):91.55%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 59 - 00020/00547 - Loss: 0.02335. [  4 s]\n",
      "Epoch: 59 - 00040/00547 - Loss: 0.02780. [  9 s]\n",
      "Epoch: 59 - 00060/00547 - Loss: 0.04949. [ 14 s]\n",
      "Epoch: 59 - 00080/00547 - Loss: 0.02071. [ 18 s]\n",
      "Epoch: 59 - 00100/00547 - Loss: 0.02290. [ 23 s]\n",
      "Epoch: 59 - 00120/00547 - Loss: 0.03056. [ 27 s]\n",
      "Epoch: 59 - 00140/00547 - Loss: 0.01832. [ 32 s]\n",
      "Epoch: 59 - 00160/00547 - Loss: 0.01530. [ 37 s]\n",
      "Epoch: 59 - 00180/00547 - Loss: 0.03061. [ 41 s]\n",
      "Epoch: 59 - 00200/00547 - Loss: 0.01791. [ 46 s]\n",
      "Epoch: 59 - 00220/00547 - Loss: 0.01428. [ 50 s]\n",
      "Epoch: 59 - 00240/00547 - Loss: 0.02286. [ 55 s]\n",
      "Epoch: 59 - 00260/00547 - Loss: 0.02594. [ 60 s]\n",
      "Epoch: 59 - 00280/00547 - Loss: 0.02644. [ 64 s]\n",
      "Epoch: 59 - 00300/00547 - Loss: 0.01534. [ 69 s]\n",
      "Epoch: 59 - 00320/00547 - Loss: 0.01737. [ 74 s]\n",
      "Epoch: 59 - 00340/00547 - Loss: 0.02150. [ 78 s]\n",
      "Epoch: 59 - 00360/00547 - Loss: 0.00913. [ 83 s]\n",
      "Epoch: 59 - 00380/00547 - Loss: 0.02948. [ 87 s]\n",
      "Epoch: 59 - 00400/00547 - Loss: 0.01675. [ 92 s]\n",
      "Epoch: 59 - 00420/00547 - Loss: 0.02163. [ 97 s]\n",
      "Epoch: 59 - 00440/00547 - Loss: 0.02092. [101 s]\n",
      "Epoch: 59 - 00460/00547 - Loss: 0.02189. [106 s]\n",
      "Epoch: 59 - 00480/00547 - Loss: 0.01874. [111 s]\n",
      "Epoch: 59 - 00500/00547 - Loss: 0.02115. [115 s]\n",
      "Epoch: 59 - 00520/00547 - Loss: 0.01556. [120 s]\n",
      "Epoch: 59 - 00540/00547 - Loss: 0.01402. [124 s]\n",
      "Epoch: 59 - loss(trn/val):0.01829/0.35528, acc(val):91.97%, lr=0.00010. [126s] @17 samples/s \n",
      "Epoch: 60 - 00020/00547 - Loss: 0.01167. [  5 s]\n",
      "Epoch: 60 - 00040/00547 - Loss: 0.01756. [  9 s]\n",
      "Epoch: 60 - 00060/00547 - Loss: 0.01440. [ 14 s]\n",
      "Epoch: 60 - 00080/00547 - Loss: 0.01709. [ 18 s]\n",
      "Epoch: 60 - 00100/00547 - Loss: 0.01344. [ 23 s]\n",
      "Epoch: 60 - 00120/00547 - Loss: 0.01135. [ 27 s]\n",
      "Epoch: 60 - 00140/00547 - Loss: 0.01399. [ 32 s]\n",
      "Epoch: 60 - 00160/00547 - Loss: 0.02029. [ 37 s]\n",
      "Epoch: 60 - 00180/00547 - Loss: 0.03354. [ 41 s]\n",
      "Epoch: 60 - 00200/00547 - Loss: 0.05067. [ 46 s]\n",
      "Epoch: 60 - 00220/00547 - Loss: 0.02132. [ 50 s]\n",
      "Epoch: 60 - 00240/00547 - Loss: 0.01960. [ 55 s]\n",
      "Epoch: 60 - 00260/00547 - Loss: 0.01104. [ 60 s]\n",
      "Epoch: 60 - 00280/00547 - Loss: 0.01712. [ 64 s]\n",
      "Epoch: 60 - 00300/00547 - Loss: 0.01753. [ 69 s]\n",
      "Epoch: 60 - 00320/00547 - Loss: 0.01851. [ 74 s]\n",
      "Epoch: 60 - 00340/00547 - Loss: 0.02259. [ 78 s]\n",
      "Epoch: 60 - 00360/00547 - Loss: 0.02200. [ 83 s]\n",
      "Epoch: 60 - 00380/00547 - Loss: 0.01841. [ 87 s]\n",
      "Epoch: 60 - 00400/00547 - Loss: 0.02371. [ 92 s]\n",
      "Epoch: 60 - 00420/00547 - Loss: 0.02427. [ 97 s]\n",
      "Epoch: 60 - 00440/00547 - Loss: 0.03289. [101 s]\n",
      "Epoch: 60 - 00460/00547 - Loss: 0.02568. [106 s]\n",
      "Epoch: 60 - 00480/00547 - Loss: 0.02164. [111 s]\n",
      "Epoch: 60 - 00500/00547 - Loss: 0.03020. [115 s]\n",
      "Epoch: 60 - 00520/00547 - Loss: 0.01190. [120 s]\n",
      "Epoch: 60 - 00540/00547 - Loss: 0.06493. [125 s]\n",
      "Epoch: 60 - loss(trn/val):0.02483/0.46809, acc(val):88.59%, lr=0.00010. [126s] @17 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.1825 \n",
      "iou=0.7993 \n",
      "acc=0.9310 \n",
      "sensitivity=0.8541 \n",
      "specificity=0.9606 \n",
      "precision=0.9152 \n",
      "f1=0.8784\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ef5c3",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9ea0abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.182494 at epoch 17\n",
      "Minimum training loss: 0.018289 at epoch 59\n",
      "Maximum validation IoU: 0.799272 at epoch 17\n",
      "Maximum training IoU: 0.966539 at epoch 59\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_Enc_Dec_Misc_RMS_lr_e-4\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba47774",
   "metadata": {},
   "source": [
    "# Training on \"Tampere\" and \"Misc\" Dataset\n",
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e1b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "identifier = 'ResNet34_Enc_Dec_Combined_RMS_lr_e-4_nopretrain'\n",
    "directory = \"../data/training_states/ResNet34_Enc_Dec\"\n",
    "path = os.path.join(directory,identifier)\n",
    "epochs = 60\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d709e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "resnet34 = models.resnet34(pretrained=True)\n",
    "res_model = ResNet(resnet34)\n",
    "model = FCN8s(res_model, 1).to(device)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "logger = SegmentationLogger([\"epoch\", \"loss\", \"lr\", \"accuracy\", \"iou\", \"sensitivity\", \"specificity\", \"precision\", \"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a72faac",
   "metadata": {},
   "source": [
    "### Dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6a0111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2588 images in training dataset\n",
      "412 images in validation dataset\n"
     ]
    }
   ],
   "source": [
    "# Get precalculated mean and standard deviation\n",
    "mean, std = dataset_statistics.TAMP_OPEN_DOCK_MISC_TRN\n",
    "\n",
    "# Transformation to normalize and unnormalize input images\n",
    "norm = transforms.Normalize(mean, std)\n",
    "inv_norm = transforms.Normalize(\n",
    "   mean= [-m/s for m, s in zip(mean, std)],\n",
    "   std= [1/s for s in std])\n",
    "\n",
    "dataset = Water('../data/WaterDataset', data_list_tamp=[\"open\", \"dock\"], data_list_misc=['training'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "dataset_val = Water('../data/WaterDataset', data_list_tamp=[\"channel\"], data_list_misc=['validation'],\n",
    "                    data_constance = False, transforms=norm, img_size=(960,640))\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "        dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f'{len(dataset)} images in training dataset')\n",
    "print(f'{len(dataset_val)} images in validation dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4836d",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eae494b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[System]\n",
      "Host                dennis-ios\n",
      "Platform            Linux-4.15.0-204-generic-x86_64-with-glibc2.17\n",
      "CUDA                10.2\n",
      "CuDNN               7605\n",
      "Python              ['3.8.13 (default, Mar 28 2022, 11:38:47) ', '[GCC 7.5.0]']\n",
      "Numpy               1.21.5\n",
      "Torch               1.11.0\n",
      "Torchvision         0.12.0\n",
      "ummon               3.8.0\n",
      " \n",
      " \n",
      "[Trainer]\n",
      "utils.segmentation_trainer.SegmentationTrainer\n",
      " \n",
      "[Model]\n",
      "FCN8s(\n",
      "  (pretrained_net): ResNet(\n",
      "    (resnet): ResNet(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      "    )\n",
      "    (intermediate): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (5): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (deconv1): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv3): ConvTranspose2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (deconv5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (bn5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (classifier): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "Trainable params:   23513705\n",
      " \n",
      "[Loss]\n",
      "BCEWithLogitsLoss()\n",
      " \n",
      "[Data]\n",
      "Training              2588    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.1 mean:-0.0 std:1.0 / Labels:min:0.0 max:1.0 mean:0.3 std:0.5\n",
      "Validation             412    \n",
      "\tShape IN:(3, 640, 960) / TARGET:(1, 640, 960) \n",
      "\tType  IN:float32 / TARGET:float32 \n",
      "\tStats Data:min:-1.9 max:2.1 mean:-0.1 std:1.0 / Labels:min:0.0 max:1.0 mean:0.5 std:0.5\n",
      " \n",
      "[Parameters]\n",
      "lrate               1.00e-04\n",
      "batch_size          4\n",
      "epochs              60\n",
      "combined_retraining 0\n",
      "using_cuda          True\n",
      "early_stopping      False\n",
      "precision           float32\n",
      "optimizer           RMSprop\n",
      "   optimizer-param  ParameterGroup0\n",
      "   optimizer-param  alpha:0.99\n",
      "   optimizer-param  centered:False\n",
      "   optimizer-param  eps:1e-08\n",
      "   optimizer-param  lr:0.0001\n",
      "   optimizer-param  momentum:0\n",
      "   optimizer-param  weight_decay:1e-05\n",
      "\n",
      "Begin training: 60 epochs.\n",
      "Epoch: 1 - 00020/00647 - Loss: 0.61890. [  4 s]\n",
      "Epoch: 1 - 00040/00647 - Loss: 0.87353. [  9 s]\n",
      "Epoch: 1 - 00060/00647 - Loss: 0.69305. [ 14 s]\n",
      "Epoch: 1 - 00080/00647 - Loss: 0.28261. [ 18 s]\n",
      "Epoch: 1 - 00100/00647 - Loss: 0.43455. [ 23 s]\n",
      "Epoch: 1 - 00120/00647 - Loss: 0.66089. [ 27 s]\n",
      "Epoch: 1 - 00140/00647 - Loss: 0.46246. [ 32 s]\n",
      "Epoch: 1 - 00160/00647 - Loss: 0.31382. [ 37 s]\n",
      "Epoch: 1 - 00180/00647 - Loss: 0.39419. [ 41 s]\n",
      "Epoch: 1 - 00200/00647 - Loss: 0.36867. [ 46 s]\n",
      "Epoch: 1 - 00220/00647 - Loss: 0.45810. [ 50 s]\n",
      "Epoch: 1 - 00240/00647 - Loss: 0.30115. [ 55 s]\n",
      "Epoch: 1 - 00260/00647 - Loss: 0.47538. [ 60 s]\n",
      "Epoch: 1 - 00280/00647 - Loss: 0.62517. [ 64 s]\n",
      "Epoch: 1 - 00300/00647 - Loss: 0.35134. [ 69 s]\n",
      "Epoch: 1 - 00320/00647 - Loss: 0.47233. [ 74 s]\n",
      "Epoch: 1 - 00340/00647 - Loss: 0.20409. [ 78 s]\n",
      "Epoch: 1 - 00360/00647 - Loss: 0.59072. [ 83 s]\n",
      "Epoch: 1 - 00380/00647 - Loss: 0.29668. [ 88 s]\n",
      "Epoch: 1 - 00400/00647 - Loss: 0.17740. [ 92 s]\n",
      "Epoch: 1 - 00420/00647 - Loss: 0.47919. [ 97 s]\n",
      "Epoch: 1 - 00440/00647 - Loss: 0.41155. [101 s]\n",
      "Epoch: 1 - 00460/00647 - Loss: 0.19457. [106 s]\n",
      "Epoch: 1 - 00480/00647 - Loss: 0.23191. [111 s]\n",
      "Epoch: 1 - 00500/00647 - Loss: 0.21987. [115 s]\n",
      "Epoch: 1 - 00520/00647 - Loss: 0.39259. [120 s]\n",
      "Epoch: 1 - 00540/00647 - Loss: 0.31950. [125 s]\n",
      "Epoch: 1 - 00560/00647 - Loss: 0.17336. [129 s]\n",
      "Epoch: 1 - 00580/00647 - Loss: 0.26899. [134 s]\n",
      "Epoch: 1 - 00600/00647 - Loss: 0.22580. [139 s]\n",
      "Epoch: 1 - 00620/00647 - Loss: 0.14157. [143 s]\n",
      "Epoch: 1 - 00640/00647 - Loss: 0.27239. [148 s]\n",
      "Epoch: 1 - loss(trn/val):0.24130/0.19598, acc(val):93.46%, lr=0.00010 [BEST]. [149s] @17 samples/s \n",
      "Epoch: 2 - 00020/00647 - Loss: 0.21350. [  5 s]\n",
      "Epoch: 2 - 00040/00647 - Loss: 0.18366. [  9 s]\n",
      "Epoch: 2 - 00060/00647 - Loss: 0.23912. [ 14 s]\n",
      "Epoch: 2 - 00080/00647 - Loss: 0.21830. [ 18 s]\n",
      "Epoch: 2 - 00100/00647 - Loss: 0.24832. [ 23 s]\n",
      "Epoch: 2 - 00120/00647 - Loss: 0.39408. [ 27 s]\n",
      "Epoch: 2 - 00140/00647 - Loss: 0.18368. [ 32 s]\n",
      "Epoch: 2 - 00160/00647 - Loss: 0.35305. [ 37 s]\n",
      "Epoch: 2 - 00180/00647 - Loss: 0.18869. [ 41 s]\n",
      "Epoch: 2 - 00200/00647 - Loss: 0.21681. [ 46 s]\n",
      "Epoch: 2 - 00220/00647 - Loss: 0.83491. [ 50 s]\n",
      "Epoch: 2 - 00240/00647 - Loss: 0.16956. [ 55 s]\n",
      "Epoch: 2 - 00260/00647 - Loss: 0.20196. [ 60 s]\n",
      "Epoch: 2 - 00280/00647 - Loss: 0.27970. [ 64 s]\n",
      "Epoch: 2 - 00300/00647 - Loss: 0.25219. [ 69 s]\n",
      "Epoch: 2 - 00320/00647 - Loss: 0.19287. [ 74 s]\n",
      "Epoch: 2 - 00340/00647 - Loss: 0.45386. [ 78 s]\n",
      "Epoch: 2 - 00360/00647 - Loss: 0.17149. [ 83 s]\n",
      "Epoch: 2 - 00380/00647 - Loss: 0.33856. [ 88 s]\n",
      "Epoch: 2 - 00400/00647 - Loss: 0.42681. [ 92 s]\n",
      "Epoch: 2 - 00420/00647 - Loss: 0.56323. [ 97 s]\n",
      "Epoch: 2 - 00440/00647 - Loss: 0.42562. [101 s]\n",
      "Epoch: 2 - 00460/00647 - Loss: 0.22818. [106 s]\n",
      "Epoch: 2 - 00480/00647 - Loss: 0.18169. [111 s]\n",
      "Epoch: 2 - 00500/00647 - Loss: 0.24866. [115 s]\n",
      "Epoch: 2 - 00520/00647 - Loss: 0.19813. [120 s]\n",
      "Epoch: 2 - 00540/00647 - Loss: 0.13386. [125 s]\n",
      "Epoch: 2 - 00560/00647 - Loss: 0.40276. [129 s]\n",
      "Epoch: 2 - 00580/00647 - Loss: 0.16717. [134 s]\n",
      "Epoch: 2 - 00600/00647 - Loss: 0.17784. [139 s]\n",
      "Epoch: 2 - 00620/00647 - Loss: 0.51416. [143 s]\n",
      "Epoch: 2 - 00640/00647 - Loss: 0.08133. [148 s]\n",
      "Epoch: 2 - loss(trn/val):0.22273/0.18086, acc(val):93.43%, lr=0.00010 [BEST]. [149s] @17 samples/s \n",
      "Epoch: 3 - 00020/00647 - Loss: 0.20936. [  5 s]\n",
      "Epoch: 3 - 00040/00647 - Loss: 0.08623. [  9 s]\n",
      "Epoch: 3 - 00060/00647 - Loss: 0.08312. [ 14 s]\n",
      "Epoch: 3 - 00080/00647 - Loss: 0.19523. [ 18 s]\n",
      "Epoch: 3 - 00100/00647 - Loss: 0.10983. [ 23 s]\n",
      "Epoch: 3 - 00120/00647 - Loss: 0.18416. [ 28 s]\n",
      "Epoch: 3 - 00140/00647 - Loss: 0.20149. [ 32 s]\n",
      "Epoch: 3 - 00160/00647 - Loss: 0.24435. [ 37 s]\n",
      "Epoch: 3 - 00180/00647 - Loss: 0.16048. [ 41 s]\n",
      "Epoch: 3 - 00200/00647 - Loss: 0.11337. [ 46 s]\n",
      "Epoch: 3 - 00220/00647 - Loss: 0.18702. [ 51 s]\n",
      "Epoch: 3 - 00240/00647 - Loss: 0.11187. [ 55 s]\n",
      "Epoch: 3 - 00260/00647 - Loss: 0.24008. [ 60 s]\n",
      "Epoch: 3 - 00280/00647 - Loss: 0.17223. [ 64 s]\n",
      "Epoch: 3 - 00300/00647 - Loss: 0.03955. [ 69 s]\n",
      "Epoch: 3 - 00320/00647 - Loss: 0.35980. [ 74 s]\n",
      "Epoch: 3 - 00340/00647 - Loss: 0.36067. [ 78 s]\n",
      "Epoch: 3 - 00360/00647 - Loss: 0.06507. [ 83 s]\n",
      "Epoch: 3 - 00380/00647 - Loss: 0.28746. [ 88 s]\n",
      "Epoch: 3 - 00400/00647 - Loss: 0.13741. [ 92 s]\n",
      "Epoch: 3 - 00420/00647 - Loss: 0.15674. [ 97 s]\n",
      "Epoch: 3 - 00440/00647 - Loss: 0.11084. [102 s]\n",
      "Epoch: 3 - 00460/00647 - Loss: 0.25230. [106 s]\n",
      "Epoch: 3 - 00480/00647 - Loss: 0.30711. [111 s]\n",
      "Epoch: 3 - 00500/00647 - Loss: 0.18938. [115 s]\n",
      "Epoch: 3 - 00520/00647 - Loss: 0.15258. [120 s]\n",
      "Epoch: 3 - 00540/00647 - Loss: 0.10744. [125 s]\n",
      "Epoch: 3 - 00560/00647 - Loss: 0.16251. [129 s]\n",
      "Epoch: 3 - 00580/00647 - Loss: 0.22000. [134 s]\n",
      "Epoch: 3 - 00600/00647 - Loss: 0.41990. [139 s]\n",
      "Epoch: 3 - 00620/00647 - Loss: 0.23127. [143 s]\n",
      "Epoch: 3 - 00640/00647 - Loss: 0.58824. [148 s]\n",
      "Epoch: 3 - loss(trn/val):0.17636/0.17044, acc(val):93.28%, lr=0.00010 [BEST]. [150s] @17 samples/s \n",
      "Epoch: 4 - 00020/00647 - Loss: 0.20366. [  5 s]\n",
      "Epoch: 4 - 00040/00647 - Loss: 0.22230. [  9 s]\n",
      "Epoch: 4 - 00060/00647 - Loss: 0.21520. [ 14 s]\n",
      "Epoch: 4 - 00080/00647 - Loss: 0.15687. [ 18 s]\n",
      "Epoch: 4 - 00100/00647 - Loss: 0.11468. [ 23 s]\n",
      "Epoch: 4 - 00120/00647 - Loss: 0.15286. [ 27 s]\n",
      "Epoch: 4 - 00140/00647 - Loss: 0.18200. [ 32 s]\n",
      "Epoch: 4 - 00160/00647 - Loss: 0.09145. [ 37 s]\n",
      "Epoch: 4 - 00180/00647 - Loss: 0.48595. [ 41 s]\n",
      "Epoch: 4 - 00200/00647 - Loss: 0.12187. [ 46 s]\n",
      "Epoch: 4 - 00220/00647 - Loss: 0.15274. [ 51 s]\n",
      "Epoch: 4 - 00240/00647 - Loss: 0.14714. [ 55 s]\n",
      "Epoch: 4 - 00260/00647 - Loss: 0.23326. [ 60 s]\n",
      "Epoch: 4 - 00280/00647 - Loss: 0.10101. [ 64 s]\n",
      "Epoch: 4 - 00300/00647 - Loss: 0.08546. [ 69 s]\n",
      "Epoch: 4 - 00320/00647 - Loss: 0.15753. [ 74 s]\n",
      "Epoch: 4 - 00340/00647 - Loss: 0.11643. [ 78 s]\n",
      "Epoch: 4 - 00360/00647 - Loss: 0.11916. [ 83 s]\n",
      "Epoch: 4 - 00380/00647 - Loss: 0.32105. [ 88 s]\n",
      "Epoch: 4 - 00400/00647 - Loss: 0.14979. [ 92 s]\n",
      "Epoch: 4 - 00420/00647 - Loss: 0.15071. [ 97 s]\n",
      "Epoch: 4 - 00440/00647 - Loss: 0.23810. [101 s]\n",
      "Epoch: 4 - 00460/00647 - Loss: 0.29500. [106 s]\n",
      "Epoch: 4 - 00480/00647 - Loss: 0.15184. [111 s]\n",
      "Epoch: 4 - 00500/00647 - Loss: 0.25827. [115 s]\n",
      "Epoch: 4 - 00520/00647 - Loss: 0.35438. [120 s]\n",
      "Epoch: 4 - 00540/00647 - Loss: 0.13063. [125 s]\n",
      "Epoch: 4 - 00560/00647 - Loss: 0.41685. [129 s]\n",
      "Epoch: 4 - 00580/00647 - Loss: 0.16635. [134 s]\n",
      "Epoch: 4 - 00600/00647 - Loss: 0.18108. [139 s]\n",
      "Epoch: 4 - 00620/00647 - Loss: 0.11261. [143 s]\n",
      "Epoch: 4 - 00640/00647 - Loss: 0.09505. [148 s]\n",
      "Epoch: 4 - loss(trn/val):0.12120/0.12928, acc(val):94.88%, lr=0.00010 [BEST]. [149s] @17 samples/s \n",
      "Epoch: 5 - 00020/00647 - Loss: 0.24821. [  4 s]\n",
      "Epoch: 5 - 00040/00647 - Loss: 0.03043. [  9 s]\n",
      "Epoch: 5 - 00060/00647 - Loss: 0.07124. [ 14 s]\n",
      "Epoch: 5 - 00080/00647 - Loss: 0.15268. [ 18 s]\n",
      "Epoch: 5 - 00100/00647 - Loss: 0.11718. [ 23 s]\n",
      "Epoch: 5 - 00120/00647 - Loss: 0.09826. [ 27 s]\n",
      "Epoch: 5 - 00140/00647 - Loss: 0.16272. [ 32 s]\n",
      "Epoch: 5 - 00160/00647 - Loss: 0.08317. [ 36 s]\n",
      "Epoch: 5 - 00180/00647 - Loss: 0.04878. [ 41 s]\n",
      "Epoch: 5 - 00200/00647 - Loss: 0.14383. [ 46 s]\n",
      "Epoch: 5 - 00220/00647 - Loss: 0.09813. [ 50 s]\n",
      "Epoch: 5 - 00240/00647 - Loss: 0.20521. [ 55 s]\n",
      "Epoch: 5 - 00260/00647 - Loss: 0.12847. [ 59 s]\n",
      "Epoch: 5 - 00280/00647 - Loss: 0.09540. [ 64 s]\n",
      "Epoch: 5 - 00300/00647 - Loss: 0.45592. [ 69 s]\n",
      "Epoch: 5 - 00320/00647 - Loss: 0.20981. [ 73 s]\n",
      "Epoch: 5 - 00340/00647 - Loss: 0.09689. [ 78 s]\n",
      "Epoch: 5 - 00360/00647 - Loss: 0.10701. [ 83 s]\n",
      "Epoch: 5 - 00380/00647 - Loss: 0.11643. [ 87 s]\n",
      "Epoch: 5 - 00400/00647 - Loss: 0.15043. [ 92 s]\n",
      "Epoch: 5 - 00420/00647 - Loss: 0.08053. [ 97 s]\n",
      "Epoch: 5 - 00440/00647 - Loss: 0.10365. [101 s]\n",
      "Epoch: 5 - 00460/00647 - Loss: 0.11299. [106 s]\n",
      "Epoch: 5 - 00480/00647 - Loss: 0.08924. [110 s]\n",
      "Epoch: 5 - 00500/00647 - Loss: 0.23046. [115 s]\n",
      "Epoch: 5 - 00520/00647 - Loss: 0.16561. [120 s]\n",
      "Epoch: 5 - 00540/00647 - Loss: 0.12486. [124 s]\n",
      "Epoch: 5 - 00560/00647 - Loss: 0.38626. [129 s]\n",
      "Epoch: 5 - 00580/00647 - Loss: 0.16091. [134 s]\n",
      "Epoch: 5 - 00600/00647 - Loss: 0.40387. [138 s]\n",
      "Epoch: 5 - 00620/00647 - Loss: 0.31142. [143 s]\n",
      "Epoch: 5 - 00640/00647 - Loss: 0.10210. [148 s]\n",
      "Epoch: 5 - loss(trn/val):0.12865/0.14291, acc(val):94.45%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 6 - 00020/00647 - Loss: 0.12390. [  5 s]\n",
      "Epoch: 6 - 00040/00647 - Loss: 0.41841. [  9 s]\n",
      "Epoch: 6 - 00060/00647 - Loss: 0.09889. [ 14 s]\n",
      "Epoch: 6 - 00080/00647 - Loss: 0.35059. [ 18 s]\n",
      "Epoch: 6 - 00100/00647 - Loss: 0.03885. [ 23 s]\n",
      "Epoch: 6 - 00120/00647 - Loss: 0.13575. [ 28 s]\n",
      "Epoch: 6 - 00140/00647 - Loss: 0.10101. [ 32 s]\n",
      "Epoch: 6 - 00160/00647 - Loss: 0.32254. [ 37 s]\n",
      "Epoch: 6 - 00180/00647 - Loss: 0.24100. [ 42 s]\n",
      "Epoch: 6 - 00200/00647 - Loss: 0.11512. [ 47 s]\n",
      "Epoch: 6 - 00220/00647 - Loss: 0.07793. [ 51 s]\n",
      "Epoch: 6 - 00240/00647 - Loss: 0.11918. [ 56 s]\n",
      "Epoch: 6 - 00260/00647 - Loss: 0.26410. [ 61 s]\n",
      "Epoch: 6 - 00280/00647 - Loss: 0.13955. [ 66 s]\n",
      "Epoch: 6 - 00300/00647 - Loss: 0.48320. [ 70 s]\n",
      "Epoch: 6 - 00320/00647 - Loss: 0.08284. [ 75 s]\n",
      "Epoch: 6 - 00340/00647 - Loss: 0.12671. [ 80 s]\n",
      "Epoch: 6 - 00360/00647 - Loss: 0.16331. [ 84 s]\n",
      "Epoch: 6 - 00380/00647 - Loss: 0.10188. [ 89 s]\n",
      "Epoch: 6 - 00400/00647 - Loss: 0.07657. [ 94 s]\n",
      "Epoch: 6 - 00420/00647 - Loss: 0.06308. [ 99 s]\n",
      "Epoch: 6 - 00440/00647 - Loss: 0.10863. [103 s]\n",
      "Epoch: 6 - 00460/00647 - Loss: 0.13997. [108 s]\n",
      "Epoch: 6 - 00480/00647 - Loss: 0.16874. [113 s]\n",
      "Epoch: 6 - 00500/00647 - Loss: 0.08580. [117 s]\n",
      "Epoch: 6 - 00520/00647 - Loss: 0.08072. [122 s]\n",
      "Epoch: 6 - 00540/00647 - Loss: 0.09900. [127 s]\n",
      "Epoch: 6 - 00560/00647 - Loss: 0.07925. [132 s]\n",
      "Epoch: 6 - 00580/00647 - Loss: 0.17146. [136 s]\n",
      "Epoch: 6 - 00600/00647 - Loss: 0.05254. [141 s]\n",
      "Epoch: 6 - 00620/00647 - Loss: 0.10879. [146 s]\n",
      "Epoch: 6 - 00640/00647 - Loss: 0.30051. [150 s]\n",
      "Epoch: 6 - loss(trn/val):0.11700/0.13969, acc(val):94.15%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 7 - 00020/00647 - Loss: 0.10459. [  6 s]\n",
      "Epoch: 7 - 00040/00647 - Loss: 0.08340. [ 10 s]\n",
      "Epoch: 7 - 00060/00647 - Loss: 0.09190. [ 15 s]\n",
      "Epoch: 7 - 00080/00647 - Loss: 0.04458. [ 20 s]\n",
      "Epoch: 7 - 00100/00647 - Loss: 0.12166. [ 24 s]\n",
      "Epoch: 7 - 00120/00647 - Loss: 0.04682. [ 29 s]\n",
      "Epoch: 7 - 00140/00647 - Loss: 0.08584. [ 34 s]\n",
      "Epoch: 7 - 00160/00647 - Loss: 0.13091. [ 38 s]\n",
      "Epoch: 7 - 00180/00647 - Loss: 0.06775. [ 43 s]\n",
      "Epoch: 7 - 00200/00647 - Loss: 0.06618. [ 48 s]\n",
      "Epoch: 7 - 00220/00647 - Loss: 0.12016. [ 52 s]\n",
      "Epoch: 7 - 00240/00647 - Loss: 0.11815. [ 57 s]\n",
      "Epoch: 7 - 00260/00647 - Loss: 0.22031. [ 62 s]\n",
      "Epoch: 7 - 00280/00647 - Loss: 0.09799. [ 67 s]\n",
      "Epoch: 7 - 00300/00647 - Loss: 0.06711. [ 71 s]\n",
      "Epoch: 7 - 00320/00647 - Loss: 0.26002. [ 76 s]\n",
      "Epoch: 7 - 00340/00647 - Loss: 0.09474. [ 81 s]\n",
      "Epoch: 7 - 00360/00647 - Loss: 0.04246. [ 85 s]\n",
      "Epoch: 7 - 00380/00647 - Loss: 0.05361. [ 90 s]\n",
      "Epoch: 7 - 00400/00647 - Loss: 0.15351. [ 95 s]\n",
      "Epoch: 7 - 00420/00647 - Loss: 0.17667. [100 s]\n",
      "Epoch: 7 - 00440/00647 - Loss: 0.06673. [104 s]\n",
      "Epoch: 7 - 00460/00647 - Loss: 0.14910. [109 s]\n",
      "Epoch: 7 - 00480/00647 - Loss: 0.07267. [114 s]\n",
      "Epoch: 7 - 00500/00647 - Loss: 0.14659. [118 s]\n",
      "Epoch: 7 - 00520/00647 - Loss: 0.05247. [123 s]\n",
      "Epoch: 7 - 00540/00647 - Loss: 0.22731. [128 s]\n",
      "Epoch: 7 - 00560/00647 - Loss: 0.35770. [133 s]\n",
      "Epoch: 7 - 00580/00647 - Loss: 0.11219. [137 s]\n",
      "Epoch: 7 - 00600/00647 - Loss: 0.04968. [142 s]\n",
      "Epoch: 7 - 00620/00647 - Loss: 0.10511. [147 s]\n",
      "Epoch: 7 - 00640/00647 - Loss: 0.14370. [151 s]\n",
      "Epoch: 7 - loss(trn/val):0.09157/0.12645, acc(val):94.92%, lr=0.00010 [BEST]. [153s] @16 samples/s \n",
      "Epoch: 8 - 00020/00647 - Loss: 0.04174. [  5 s]\n",
      "Epoch: 8 - 00040/00647 - Loss: 0.05879. [  9 s]\n",
      "Epoch: 8 - 00060/00647 - Loss: 0.03957. [ 14 s]\n",
      "Epoch: 8 - 00080/00647 - Loss: 0.07426. [ 19 s]\n",
      "Epoch: 8 - 00100/00647 - Loss: 0.08880. [ 23 s]\n",
      "Epoch: 8 - 00120/00647 - Loss: 0.26023. [ 28 s]\n",
      "Epoch: 8 - 00140/00647 - Loss: 0.11731. [ 32 s]\n",
      "Epoch: 8 - 00160/00647 - Loss: 0.10855. [ 37 s]\n",
      "Epoch: 8 - 00180/00647 - Loss: 0.05682. [ 42 s]\n",
      "Epoch: 8 - 00200/00647 - Loss: 0.11688. [ 47 s]\n",
      "Epoch: 8 - 00220/00647 - Loss: 0.07015. [ 51 s]\n",
      "Epoch: 8 - 00240/00647 - Loss: 0.07252. [ 56 s]\n",
      "Epoch: 8 - 00260/00647 - Loss: 0.08525. [ 61 s]\n",
      "Epoch: 8 - 00280/00647 - Loss: 0.07271. [ 66 s]\n",
      "Epoch: 8 - 00300/00647 - Loss: 0.11452. [ 70 s]\n",
      "Epoch: 8 - 00320/00647 - Loss: 0.13959. [ 75 s]\n",
      "Epoch: 8 - 00340/00647 - Loss: 0.08085. [ 80 s]\n",
      "Epoch: 8 - 00360/00647 - Loss: 0.06142. [ 84 s]\n",
      "Epoch: 8 - 00380/00647 - Loss: 0.16867. [ 89 s]\n",
      "Epoch: 8 - 00400/00647 - Loss: 0.08240. [ 94 s]\n",
      "Epoch: 8 - 00420/00647 - Loss: 0.05109. [ 98 s]\n",
      "Epoch: 8 - 00440/00647 - Loss: 0.31856. [103 s]\n",
      "Epoch: 8 - 00460/00647 - Loss: 0.16912. [108 s]\n",
      "Epoch: 8 - 00480/00647 - Loss: 0.09639. [113 s]\n",
      "Epoch: 8 - 00500/00647 - Loss: 0.19426. [117 s]\n",
      "Epoch: 8 - 00520/00647 - Loss: 0.06961. [122 s]\n",
      "Epoch: 8 - 00540/00647 - Loss: 0.04909. [127 s]\n",
      "Epoch: 8 - 00560/00647 - Loss: 0.17777. [131 s]\n",
      "Epoch: 8 - 00580/00647 - Loss: 0.13892. [136 s]\n",
      "Epoch: 8 - 00600/00647 - Loss: 0.07694. [141 s]\n",
      "Epoch: 8 - 00620/00647 - Loss: 0.09926. [146 s]\n",
      "Epoch: 8 - 00640/00647 - Loss: 0.14770. [150 s]\n",
      "Epoch: 8 - loss(trn/val):0.09791/0.13820, acc(val):94.50%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 9 - 00020/00647 - Loss: 0.02426. [  4 s]\n",
      "Epoch: 9 - 00040/00647 - Loss: 0.04891. [  9 s]\n",
      "Epoch: 9 - 00060/00647 - Loss: 0.15037. [ 14 s]\n",
      "Epoch: 9 - 00080/00647 - Loss: 0.08086. [ 18 s]\n",
      "Epoch: 9 - 00100/00647 - Loss: 0.07913. [ 23 s]\n",
      "Epoch: 9 - 00120/00647 - Loss: 0.10023. [ 27 s]\n",
      "Epoch: 9 - 00140/00647 - Loss: 0.12269. [ 32 s]\n",
      "Epoch: 9 - 00160/00647 - Loss: 0.02937. [ 37 s]\n",
      "Epoch: 9 - 00180/00647 - Loss: 0.12823. [ 41 s]\n",
      "Epoch: 9 - 00200/00647 - Loss: 0.15712. [ 46 s]\n",
      "Epoch: 9 - 00220/00647 - Loss: 0.42722. [ 50 s]\n",
      "Epoch: 9 - 00240/00647 - Loss: 0.12004. [ 55 s]\n",
      "Epoch: 9 - 00260/00647 - Loss: 0.21367. [ 60 s]\n",
      "Epoch: 9 - 00280/00647 - Loss: 0.11454. [ 64 s]\n",
      "Epoch: 9 - 00300/00647 - Loss: 0.06671. [ 69 s]\n",
      "Epoch: 9 - 00320/00647 - Loss: 0.20395. [ 74 s]\n",
      "Epoch: 9 - 00340/00647 - Loss: 0.04093. [ 78 s]\n",
      "Epoch: 9 - 00360/00647 - Loss: 0.08455. [ 83 s]\n",
      "Epoch: 9 - 00380/00647 - Loss: 0.38645. [ 88 s]\n",
      "Epoch: 9 - 00400/00647 - Loss: 0.08956. [ 92 s]\n",
      "Epoch: 9 - 00420/00647 - Loss: 0.20806. [ 97 s]\n",
      "Epoch: 9 - 00440/00647 - Loss: 0.09167. [101 s]\n",
      "Epoch: 9 - 00460/00647 - Loss: 0.12020. [106 s]\n",
      "Epoch: 9 - 00480/00647 - Loss: 0.07158. [111 s]\n",
      "Epoch: 9 - 00500/00647 - Loss: 0.05874. [115 s]\n",
      "Epoch: 9 - 00520/00647 - Loss: 0.14998. [120 s]\n",
      "Epoch: 9 - 00540/00647 - Loss: 0.06921. [125 s]\n",
      "Epoch: 9 - 00560/00647 - Loss: 0.06904. [129 s]\n",
      "Epoch: 9 - 00580/00647 - Loss: 0.05694. [134 s]\n",
      "Epoch: 9 - 00600/00647 - Loss: 0.14226. [139 s]\n",
      "Epoch: 9 - 00620/00647 - Loss: 0.04303. [143 s]\n",
      "Epoch: 9 - 00640/00647 - Loss: 0.10577. [148 s]\n",
      "Epoch: 9 - loss(trn/val):0.08476/0.09549, acc(val):96.25%, lr=0.00010 [BEST]. [150s] @17 samples/s \n",
      "Epoch: 10 - 00020/00647 - Loss: 0.04955. [  4 s]\n",
      "Epoch: 10 - 00040/00647 - Loss: 0.05982. [  9 s]\n",
      "Epoch: 10 - 00060/00647 - Loss: 0.12873. [ 14 s]\n",
      "Epoch: 10 - 00080/00647 - Loss: 0.10307. [ 18 s]\n",
      "Epoch: 10 - 00100/00647 - Loss: 0.14227. [ 23 s]\n",
      "Epoch: 10 - 00120/00647 - Loss: 0.03962. [ 27 s]\n",
      "Epoch: 10 - 00140/00647 - Loss: 0.06528. [ 32 s]\n",
      "Epoch: 10 - 00160/00647 - Loss: 0.07538. [ 37 s]\n",
      "Epoch: 10 - 00180/00647 - Loss: 0.06644. [ 41 s]\n",
      "Epoch: 10 - 00200/00647 - Loss: 0.10399. [ 46 s]\n",
      "Epoch: 10 - 00220/00647 - Loss: 0.16544. [ 50 s]\n",
      "Epoch: 10 - 00240/00647 - Loss: 0.08124. [ 55 s]\n",
      "Epoch: 10 - 00260/00647 - Loss: 0.11323. [ 60 s]\n",
      "Epoch: 10 - 00280/00647 - Loss: 0.10426. [ 64 s]\n",
      "Epoch: 10 - 00300/00647 - Loss: 0.18314. [ 69 s]\n",
      "Epoch: 10 - 00320/00647 - Loss: 0.05892. [ 74 s]\n",
      "Epoch: 10 - 00340/00647 - Loss: 0.02705. [ 78 s]\n",
      "Epoch: 10 - 00360/00647 - Loss: 0.06688. [ 83 s]\n",
      "Epoch: 10 - 00380/00647 - Loss: 0.16573. [ 88 s]\n",
      "Epoch: 10 - 00400/00647 - Loss: 0.07517. [ 92 s]\n",
      "Epoch: 10 - 00420/00647 - Loss: 0.06239. [ 97 s]\n",
      "Epoch: 10 - 00440/00647 - Loss: 0.13620. [101 s]\n",
      "Epoch: 10 - 00460/00647 - Loss: 0.09641. [106 s]\n",
      "Epoch: 10 - 00480/00647 - Loss: 0.05939. [111 s]\n",
      "Epoch: 10 - 00500/00647 - Loss: 0.04740. [115 s]\n",
      "Epoch: 10 - 00520/00647 - Loss: 0.13642. [120 s]\n",
      "Epoch: 10 - 00540/00647 - Loss: 0.05766. [125 s]\n",
      "Epoch: 10 - 00560/00647 - Loss: 0.38113. [129 s]\n",
      "Epoch: 10 - 00580/00647 - Loss: 0.13111. [134 s]\n",
      "Epoch: 10 - 00600/00647 - Loss: 0.05102. [139 s]\n",
      "Epoch: 10 - 00620/00647 - Loss: 0.06348. [143 s]\n",
      "Epoch: 10 - 00640/00647 - Loss: 0.09636. [148 s]\n",
      "Epoch: 10 - loss(trn/val):0.07407/0.12358, acc(val):95.31%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 11 - 00020/00647 - Loss: 0.07274. [  4 s]\n",
      "Epoch: 11 - 00040/00647 - Loss: 0.05590. [  9 s]\n",
      "Epoch: 11 - 00060/00647 - Loss: 0.04525. [ 14 s]\n",
      "Epoch: 11 - 00080/00647 - Loss: 0.08668. [ 18 s]\n",
      "Epoch: 11 - 00100/00647 - Loss: 0.04595. [ 23 s]\n",
      "Epoch: 11 - 00120/00647 - Loss: 0.07072. [ 27 s]\n",
      "Epoch: 11 - 00140/00647 - Loss: 0.09940. [ 32 s]\n",
      "Epoch: 11 - 00160/00647 - Loss: 0.18585. [ 37 s]\n",
      "Epoch: 11 - 00180/00647 - Loss: 0.02540. [ 41 s]\n",
      "Epoch: 11 - 00200/00647 - Loss: 0.08003. [ 46 s]\n",
      "Epoch: 11 - 00220/00647 - Loss: 0.16519. [ 51 s]\n",
      "Epoch: 11 - 00240/00647 - Loss: 0.03889. [ 55 s]\n",
      "Epoch: 11 - 00260/00647 - Loss: 0.06640. [ 60 s]\n",
      "Epoch: 11 - 00280/00647 - Loss: 0.17990. [ 64 s]\n",
      "Epoch: 11 - 00300/00647 - Loss: 0.30780. [ 69 s]\n",
      "Epoch: 11 - 00320/00647 - Loss: 0.07128. [ 74 s]\n",
      "Epoch: 11 - 00340/00647 - Loss: 0.12839. [ 78 s]\n",
      "Epoch: 11 - 00360/00647 - Loss: 0.09634. [ 83 s]\n",
      "Epoch: 11 - 00380/00647 - Loss: 0.04439. [ 88 s]\n",
      "Epoch: 11 - 00400/00647 - Loss: 0.13865. [ 92 s]\n",
      "Epoch: 11 - 00420/00647 - Loss: 0.09345. [ 97 s]\n",
      "Epoch: 11 - 00440/00647 - Loss: 0.30031. [102 s]\n",
      "Epoch: 11 - 00460/00647 - Loss: 0.06324. [106 s]\n",
      "Epoch: 11 - 00480/00647 - Loss: 0.13001. [111 s]\n",
      "Epoch: 11 - 00500/00647 - Loss: 0.05220. [116 s]\n",
      "Epoch: 11 - 00520/00647 - Loss: 0.06016. [120 s]\n",
      "Epoch: 11 - 00540/00647 - Loss: 0.08910. [125 s]\n",
      "Epoch: 11 - 00560/00647 - Loss: 0.16120. [130 s]\n",
      "Epoch: 11 - 00580/00647 - Loss: 0.09154. [134 s]\n",
      "Epoch: 11 - 00600/00647 - Loss: 0.03876. [139 s]\n",
      "Epoch: 11 - 00620/00647 - Loss: 0.07336. [143 s]\n",
      "Epoch: 11 - 00640/00647 - Loss: 0.04674. [148 s]\n",
      "Epoch: 11 - loss(trn/val):0.06475/0.14181, acc(val):94.71%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 12 - 00020/00647 - Loss: 0.15837. [  5 s]\n",
      "Epoch: 12 - 00040/00647 - Loss: 0.03121. [  9 s]\n",
      "Epoch: 12 - 00060/00647 - Loss: 0.06370. [ 14 s]\n",
      "Epoch: 12 - 00080/00647 - Loss: 0.10100. [ 18 s]\n",
      "Epoch: 12 - 00100/00647 - Loss: 0.07261. [ 23 s]\n",
      "Epoch: 12 - 00120/00647 - Loss: 0.03333. [ 27 s]\n",
      "Epoch: 12 - 00140/00647 - Loss: 0.06036. [ 32 s]\n",
      "Epoch: 12 - 00160/00647 - Loss: 0.07939. [ 37 s]\n",
      "Epoch: 12 - 00180/00647 - Loss: 0.10168. [ 41 s]\n",
      "Epoch: 12 - 00200/00647 - Loss: 0.03947. [ 46 s]\n",
      "Epoch: 12 - 00220/00647 - Loss: 0.05014. [ 51 s]\n",
      "Epoch: 12 - 00240/00647 - Loss: 0.04470. [ 55 s]\n",
      "Epoch: 12 - 00260/00647 - Loss: 0.06782. [ 60 s]\n",
      "Epoch: 12 - 00280/00647 - Loss: 0.03152. [ 65 s]\n",
      "Epoch: 12 - 00300/00647 - Loss: 0.08391. [ 69 s]\n",
      "Epoch: 12 - 00320/00647 - Loss: 0.03255. [ 74 s]\n",
      "Epoch: 12 - 00340/00647 - Loss: 0.07964. [ 79 s]\n",
      "Epoch: 12 - 00360/00647 - Loss: 0.08706. [ 83 s]\n",
      "Epoch: 12 - 00380/00647 - Loss: 0.06282. [ 88 s]\n",
      "Epoch: 12 - 00400/00647 - Loss: 0.04369. [ 92 s]\n",
      "Epoch: 12 - 00420/00647 - Loss: 0.09866. [ 97 s]\n",
      "Epoch: 12 - 00440/00647 - Loss: 0.26986. [102 s]\n",
      "Epoch: 12 - 00460/00647 - Loss: 0.05286. [106 s]\n",
      "Epoch: 12 - 00480/00647 - Loss: 0.10140. [111 s]\n",
      "Epoch: 12 - 00500/00647 - Loss: 0.05007. [116 s]\n",
      "Epoch: 12 - 00520/00647 - Loss: 0.04923. [120 s]\n",
      "Epoch: 12 - 00540/00647 - Loss: 0.08462. [125 s]\n",
      "Epoch: 12 - 00560/00647 - Loss: 0.11006. [129 s]\n",
      "Epoch: 12 - 00580/00647 - Loss: 0.29890. [134 s]\n",
      "Epoch: 12 - 00600/00647 - Loss: 0.17212. [139 s]\n",
      "Epoch: 12 - 00620/00647 - Loss: 0.11423. [143 s]\n",
      "Epoch: 12 - 00640/00647 - Loss: 0.06431. [148 s]\n",
      "Epoch: 12 - loss(trn/val):0.06805/0.13878, acc(val):95.17%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 13 - 00020/00647 - Loss: 0.04088. [  4 s]\n",
      "Epoch: 13 - 00040/00647 - Loss: 0.04359. [  9 s]\n",
      "Epoch: 13 - 00060/00647 - Loss: 0.13196. [ 14 s]\n",
      "Epoch: 13 - 00080/00647 - Loss: 0.01844. [ 18 s]\n",
      "Epoch: 13 - 00100/00647 - Loss: 0.06372. [ 23 s]\n",
      "Epoch: 13 - 00120/00647 - Loss: 0.14787. [ 27 s]\n",
      "Epoch: 13 - 00140/00647 - Loss: 0.04240. [ 32 s]\n",
      "Epoch: 13 - 00160/00647 - Loss: 0.03627. [ 37 s]\n",
      "Epoch: 13 - 00180/00647 - Loss: 0.04541. [ 41 s]\n",
      "Epoch: 13 - 00200/00647 - Loss: 0.05047. [ 46 s]\n",
      "Epoch: 13 - 00220/00647 - Loss: 0.06014. [ 51 s]\n",
      "Epoch: 13 - 00240/00647 - Loss: 0.08104. [ 55 s]\n",
      "Epoch: 13 - 00260/00647 - Loss: 0.11207. [ 60 s]\n",
      "Epoch: 13 - 00280/00647 - Loss: 0.13011. [ 64 s]\n",
      "Epoch: 13 - 00300/00647 - Loss: 0.07199. [ 69 s]\n",
      "Epoch: 13 - 00320/00647 - Loss: 0.07894. [ 74 s]\n",
      "Epoch: 13 - 00340/00647 - Loss: 0.26426. [ 78 s]\n",
      "Epoch: 13 - 00360/00647 - Loss: 0.04211. [ 83 s]\n",
      "Epoch: 13 - 00380/00647 - Loss: 0.03633. [ 88 s]\n",
      "Epoch: 13 - 00400/00647 - Loss: 0.09568. [ 92 s]\n",
      "Epoch: 13 - 00420/00647 - Loss: 0.02817. [ 97 s]\n",
      "Epoch: 13 - 00440/00647 - Loss: 0.08252. [102 s]\n",
      "Epoch: 13 - 00460/00647 - Loss: 0.03513. [106 s]\n",
      "Epoch: 13 - 00480/00647 - Loss: 0.10528. [111 s]\n",
      "Epoch: 13 - 00500/00647 - Loss: 0.02279. [115 s]\n",
      "Epoch: 13 - 00520/00647 - Loss: 0.06988. [120 s]\n",
      "Epoch: 13 - 00540/00647 - Loss: 0.07580. [125 s]\n",
      "Epoch: 13 - 00560/00647 - Loss: 0.08883. [129 s]\n",
      "Epoch: 13 - 00580/00647 - Loss: 0.02308. [134 s]\n",
      "Epoch: 13 - 00600/00647 - Loss: 0.06735. [139 s]\n",
      "Epoch: 13 - 00620/00647 - Loss: 0.07090. [143 s]\n",
      "Epoch: 13 - 00640/00647 - Loss: 0.04160. [148 s]\n",
      "Epoch: 13 - loss(trn/val):0.06266/0.12545, acc(val):95.56%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 14 - 00020/00647 - Loss: 0.09562. [  4 s]\n",
      "Epoch: 14 - 00040/00647 - Loss: 0.07687. [  9 s]\n",
      "Epoch: 14 - 00060/00647 - Loss: 0.14026. [ 14 s]\n",
      "Epoch: 14 - 00080/00647 - Loss: 0.03210. [ 18 s]\n",
      "Epoch: 14 - 00100/00647 - Loss: 0.05186. [ 23 s]\n",
      "Epoch: 14 - 00120/00647 - Loss: 0.02933. [ 27 s]\n",
      "Epoch: 14 - 00140/00647 - Loss: 0.02877. [ 32 s]\n",
      "Epoch: 14 - 00160/00647 - Loss: 0.04989. [ 37 s]\n",
      "Epoch: 14 - 00180/00647 - Loss: 0.09792. [ 41 s]\n",
      "Epoch: 14 - 00200/00647 - Loss: 0.07276. [ 46 s]\n",
      "Epoch: 14 - 00220/00647 - Loss: 0.05058. [ 50 s]\n",
      "Epoch: 14 - 00240/00647 - Loss: 0.04943. [ 55 s]\n",
      "Epoch: 14 - 00260/00647 - Loss: 0.07026. [ 60 s]\n",
      "Epoch: 14 - 00280/00647 - Loss: 0.06312. [ 64 s]\n",
      "Epoch: 14 - 00300/00647 - Loss: 0.05126. [ 69 s]\n",
      "Epoch: 14 - 00320/00647 - Loss: 0.11545. [ 74 s]\n",
      "Epoch: 14 - 00340/00647 - Loss: 0.04212. [ 78 s]\n",
      "Epoch: 14 - 00360/00647 - Loss: 0.01876. [ 83 s]\n",
      "Epoch: 14 - 00380/00647 - Loss: 0.04890. [ 88 s]\n",
      "Epoch: 14 - 00400/00647 - Loss: 0.09195. [ 92 s]\n",
      "Epoch: 14 - 00420/00647 - Loss: 0.10421. [ 97 s]\n",
      "Epoch: 14 - 00440/00647 - Loss: 0.03783. [101 s]\n",
      "Epoch: 14 - 00460/00647 - Loss: 0.07250. [106 s]\n",
      "Epoch: 14 - 00480/00647 - Loss: 0.08462. [111 s]\n",
      "Epoch: 14 - 00500/00647 - Loss: 0.02974. [115 s]\n",
      "Epoch: 14 - 00520/00647 - Loss: 0.12710. [120 s]\n",
      "Epoch: 14 - 00540/00647 - Loss: 0.20037. [125 s]\n",
      "Epoch: 14 - 00560/00647 - Loss: 0.12897. [129 s]\n",
      "Epoch: 14 - 00580/00647 - Loss: 0.05948. [134 s]\n",
      "Epoch: 14 - 00600/00647 - Loss: 0.08711. [139 s]\n",
      "Epoch: 14 - 00620/00647 - Loss: 0.05011. [143 s]\n",
      "Epoch: 14 - 00640/00647 - Loss: 0.04696. [148 s]\n",
      "Epoch: 14 - loss(trn/val):0.06257/0.12731, acc(val):95.13%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 15 - 00020/00647 - Loss: 0.07391. [  5 s]\n",
      "Epoch: 15 - 00040/00647 - Loss: 0.06701. [  9 s]\n",
      "Epoch: 15 - 00060/00647 - Loss: 0.01889. [ 14 s]\n",
      "Epoch: 15 - 00080/00647 - Loss: 0.18317. [ 18 s]\n",
      "Epoch: 15 - 00100/00647 - Loss: 0.10023. [ 23 s]\n",
      "Epoch: 15 - 00120/00647 - Loss: 0.07060. [ 27 s]\n",
      "Epoch: 15 - 00140/00647 - Loss: 0.03547. [ 32 s]\n",
      "Epoch: 15 - 00160/00647 - Loss: 0.06592. [ 37 s]\n",
      "Epoch: 15 - 00180/00647 - Loss: 0.04687. [ 41 s]\n",
      "Epoch: 15 - 00200/00647 - Loss: 0.05724. [ 46 s]\n",
      "Epoch: 15 - 00220/00647 - Loss: 0.05266. [ 51 s]\n",
      "Epoch: 15 - 00240/00647 - Loss: 0.03622. [ 55 s]\n",
      "Epoch: 15 - 00260/00647 - Loss: 0.07682. [ 60 s]\n",
      "Epoch: 15 - 00280/00647 - Loss: 0.02528. [ 64 s]\n",
      "Epoch: 15 - 00300/00647 - Loss: 0.04455. [ 69 s]\n",
      "Epoch: 15 - 00320/00647 - Loss: 0.09753. [ 74 s]\n",
      "Epoch: 15 - 00340/00647 - Loss: 0.02931. [ 78 s]\n",
      "Epoch: 15 - 00360/00647 - Loss: 0.07813. [ 83 s]\n",
      "Epoch: 15 - 00380/00647 - Loss: 0.04633. [ 88 s]\n",
      "Epoch: 15 - 00400/00647 - Loss: 0.07389. [ 92 s]\n",
      "Epoch: 15 - 00420/00647 - Loss: 0.03321. [ 97 s]\n",
      "Epoch: 15 - 00440/00647 - Loss: 0.10005. [101 s]\n",
      "Epoch: 15 - 00460/00647 - Loss: 0.07932. [106 s]\n",
      "Epoch: 15 - 00480/00647 - Loss: 0.08167. [111 s]\n",
      "Epoch: 15 - 00500/00647 - Loss: 0.02422. [115 s]\n",
      "Epoch: 15 - 00520/00647 - Loss: 0.05647. [120 s]\n",
      "Epoch: 15 - 00540/00647 - Loss: 0.07132. [125 s]\n",
      "Epoch: 15 - 00560/00647 - Loss: 0.05710. [129 s]\n",
      "Epoch: 15 - 00580/00647 - Loss: 0.07153. [134 s]\n",
      "Epoch: 15 - 00600/00647 - Loss: 0.05191. [139 s]\n",
      "Epoch: 15 - 00620/00647 - Loss: 0.13173. [143 s]\n",
      "Epoch: 15 - 00640/00647 - Loss: 0.04709. [148 s]\n",
      "Epoch: 15 - loss(trn/val):0.07103/0.13999, acc(val):94.61%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 16 - 00020/00647 - Loss: 0.09886. [  4 s]\n",
      "Epoch: 16 - 00040/00647 - Loss: 0.04323. [  9 s]\n",
      "Epoch: 16 - 00060/00647 - Loss: 0.07065. [ 14 s]\n",
      "Epoch: 16 - 00080/00647 - Loss: 0.05088. [ 18 s]\n",
      "Epoch: 16 - 00100/00647 - Loss: 0.08983. [ 23 s]\n",
      "Epoch: 16 - 00120/00647 - Loss: 0.08414. [ 27 s]\n",
      "Epoch: 16 - 00140/00647 - Loss: 0.09485. [ 32 s]\n",
      "Epoch: 16 - 00160/00647 - Loss: 0.05968. [ 37 s]\n",
      "Epoch: 16 - 00180/00647 - Loss: 0.04579. [ 41 s]\n",
      "Epoch: 16 - 00200/00647 - Loss: 0.03255. [ 46 s]\n",
      "Epoch: 16 - 00220/00647 - Loss: 0.04387. [ 51 s]\n",
      "Epoch: 16 - 00240/00647 - Loss: 0.07548. [ 55 s]\n",
      "Epoch: 16 - 00260/00647 - Loss: 0.06450. [ 60 s]\n",
      "Epoch: 16 - 00280/00647 - Loss: 0.02484. [ 65 s]\n",
      "Epoch: 16 - 00300/00647 - Loss: 0.03966. [ 69 s]\n",
      "Epoch: 16 - 00320/00647 - Loss: 0.03212. [ 74 s]\n",
      "Epoch: 16 - 00340/00647 - Loss: 0.02333. [ 78 s]\n",
      "Epoch: 16 - 00360/00647 - Loss: 0.05298. [ 83 s]\n",
      "Epoch: 16 - 00380/00647 - Loss: 0.04155. [ 88 s]\n",
      "Epoch: 16 - 00400/00647 - Loss: 0.02762. [ 92 s]\n",
      "Epoch: 16 - 00420/00647 - Loss: 0.05633. [ 97 s]\n",
      "Epoch: 16 - 00440/00647 - Loss: 0.02679. [102 s]\n",
      "Epoch: 16 - 00460/00647 - Loss: 0.04700. [106 s]\n",
      "Epoch: 16 - 00480/00647 - Loss: 0.04779. [111 s]\n",
      "Epoch: 16 - 00500/00647 - Loss: 0.04195. [116 s]\n",
      "Epoch: 16 - 00520/00647 - Loss: 0.02777. [120 s]\n",
      "Epoch: 16 - 00540/00647 - Loss: 0.05439. [125 s]\n",
      "Epoch: 16 - 00560/00647 - Loss: 0.14810. [129 s]\n",
      "Epoch: 16 - 00580/00647 - Loss: 0.06562. [134 s]\n",
      "Epoch: 16 - 00600/00647 - Loss: 0.18502. [139 s]\n",
      "Epoch: 16 - 00620/00647 - Loss: 0.06612. [143 s]\n",
      "Epoch: 16 - 00640/00647 - Loss: 0.02955. [148 s]\n",
      "Epoch: 16 - loss(trn/val):0.06547/0.15316, acc(val):94.35%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 17 - 00020/00647 - Loss: 0.06403. [  5 s]\n",
      "Epoch: 17 - 00040/00647 - Loss: 0.03928. [  9 s]\n",
      "Epoch: 17 - 00060/00647 - Loss: 0.03976. [ 14 s]\n",
      "Epoch: 17 - 00080/00647 - Loss: 0.04286. [ 18 s]\n",
      "Epoch: 17 - 00100/00647 - Loss: 0.04617. [ 23 s]\n",
      "Epoch: 17 - 00120/00647 - Loss: 0.11276. [ 27 s]\n",
      "Epoch: 17 - 00140/00647 - Loss: 0.10285. [ 32 s]\n",
      "Epoch: 17 - 00160/00647 - Loss: 0.09673. [ 37 s]\n",
      "Epoch: 17 - 00180/00647 - Loss: 0.06745. [ 41 s]\n",
      "Epoch: 17 - 00200/00647 - Loss: 0.03049. [ 46 s]\n",
      "Epoch: 17 - 00220/00647 - Loss: 0.03669. [ 51 s]\n",
      "Epoch: 17 - 00240/00647 - Loss: 0.04165. [ 55 s]\n",
      "Epoch: 17 - 00260/00647 - Loss: 0.12444. [ 60 s]\n",
      "Epoch: 17 - 00280/00647 - Loss: 0.05863. [ 64 s]\n",
      "Epoch: 17 - 00300/00647 - Loss: 0.08054. [ 69 s]\n",
      "Epoch: 17 - 00320/00647 - Loss: 0.12057. [ 74 s]\n",
      "Epoch: 17 - 00340/00647 - Loss: 0.06886. [ 78 s]\n",
      "Epoch: 17 - 00360/00647 - Loss: 0.02363. [ 83 s]\n",
      "Epoch: 17 - 00380/00647 - Loss: 0.04679. [ 88 s]\n",
      "Epoch: 17 - 00400/00647 - Loss: 0.03221. [ 92 s]\n",
      "Epoch: 17 - 00420/00647 - Loss: 0.03986. [ 97 s]\n",
      "Epoch: 17 - 00440/00647 - Loss: 0.04251. [102 s]\n",
      "Epoch: 17 - 00460/00647 - Loss: 0.06272. [106 s]\n",
      "Epoch: 17 - 00480/00647 - Loss: 0.01999. [111 s]\n",
      "Epoch: 17 - 00500/00647 - Loss: 0.12302. [115 s]\n",
      "Epoch: 17 - 00520/00647 - Loss: 0.07974. [120 s]\n",
      "Epoch: 17 - 00540/00647 - Loss: 0.10960. [125 s]\n",
      "Epoch: 17 - 00560/00647 - Loss: 0.05796. [129 s]\n",
      "Epoch: 17 - 00580/00647 - Loss: 0.08035. [134 s]\n",
      "Epoch: 17 - 00600/00647 - Loss: 0.04261. [139 s]\n",
      "Epoch: 17 - 00620/00647 - Loss: 0.05778. [143 s]\n",
      "Epoch: 17 - 00640/00647 - Loss: 0.02924. [148 s]\n",
      "Epoch: 17 - loss(trn/val):0.04837/0.11867, acc(val):96.05%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 18 - 00020/00647 - Loss: 0.05855. [  4 s]\n",
      "Epoch: 18 - 00040/00647 - Loss: 0.03263. [  9 s]\n",
      "Epoch: 18 - 00060/00647 - Loss: 0.09142. [ 14 s]\n",
      "Epoch: 18 - 00080/00647 - Loss: 0.06438. [ 18 s]\n",
      "Epoch: 18 - 00100/00647 - Loss: 0.10667. [ 23 s]\n",
      "Epoch: 18 - 00120/00647 - Loss: 0.02567. [ 27 s]\n",
      "Epoch: 18 - 00140/00647 - Loss: 0.04524. [ 32 s]\n",
      "Epoch: 18 - 00160/00647 - Loss: 0.02752. [ 37 s]\n",
      "Epoch: 18 - 00180/00647 - Loss: 0.05741. [ 41 s]\n",
      "Epoch: 18 - 00200/00647 - Loss: 0.03887. [ 46 s]\n",
      "Epoch: 18 - 00220/00647 - Loss: 0.10103. [ 50 s]\n",
      "Epoch: 18 - 00240/00647 - Loss: 0.04281. [ 55 s]\n",
      "Epoch: 18 - 00260/00647 - Loss: 0.03809. [ 60 s]\n",
      "Epoch: 18 - 00280/00647 - Loss: 0.03707. [ 64 s]\n",
      "Epoch: 18 - 00300/00647 - Loss: 0.03157. [ 69 s]\n",
      "Epoch: 18 - 00320/00647 - Loss: 0.06417. [ 74 s]\n",
      "Epoch: 18 - 00340/00647 - Loss: 0.01390. [ 78 s]\n",
      "Epoch: 18 - 00360/00647 - Loss: 0.03192. [ 83 s]\n",
      "Epoch: 18 - 00380/00647 - Loss: 0.06244. [ 88 s]\n",
      "Epoch: 18 - 00400/00647 - Loss: 0.12260. [ 92 s]\n",
      "Epoch: 18 - 00420/00647 - Loss: 0.03830. [ 97 s]\n",
      "Epoch: 18 - 00440/00647 - Loss: 0.02875. [102 s]\n",
      "Epoch: 18 - 00460/00647 - Loss: 0.05297. [106 s]\n",
      "Epoch: 18 - 00480/00647 - Loss: 0.08423. [111 s]\n",
      "Epoch: 18 - 00500/00647 - Loss: 0.04208. [115 s]\n",
      "Epoch: 18 - 00520/00647 - Loss: 0.05163. [120 s]\n",
      "Epoch: 18 - 00540/00647 - Loss: 0.03697. [125 s]\n",
      "Epoch: 18 - 00560/00647 - Loss: 0.05199. [129 s]\n",
      "Epoch: 18 - 00580/00647 - Loss: 0.03788. [134 s]\n",
      "Epoch: 18 - 00600/00647 - Loss: 0.02914. [139 s]\n",
      "Epoch: 18 - 00620/00647 - Loss: 0.03700. [143 s]\n",
      "Epoch: 18 - 00640/00647 - Loss: 0.07631. [148 s]\n",
      "Epoch: 18 - loss(trn/val):0.05616/0.17817, acc(val):94.70%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 19 - 00020/00647 - Loss: 0.07714. [  5 s]\n",
      "Epoch: 19 - 00040/00647 - Loss: 0.02641. [  9 s]\n",
      "Epoch: 19 - 00060/00647 - Loss: 0.04974. [ 14 s]\n",
      "Epoch: 19 - 00080/00647 - Loss: 0.04290. [ 18 s]\n",
      "Epoch: 19 - 00100/00647 - Loss: 0.07916. [ 23 s]\n",
      "Epoch: 19 - 00120/00647 - Loss: 0.05161. [ 27 s]\n",
      "Epoch: 19 - 00140/00647 - Loss: 0.03282. [ 32 s]\n",
      "Epoch: 19 - 00160/00647 - Loss: 0.09282. [ 37 s]\n",
      "Epoch: 19 - 00180/00647 - Loss: 0.03364. [ 41 s]\n",
      "Epoch: 19 - 00200/00647 - Loss: 0.03084. [ 46 s]\n",
      "Epoch: 19 - 00220/00647 - Loss: 0.03497. [ 51 s]\n",
      "Epoch: 19 - 00240/00647 - Loss: 0.01872. [ 55 s]\n",
      "Epoch: 19 - 00260/00647 - Loss: 0.03793. [ 60 s]\n",
      "Epoch: 19 - 00280/00647 - Loss: 0.08420. [ 65 s]\n",
      "Epoch: 19 - 00300/00647 - Loss: 0.07453. [ 69 s]\n",
      "Epoch: 19 - 00320/00647 - Loss: 0.06690. [ 74 s]\n",
      "Epoch: 19 - 00340/00647 - Loss: 0.02407. [ 79 s]\n",
      "Epoch: 19 - 00360/00647 - Loss: 0.08180. [ 83 s]\n",
      "Epoch: 19 - 00380/00647 - Loss: 0.15011. [ 88 s]\n",
      "Epoch: 19 - 00400/00647 - Loss: 0.04338. [ 92 s]\n",
      "Epoch: 19 - 00420/00647 - Loss: 0.02540. [ 97 s]\n",
      "Epoch: 19 - 00440/00647 - Loss: 0.04356. [102 s]\n",
      "Epoch: 19 - 00460/00647 - Loss: 0.05766. [106 s]\n",
      "Epoch: 19 - 00480/00647 - Loss: 0.01905. [111 s]\n",
      "Epoch: 19 - 00500/00647 - Loss: 0.02372. [116 s]\n",
      "Epoch: 19 - 00520/00647 - Loss: 0.07022. [120 s]\n",
      "Epoch: 19 - 00540/00647 - Loss: 0.04404. [125 s]\n",
      "Epoch: 19 - 00560/00647 - Loss: 0.07576. [130 s]\n",
      "Epoch: 19 - 00580/00647 - Loss: 0.08059. [134 s]\n",
      "Epoch: 19 - 00600/00647 - Loss: 0.02546. [139 s]\n",
      "Epoch: 19 - 00620/00647 - Loss: 0.03423. [143 s]\n",
      "Epoch: 19 - 00640/00647 - Loss: 0.05417. [148 s]\n",
      "Epoch: 19 - loss(trn/val):0.04356/0.13251, acc(val):95.77%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 20 - 00020/00647 - Loss: 0.06304. [  4 s]\n",
      "Epoch: 20 - 00040/00647 - Loss: 0.03104. [  9 s]\n",
      "Epoch: 20 - 00060/00647 - Loss: 0.03060. [ 14 s]\n",
      "Epoch: 20 - 00080/00647 - Loss: 0.03977. [ 18 s]\n",
      "Epoch: 20 - 00100/00647 - Loss: 0.08997. [ 23 s]\n",
      "Epoch: 20 - 00120/00647 - Loss: 0.03315. [ 27 s]\n",
      "Epoch: 20 - 00140/00647 - Loss: 0.02671. [ 32 s]\n",
      "Epoch: 20 - 00160/00647 - Loss: 0.05186. [ 37 s]\n",
      "Epoch: 20 - 00180/00647 - Loss: 0.04695. [ 41 s]\n",
      "Epoch: 20 - 00200/00647 - Loss: 0.03616. [ 46 s]\n",
      "Epoch: 20 - 00220/00647 - Loss: 0.10045. [ 51 s]\n",
      "Epoch: 20 - 00240/00647 - Loss: 0.03955. [ 55 s]\n",
      "Epoch: 20 - 00260/00647 - Loss: 0.10458. [ 60 s]\n",
      "Epoch: 20 - 00280/00647 - Loss: 0.04568. [ 65 s]\n",
      "Epoch: 20 - 00300/00647 - Loss: 0.01371. [ 69 s]\n",
      "Epoch: 20 - 00320/00647 - Loss: 0.04336. [ 74 s]\n",
      "Epoch: 20 - 00340/00647 - Loss: 0.10556. [ 79 s]\n",
      "Epoch: 20 - 00360/00647 - Loss: 0.02731. [ 83 s]\n",
      "Epoch: 20 - 00380/00647 - Loss: 0.02439. [ 88 s]\n",
      "Epoch: 20 - 00400/00647 - Loss: 0.04599. [ 93 s]\n",
      "Epoch: 20 - 00420/00647 - Loss: 0.03453. [ 97 s]\n",
      "Epoch: 20 - 00440/00647 - Loss: 0.04387. [102 s]\n",
      "Epoch: 20 - 00460/00647 - Loss: 0.09081. [107 s]\n",
      "Epoch: 20 - 00480/00647 - Loss: 0.03465. [111 s]\n",
      "Epoch: 20 - 00500/00647 - Loss: 0.16780. [116 s]\n",
      "Epoch: 20 - 00520/00647 - Loss: 0.03546. [121 s]\n",
      "Epoch: 20 - 00540/00647 - Loss: 0.06761. [125 s]\n",
      "Epoch: 20 - 00560/00647 - Loss: 0.02896. [130 s]\n",
      "Epoch: 20 - 00580/00647 - Loss: 0.04101. [135 s]\n",
      "Epoch: 20 - 00600/00647 - Loss: 0.11149. [139 s]\n",
      "Epoch: 20 - 00620/00647 - Loss: 0.04269. [144 s]\n",
      "Epoch: 20 - 00640/00647 - Loss: 0.14254. [149 s]\n",
      "Epoch: 20 - loss(trn/val):0.05779/0.17259, acc(val):94.03%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 21 - 00020/00647 - Loss: 0.06640. [  5 s]\n",
      "Epoch: 21 - 00040/00647 - Loss: 0.04622. [  9 s]\n",
      "Epoch: 21 - 00060/00647 - Loss: 0.03008. [ 14 s]\n",
      "Epoch: 21 - 00080/00647 - Loss: 0.01790. [ 18 s]\n",
      "Epoch: 21 - 00100/00647 - Loss: 0.03682. [ 23 s]\n",
      "Epoch: 21 - 00120/00647 - Loss: 0.03781. [ 28 s]\n",
      "Epoch: 21 - 00140/00647 - Loss: 0.05536. [ 32 s]\n",
      "Epoch: 21 - 00160/00647 - Loss: 0.06927. [ 37 s]\n",
      "Epoch: 21 - 00180/00647 - Loss: 0.04575. [ 42 s]\n",
      "Epoch: 21 - 00200/00647 - Loss: 0.03806. [ 47 s]\n",
      "Epoch: 21 - 00220/00647 - Loss: 0.06036. [ 51 s]\n",
      "Epoch: 21 - 00240/00647 - Loss: 0.05713. [ 56 s]\n",
      "Epoch: 21 - 00260/00647 - Loss: 0.02873. [ 61 s]\n",
      "Epoch: 21 - 00280/00647 - Loss: 0.03490. [ 65 s]\n",
      "Epoch: 21 - 00300/00647 - Loss: 0.11298. [ 70 s]\n",
      "Epoch: 21 - 00320/00647 - Loss: 0.05421. [ 75 s]\n",
      "Epoch: 21 - 00340/00647 - Loss: 0.05641. [ 79 s]\n",
      "Epoch: 21 - 00360/00647 - Loss: 0.03759. [ 84 s]\n",
      "Epoch: 21 - 00380/00647 - Loss: 0.01972. [ 89 s]\n",
      "Epoch: 21 - 00400/00647 - Loss: 0.04065. [ 93 s]\n",
      "Epoch: 21 - 00420/00647 - Loss: 0.05473. [ 98 s]\n",
      "Epoch: 21 - 00440/00647 - Loss: 0.05961. [103 s]\n",
      "Epoch: 21 - 00460/00647 - Loss: 0.04213. [107 s]\n",
      "Epoch: 21 - 00480/00647 - Loss: 0.03749. [112 s]\n",
      "Epoch: 21 - 00500/00647 - Loss: 0.04837. [117 s]\n",
      "Epoch: 21 - 00520/00647 - Loss: 0.05422. [121 s]\n",
      "Epoch: 21 - 00540/00647 - Loss: 0.05218. [126 s]\n",
      "Epoch: 21 - 00560/00647 - Loss: 0.06318. [131 s]\n",
      "Epoch: 21 - 00580/00647 - Loss: 0.07723. [135 s]\n",
      "Epoch: 21 - 00600/00647 - Loss: 0.04604. [140 s]\n",
      "Epoch: 21 - 00620/00647 - Loss: 0.02828. [145 s]\n",
      "Epoch: 21 - 00640/00647 - Loss: 0.03665. [149 s]\n",
      "Epoch: 21 - loss(trn/val):0.04298/0.11678, acc(val):95.96%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 22 - 00020/00647 - Loss: 0.03527. [  5 s]\n",
      "Epoch: 22 - 00040/00647 - Loss: 0.02115. [  9 s]\n",
      "Epoch: 22 - 00060/00647 - Loss: 0.04578. [ 14 s]\n",
      "Epoch: 22 - 00080/00647 - Loss: 0.04041. [ 18 s]\n",
      "Epoch: 22 - 00100/00647 - Loss: 0.05228. [ 23 s]\n",
      "Epoch: 22 - 00120/00647 - Loss: 0.02096. [ 28 s]\n",
      "Epoch: 22 - 00140/00647 - Loss: 0.03341. [ 32 s]\n",
      "Epoch: 22 - 00160/00647 - Loss: 0.04449. [ 37 s]\n",
      "Epoch: 22 - 00180/00647 - Loss: 0.04058. [ 42 s]\n",
      "Epoch: 22 - 00200/00647 - Loss: 0.04586. [ 46 s]\n",
      "Epoch: 22 - 00220/00647 - Loss: 0.03479. [ 51 s]\n",
      "Epoch: 22 - 00240/00647 - Loss: 0.03818. [ 56 s]\n",
      "Epoch: 22 - 00260/00647 - Loss: 0.05804. [ 60 s]\n",
      "Epoch: 22 - 00280/00647 - Loss: 0.03547. [ 65 s]\n",
      "Epoch: 22 - 00300/00647 - Loss: 0.05789. [ 70 s]\n",
      "Epoch: 22 - 00320/00647 - Loss: 0.00915. [ 74 s]\n",
      "Epoch: 22 - 00340/00647 - Loss: 0.04120. [ 79 s]\n",
      "Epoch: 22 - 00360/00647 - Loss: 0.05922. [ 84 s]\n",
      "Epoch: 22 - 00380/00647 - Loss: 0.01487. [ 88 s]\n",
      "Epoch: 22 - 00400/00647 - Loss: 0.04559. [ 93 s]\n",
      "Epoch: 22 - 00420/00647 - Loss: 0.02760. [ 98 s]\n",
      "Epoch: 22 - 00440/00647 - Loss: 0.03707. [102 s]\n",
      "Epoch: 22 - 00460/00647 - Loss: 0.02467. [107 s]\n",
      "Epoch: 22 - 00480/00647 - Loss: 0.03185. [112 s]\n",
      "Epoch: 22 - 00500/00647 - Loss: 0.04395. [116 s]\n",
      "Epoch: 22 - 00520/00647 - Loss: 0.05549. [121 s]\n",
      "Epoch: 22 - 00540/00647 - Loss: 0.04598. [126 s]\n",
      "Epoch: 22 - 00560/00647 - Loss: 0.08702. [130 s]\n",
      "Epoch: 22 - 00580/00647 - Loss: 0.02509. [135 s]\n",
      "Epoch: 22 - 00600/00647 - Loss: 0.07875. [140 s]\n",
      "Epoch: 22 - 00620/00647 - Loss: 0.03147. [144 s]\n",
      "Epoch: 22 - 00640/00647 - Loss: 0.03807. [149 s]\n",
      "Epoch: 22 - loss(trn/val):0.03981/0.10891, acc(val):96.26%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 23 - 00020/00647 - Loss: 0.05787. [  5 s]\n",
      "Epoch: 23 - 00040/00647 - Loss: 0.04418. [  9 s]\n",
      "Epoch: 23 - 00060/00647 - Loss: 0.03181. [ 14 s]\n",
      "Epoch: 23 - 00080/00647 - Loss: 0.06197. [ 18 s]\n",
      "Epoch: 23 - 00100/00647 - Loss: 0.38515. [ 23 s]\n",
      "Epoch: 23 - 00120/00647 - Loss: 0.08272. [ 27 s]\n",
      "Epoch: 23 - 00140/00647 - Loss: 0.04551. [ 32 s]\n",
      "Epoch: 23 - 00160/00647 - Loss: 0.03665. [ 37 s]\n",
      "Epoch: 23 - 00180/00647 - Loss: 0.03957. [ 41 s]\n",
      "Epoch: 23 - 00200/00647 - Loss: 0.04733. [ 46 s]\n",
      "Epoch: 23 - 00220/00647 - Loss: 0.03202. [ 50 s]\n",
      "Epoch: 23 - 00240/00647 - Loss: 0.03224. [ 55 s]\n",
      "Epoch: 23 - 00260/00647 - Loss: 0.04091. [ 60 s]\n",
      "Epoch: 23 - 00280/00647 - Loss: 0.04918. [ 64 s]\n",
      "Epoch: 23 - 00300/00647 - Loss: 0.02759. [ 69 s]\n",
      "Epoch: 23 - 00320/00647 - Loss: 0.02763. [ 74 s]\n",
      "Epoch: 23 - 00340/00647 - Loss: 0.02834. [ 78 s]\n",
      "Epoch: 23 - 00360/00647 - Loss: 0.04753. [ 83 s]\n",
      "Epoch: 23 - 00380/00647 - Loss: 0.04727. [ 87 s]\n",
      "Epoch: 23 - 00400/00647 - Loss: 0.03048. [ 92 s]\n",
      "Epoch: 23 - 00420/00647 - Loss: 0.03374. [ 97 s]\n",
      "Epoch: 23 - 00440/00647 - Loss: 0.02991. [101 s]\n",
      "Epoch: 23 - 00460/00647 - Loss: 0.11608. [106 s]\n",
      "Epoch: 23 - 00480/00647 - Loss: 0.01893. [111 s]\n",
      "Epoch: 23 - 00500/00647 - Loss: 0.05335. [115 s]\n",
      "Epoch: 23 - 00520/00647 - Loss: 0.04175. [120 s]\n",
      "Epoch: 23 - 00540/00647 - Loss: 0.09222. [125 s]\n",
      "Epoch: 23 - 00560/00647 - Loss: 0.03808. [129 s]\n",
      "Epoch: 23 - 00580/00647 - Loss: 0.04225. [134 s]\n",
      "Epoch: 23 - 00600/00647 - Loss: 0.03851. [138 s]\n",
      "Epoch: 23 - 00620/00647 - Loss: 0.01753. [143 s]\n",
      "Epoch: 23 - 00640/00647 - Loss: 0.03063. [148 s]\n",
      "Epoch: 23 - loss(trn/val):0.03710/0.14009, acc(val):95.58%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 24 - 00020/00647 - Loss: 0.02700. [  4 s]\n",
      "Epoch: 24 - 00040/00647 - Loss: 0.03660. [  9 s]\n",
      "Epoch: 24 - 00060/00647 - Loss: 0.02995. [ 14 s]\n",
      "Epoch: 24 - 00080/00647 - Loss: 0.03052. [ 18 s]\n",
      "Epoch: 24 - 00100/00647 - Loss: 0.02201. [ 23 s]\n",
      "Epoch: 24 - 00120/00647 - Loss: 0.04829. [ 27 s]\n",
      "Epoch: 24 - 00140/00647 - Loss: 0.03793. [ 32 s]\n",
      "Epoch: 24 - 00160/00647 - Loss: 0.02449. [ 36 s]\n",
      "Epoch: 24 - 00180/00647 - Loss: 0.02406. [ 41 s]\n",
      "Epoch: 24 - 00200/00647 - Loss: 0.04304. [ 46 s]\n",
      "Epoch: 24 - 00220/00647 - Loss: 0.02837. [ 50 s]\n",
      "Epoch: 24 - 00240/00647 - Loss: 0.02678. [ 55 s]\n",
      "Epoch: 24 - 00260/00647 - Loss: 0.10124. [ 60 s]\n",
      "Epoch: 24 - 00280/00647 - Loss: 0.20156. [ 64 s]\n",
      "Epoch: 24 - 00300/00647 - Loss: 0.05772. [ 69 s]\n",
      "Epoch: 24 - 00320/00647 - Loss: 0.01680. [ 74 s]\n",
      "Epoch: 24 - 00340/00647 - Loss: 0.05864. [ 78 s]\n",
      "Epoch: 24 - 00360/00647 - Loss: 0.03887. [ 83 s]\n",
      "Epoch: 24 - 00380/00647 - Loss: 0.02911. [ 88 s]\n",
      "Epoch: 24 - 00400/00647 - Loss: 0.03341. [ 92 s]\n",
      "Epoch: 24 - 00420/00647 - Loss: 0.05178. [ 97 s]\n",
      "Epoch: 24 - 00440/00647 - Loss: 0.01906. [101 s]\n",
      "Epoch: 24 - 00460/00647 - Loss: 0.04389. [106 s]\n",
      "Epoch: 24 - 00480/00647 - Loss: 0.03809. [111 s]\n",
      "Epoch: 24 - 00500/00647 - Loss: 0.03406. [115 s]\n",
      "Epoch: 24 - 00520/00647 - Loss: 0.06232. [120 s]\n",
      "Epoch: 24 - 00540/00647 - Loss: 0.03956. [125 s]\n",
      "Epoch: 24 - 00560/00647 - Loss: 0.07051. [129 s]\n",
      "Epoch: 24 - 00580/00647 - Loss: 0.06602. [134 s]\n",
      "Epoch: 24 - 00600/00647 - Loss: 0.03802. [139 s]\n",
      "Epoch: 24 - 00620/00647 - Loss: 0.03581. [143 s]\n",
      "Epoch: 24 - 00640/00647 - Loss: 0.01006. [148 s]\n",
      "Epoch: 24 - loss(trn/val):0.04074/0.16294, acc(val):95.20%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 25 - 00020/00647 - Loss: 0.04523. [  4 s]\n",
      "Epoch: 25 - 00040/00647 - Loss: 0.01932. [  9 s]\n",
      "Epoch: 25 - 00060/00647 - Loss: 0.05631. [ 14 s]\n",
      "Epoch: 25 - 00080/00647 - Loss: 0.01315. [ 18 s]\n",
      "Epoch: 25 - 00100/00647 - Loss: 0.10006. [ 23 s]\n",
      "Epoch: 25 - 00120/00647 - Loss: 0.04319. [ 27 s]\n",
      "Epoch: 25 - 00140/00647 - Loss: 0.02676. [ 32 s]\n",
      "Epoch: 25 - 00160/00647 - Loss: 0.04966. [ 37 s]\n",
      "Epoch: 25 - 00180/00647 - Loss: 0.03157. [ 41 s]\n",
      "Epoch: 25 - 00200/00647 - Loss: 0.02580. [ 46 s]\n",
      "Epoch: 25 - 00220/00647 - Loss: 0.03125. [ 51 s]\n",
      "Epoch: 25 - 00240/00647 - Loss: 0.01917. [ 55 s]\n",
      "Epoch: 25 - 00260/00647 - Loss: 0.04381. [ 60 s]\n",
      "Epoch: 25 - 00280/00647 - Loss: 0.03084. [ 65 s]\n",
      "Epoch: 25 - 00300/00647 - Loss: 0.02900. [ 69 s]\n",
      "Epoch: 25 - 00320/00647 - Loss: 0.06190. [ 74 s]\n",
      "Epoch: 25 - 00340/00647 - Loss: 0.08953. [ 78 s]\n",
      "Epoch: 25 - 00360/00647 - Loss: 0.04573. [ 83 s]\n",
      "Epoch: 25 - 00380/00647 - Loss: 0.04341. [ 88 s]\n",
      "Epoch: 25 - 00400/00647 - Loss: 0.05964. [ 92 s]\n",
      "Epoch: 25 - 00420/00647 - Loss: 0.07806. [ 97 s]\n",
      "Epoch: 25 - 00440/00647 - Loss: 0.06172. [102 s]\n",
      "Epoch: 25 - 00460/00647 - Loss: 0.05120. [106 s]\n",
      "Epoch: 25 - 00480/00647 - Loss: 0.06495. [111 s]\n",
      "Epoch: 25 - 00500/00647 - Loss: 0.01697. [116 s]\n",
      "Epoch: 25 - 00520/00647 - Loss: 0.15010. [120 s]\n",
      "Epoch: 25 - 00540/00647 - Loss: 0.03004. [125 s]\n",
      "Epoch: 25 - 00560/00647 - Loss: 0.15226. [129 s]\n",
      "Epoch: 25 - 00580/00647 - Loss: 0.02919. [134 s]\n",
      "Epoch: 25 - 00600/00647 - Loss: 0.08763. [139 s]\n",
      "Epoch: 25 - 00620/00647 - Loss: 0.03616. [143 s]\n",
      "Epoch: 25 - 00640/00647 - Loss: 0.01521. [148 s]\n",
      "Epoch: 25 - loss(trn/val):0.03553/0.18045, acc(val):94.19%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 26 - 00020/00647 - Loss: 0.02740. [  5 s]\n",
      "Epoch: 26 - 00040/00647 - Loss: 0.04301. [  9 s]\n",
      "Epoch: 26 - 00060/00647 - Loss: 0.02710. [ 14 s]\n",
      "Epoch: 26 - 00080/00647 - Loss: 0.01367. [ 18 s]\n",
      "Epoch: 26 - 00100/00647 - Loss: 0.02419. [ 23 s]\n",
      "Epoch: 26 - 00120/00647 - Loss: 0.02983. [ 27 s]\n",
      "Epoch: 26 - 00140/00647 - Loss: 0.01493. [ 32 s]\n",
      "Epoch: 26 - 00160/00647 - Loss: 0.05017. [ 37 s]\n",
      "Epoch: 26 - 00180/00647 - Loss: 0.05468. [ 41 s]\n",
      "Epoch: 26 - 00200/00647 - Loss: 0.01657. [ 46 s]\n",
      "Epoch: 26 - 00220/00647 - Loss: 0.03683. [ 51 s]\n",
      "Epoch: 26 - 00240/00647 - Loss: 0.03043. [ 55 s]\n",
      "Epoch: 26 - 00260/00647 - Loss: 0.07729. [ 60 s]\n",
      "Epoch: 26 - 00280/00647 - Loss: 0.03176. [ 64 s]\n",
      "Epoch: 26 - 00300/00647 - Loss: 0.02657. [ 69 s]\n",
      "Epoch: 26 - 00320/00647 - Loss: 0.03812. [ 74 s]\n",
      "Epoch: 26 - 00340/00647 - Loss: 0.08016. [ 78 s]\n",
      "Epoch: 26 - 00360/00647 - Loss: 0.02444. [ 83 s]\n",
      "Epoch: 26 - 00380/00647 - Loss: 0.02995. [ 88 s]\n",
      "Epoch: 26 - 00400/00647 - Loss: 0.03964. [ 92 s]\n",
      "Epoch: 26 - 00420/00647 - Loss: 0.04156. [ 97 s]\n",
      "Epoch: 26 - 00440/00647 - Loss: 0.02152. [101 s]\n",
      "Epoch: 26 - 00460/00647 - Loss: 0.01447. [106 s]\n",
      "Epoch: 26 - 00480/00647 - Loss: 0.04031. [111 s]\n",
      "Epoch: 26 - 00500/00647 - Loss: 0.08544. [115 s]\n",
      "Epoch: 26 - 00520/00647 - Loss: 0.05926. [120 s]\n",
      "Epoch: 26 - 00540/00647 - Loss: 0.03643. [125 s]\n",
      "Epoch: 26 - 00560/00647 - Loss: 0.03781. [129 s]\n",
      "Epoch: 26 - 00580/00647 - Loss: 0.06262. [134 s]\n",
      "Epoch: 26 - 00600/00647 - Loss: 0.03601. [139 s]\n",
      "Epoch: 26 - 00620/00647 - Loss: 0.02592. [143 s]\n",
      "Epoch: 26 - 00640/00647 - Loss: 0.04856. [148 s]\n",
      "Epoch: 26 - loss(trn/val):0.03598/0.13402, acc(val):95.71%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 27 - 00020/00647 - Loss: 0.06409. [  5 s]\n",
      "Epoch: 27 - 00040/00647 - Loss: 0.03129. [  9 s]\n",
      "Epoch: 27 - 00060/00647 - Loss: 0.02785. [ 14 s]\n",
      "Epoch: 27 - 00080/00647 - Loss: 0.03370. [ 18 s]\n",
      "Epoch: 27 - 00100/00647 - Loss: 0.08396. [ 23 s]\n",
      "Epoch: 27 - 00120/00647 - Loss: 0.02768. [ 27 s]\n",
      "Epoch: 27 - 00140/00647 - Loss: 0.05567. [ 32 s]\n",
      "Epoch: 27 - 00160/00647 - Loss: 0.00971. [ 37 s]\n",
      "Epoch: 27 - 00180/00647 - Loss: 0.02281. [ 41 s]\n",
      "Epoch: 27 - 00200/00647 - Loss: 0.05440. [ 46 s]\n",
      "Epoch: 27 - 00220/00647 - Loss: 0.03828. [ 51 s]\n",
      "Epoch: 27 - 00240/00647 - Loss: 0.02944. [ 55 s]\n",
      "Epoch: 27 - 00260/00647 - Loss: 0.03797. [ 60 s]\n",
      "Epoch: 27 - 00280/00647 - Loss: 0.06668. [ 64 s]\n",
      "Epoch: 27 - 00300/00647 - Loss: 0.02790. [ 69 s]\n",
      "Epoch: 27 - 00320/00647 - Loss: 0.03859. [ 74 s]\n",
      "Epoch: 27 - 00340/00647 - Loss: 0.03128. [ 78 s]\n",
      "Epoch: 27 - 00360/00647 - Loss: 0.03887. [ 83 s]\n",
      "Epoch: 27 - 00380/00647 - Loss: 0.02057. [ 88 s]\n",
      "Epoch: 27 - 00400/00647 - Loss: 0.06631. [ 92 s]\n",
      "Epoch: 27 - 00420/00647 - Loss: 0.06359. [ 97 s]\n",
      "Epoch: 27 - 00440/00647 - Loss: 0.03683. [102 s]\n",
      "Epoch: 27 - 00460/00647 - Loss: 0.08454. [106 s]\n",
      "Epoch: 27 - 00480/00647 - Loss: 0.03265. [111 s]\n",
      "Epoch: 27 - 00500/00647 - Loss: 0.05871. [115 s]\n",
      "Epoch: 27 - 00520/00647 - Loss: 0.01796. [120 s]\n",
      "Epoch: 27 - 00540/00647 - Loss: 0.05175. [125 s]\n",
      "Epoch: 27 - 00560/00647 - Loss: 0.02963. [129 s]\n",
      "Epoch: 27 - 00580/00647 - Loss: 0.06762. [134 s]\n",
      "Epoch: 27 - 00600/00647 - Loss: 0.02009. [139 s]\n",
      "Epoch: 27 - 00620/00647 - Loss: 0.02177. [143 s]\n",
      "Epoch: 27 - 00640/00647 - Loss: 0.07438. [148 s]\n",
      "Epoch: 27 - loss(trn/val):0.03627/0.16922, acc(val):95.35%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 28 - 00020/00647 - Loss: 0.04607. [  4 s]\n",
      "Epoch: 28 - 00040/00647 - Loss: 0.04279. [  9 s]\n",
      "Epoch: 28 - 00060/00647 - Loss: 0.02629. [ 14 s]\n",
      "Epoch: 28 - 00080/00647 - Loss: 0.05648. [ 18 s]\n",
      "Epoch: 28 - 00100/00647 - Loss: 0.03853. [ 23 s]\n",
      "Epoch: 28 - 00120/00647 - Loss: 0.02759. [ 27 s]\n",
      "Epoch: 28 - 00140/00647 - Loss: 0.01980. [ 32 s]\n",
      "Epoch: 28 - 00160/00647 - Loss: 0.02544. [ 37 s]\n",
      "Epoch: 28 - 00180/00647 - Loss: 0.05472. [ 41 s]\n",
      "Epoch: 28 - 00200/00647 - Loss: 0.07674. [ 46 s]\n",
      "Epoch: 28 - 00220/00647 - Loss: 0.02244. [ 50 s]\n",
      "Epoch: 28 - 00240/00647 - Loss: 0.05002. [ 55 s]\n",
      "Epoch: 28 - 00260/00647 - Loss: 0.01940. [ 60 s]\n",
      "Epoch: 28 - 00280/00647 - Loss: 0.03270. [ 64 s]\n",
      "Epoch: 28 - 00300/00647 - Loss: 0.03460. [ 69 s]\n",
      "Epoch: 28 - 00320/00647 - Loss: 0.05054. [ 74 s]\n",
      "Epoch: 28 - 00340/00647 - Loss: 0.02107. [ 78 s]\n",
      "Epoch: 28 - 00360/00647 - Loss: 0.01501. [ 83 s]\n",
      "Epoch: 28 - 00380/00647 - Loss: 0.04234. [ 88 s]\n",
      "Epoch: 28 - 00400/00647 - Loss: 0.02253. [ 92 s]\n",
      "Epoch: 28 - 00420/00647 - Loss: 0.03988. [ 97 s]\n",
      "Epoch: 28 - 00440/00647 - Loss: 0.02812. [101 s]\n",
      "Epoch: 28 - 00460/00647 - Loss: 0.02620. [106 s]\n",
      "Epoch: 28 - 00480/00647 - Loss: 0.09800. [111 s]\n",
      "Epoch: 28 - 00500/00647 - Loss: 0.02762. [115 s]\n",
      "Epoch: 28 - 00520/00647 - Loss: 0.03222. [120 s]\n",
      "Epoch: 28 - 00540/00647 - Loss: 0.04213. [125 s]\n",
      "Epoch: 28 - 00560/00647 - Loss: 0.03159. [129 s]\n",
      "Epoch: 28 - 00580/00647 - Loss: 0.01155. [134 s]\n",
      "Epoch: 28 - 00600/00647 - Loss: 0.04067. [139 s]\n",
      "Epoch: 28 - 00620/00647 - Loss: 0.04418. [143 s]\n",
      "Epoch: 28 - 00640/00647 - Loss: 0.10375. [148 s]\n",
      "Epoch: 28 - loss(trn/val):0.04284/0.16986, acc(val):94.67%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 29 - 00020/00647 - Loss: 0.01603. [  5 s]\n",
      "Epoch: 29 - 00040/00647 - Loss: 0.04588. [  9 s]\n",
      "Epoch: 29 - 00060/00647 - Loss: 0.03127. [ 14 s]\n",
      "Epoch: 29 - 00080/00647 - Loss: 0.03184. [ 18 s]\n",
      "Epoch: 29 - 00100/00647 - Loss: 0.02755. [ 23 s]\n",
      "Epoch: 29 - 00120/00647 - Loss: 0.04719. [ 27 s]\n",
      "Epoch: 29 - 00140/00647 - Loss: 0.03368. [ 32 s]\n",
      "Epoch: 29 - 00160/00647 - Loss: 0.04529. [ 37 s]\n",
      "Epoch: 29 - 00180/00647 - Loss: 0.01787. [ 41 s]\n",
      "Epoch: 29 - 00200/00647 - Loss: 0.01946. [ 46 s]\n",
      "Epoch: 29 - 00220/00647 - Loss: 0.03505. [ 51 s]\n",
      "Epoch: 29 - 00240/00647 - Loss: 0.02754. [ 55 s]\n",
      "Epoch: 29 - 00260/00647 - Loss: 0.02668. [ 60 s]\n",
      "Epoch: 29 - 00280/00647 - Loss: 0.02326. [ 65 s]\n",
      "Epoch: 29 - 00300/00647 - Loss: 0.03954. [ 69 s]\n",
      "Epoch: 29 - 00320/00647 - Loss: 0.02721. [ 74 s]\n",
      "Epoch: 29 - 00340/00647 - Loss: 0.01710. [ 79 s]\n",
      "Epoch: 29 - 00360/00647 - Loss: 0.02513. [ 83 s]\n",
      "Epoch: 29 - 00380/00647 - Loss: 0.04430. [ 88 s]\n",
      "Epoch: 29 - 00400/00647 - Loss: 0.04583. [ 92 s]\n",
      "Epoch: 29 - 00420/00647 - Loss: 0.02919. [ 97 s]\n",
      "Epoch: 29 - 00440/00647 - Loss: 0.03906. [102 s]\n",
      "Epoch: 29 - 00460/00647 - Loss: 0.03336. [106 s]\n",
      "Epoch: 29 - 00480/00647 - Loss: 0.03227. [111 s]\n",
      "Epoch: 29 - 00500/00647 - Loss: 0.04732. [116 s]\n",
      "Epoch: 29 - 00520/00647 - Loss: 0.02664. [120 s]\n",
      "Epoch: 29 - 00540/00647 - Loss: 0.03201. [125 s]\n",
      "Epoch: 29 - 00560/00647 - Loss: 0.04899. [130 s]\n",
      "Epoch: 29 - 00580/00647 - Loss: 0.03749. [134 s]\n",
      "Epoch: 29 - 00600/00647 - Loss: 0.02769. [139 s]\n",
      "Epoch: 29 - 00620/00647 - Loss: 0.04011. [144 s]\n",
      "Epoch: 29 - 00640/00647 - Loss: 0.01698. [148 s]\n",
      "Epoch: 29 - loss(trn/val):0.03205/0.14651, acc(val):95.32%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 30 - 00020/00647 - Loss: 0.02823. [  5 s]\n",
      "Epoch: 30 - 00040/00647 - Loss: 0.01955. [  9 s]\n",
      "Epoch: 30 - 00060/00647 - Loss: 0.02213. [ 14 s]\n",
      "Epoch: 30 - 00080/00647 - Loss: 0.05035. [ 18 s]\n",
      "Epoch: 30 - 00100/00647 - Loss: 0.08114. [ 23 s]\n",
      "Epoch: 30 - 00120/00647 - Loss: 0.02900. [ 27 s]\n",
      "Epoch: 30 - 00140/00647 - Loss: 0.03698. [ 32 s]\n",
      "Epoch: 30 - 00160/00647 - Loss: 0.02768. [ 37 s]\n",
      "Epoch: 30 - 00180/00647 - Loss: 0.04330. [ 41 s]\n",
      "Epoch: 30 - 00200/00647 - Loss: 0.03612. [ 46 s]\n",
      "Epoch: 30 - 00220/00647 - Loss: 0.02853. [ 51 s]\n",
      "Epoch: 30 - 00240/00647 - Loss: 0.04960. [ 55 s]\n",
      "Epoch: 30 - 00260/00647 - Loss: 0.03336. [ 60 s]\n",
      "Epoch: 30 - 00280/00647 - Loss: 0.03710. [ 64 s]\n",
      "Epoch: 30 - 00300/00647 - Loss: 0.01586. [ 69 s]\n",
      "Epoch: 30 - 00320/00647 - Loss: 0.04291. [ 74 s]\n",
      "Epoch: 30 - 00340/00647 - Loss: 0.03474. [ 78 s]\n",
      "Epoch: 30 - 00360/00647 - Loss: 0.01916. [ 83 s]\n",
      "Epoch: 30 - 00380/00647 - Loss: 0.02276. [ 88 s]\n",
      "Epoch: 30 - 00400/00647 - Loss: 0.04933. [ 92 s]\n",
      "Epoch: 30 - 00420/00647 - Loss: 0.01521. [ 97 s]\n",
      "Epoch: 30 - 00440/00647 - Loss: 0.05769. [102 s]\n",
      "Epoch: 30 - 00460/00647 - Loss: 0.03080. [106 s]\n",
      "Epoch: 30 - 00480/00647 - Loss: 0.02217. [111 s]\n",
      "Epoch: 30 - 00500/00647 - Loss: 0.01814. [116 s]\n",
      "Epoch: 30 - 00520/00647 - Loss: 0.01638. [120 s]\n",
      "Epoch: 30 - 00540/00647 - Loss: 0.01714. [125 s]\n",
      "Epoch: 30 - 00560/00647 - Loss: 0.03118. [129 s]\n",
      "Epoch: 30 - 00580/00647 - Loss: 0.05523. [134 s]\n",
      "Epoch: 30 - 00600/00647 - Loss: 0.03106. [139 s]\n",
      "Epoch: 30 - 00620/00647 - Loss: 0.01859. [143 s]\n",
      "Epoch: 30 - 00640/00647 - Loss: 0.03237. [148 s]\n",
      "Epoch: 30 - loss(trn/val):0.03028/0.17397, acc(val):95.09%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 31 - 00020/00647 - Loss: 0.02049. [  5 s]\n",
      "Epoch: 31 - 00040/00647 - Loss: 0.02902. [  9 s]\n",
      "Epoch: 31 - 00060/00647 - Loss: 0.03732. [ 14 s]\n",
      "Epoch: 31 - 00080/00647 - Loss: 0.05039. [ 18 s]\n",
      "Epoch: 31 - 00100/00647 - Loss: 0.03197. [ 23 s]\n",
      "Epoch: 31 - 00120/00647 - Loss: 0.03239. [ 27 s]\n",
      "Epoch: 31 - 00140/00647 - Loss: 0.01091. [ 32 s]\n",
      "Epoch: 31 - 00160/00647 - Loss: 0.02216. [ 37 s]\n",
      "Epoch: 31 - 00180/00647 - Loss: 0.02926. [ 41 s]\n",
      "Epoch: 31 - 00200/00647 - Loss: 0.65925. [ 46 s]\n",
      "Epoch: 31 - 00220/00647 - Loss: 0.08050. [ 51 s]\n",
      "Epoch: 31 - 00240/00647 - Loss: 0.03813. [ 55 s]\n",
      "Epoch: 31 - 00260/00647 - Loss: 0.02026. [ 60 s]\n",
      "Epoch: 31 - 00280/00647 - Loss: 0.02564. [ 65 s]\n",
      "Epoch: 31 - 00300/00647 - Loss: 0.03412. [ 69 s]\n",
      "Epoch: 31 - 00320/00647 - Loss: 0.03005. [ 74 s]\n",
      "Epoch: 31 - 00340/00647 - Loss: 0.00848. [ 78 s]\n",
      "Epoch: 31 - 00360/00647 - Loss: 0.03274. [ 83 s]\n",
      "Epoch: 31 - 00380/00647 - Loss: 0.05176. [ 88 s]\n",
      "Epoch: 31 - 00400/00647 - Loss: 0.02065. [ 92 s]\n",
      "Epoch: 31 - 00420/00647 - Loss: 0.02593. [ 97 s]\n",
      "Epoch: 31 - 00440/00647 - Loss: 0.04800. [102 s]\n",
      "Epoch: 31 - 00460/00647 - Loss: 0.07368. [106 s]\n",
      "Epoch: 31 - 00480/00647 - Loss: 0.04201. [111 s]\n",
      "Epoch: 31 - 00500/00647 - Loss: 0.03736. [116 s]\n",
      "Epoch: 31 - 00520/00647 - Loss: 0.03232. [120 s]\n",
      "Epoch: 31 - 00540/00647 - Loss: 0.03912. [125 s]\n",
      "Epoch: 31 - 00560/00647 - Loss: 0.02461. [130 s]\n",
      "Epoch: 31 - 00580/00647 - Loss: 0.02153. [134 s]\n",
      "Epoch: 31 - 00600/00647 - Loss: 0.04054. [139 s]\n",
      "Epoch: 31 - 00620/00647 - Loss: 0.02371. [143 s]\n",
      "Epoch: 31 - 00640/00647 - Loss: 0.02789. [148 s]\n",
      "Epoch: 31 - loss(trn/val):0.02969/0.15598, acc(val):95.26%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 32 - 00020/00647 - Loss: 0.02097. [  5 s]\n",
      "Epoch: 32 - 00040/00647 - Loss: 0.04986. [  9 s]\n",
      "Epoch: 32 - 00060/00647 - Loss: 0.03148. [ 14 s]\n",
      "Epoch: 32 - 00080/00647 - Loss: 0.02908. [ 18 s]\n",
      "Epoch: 32 - 00100/00647 - Loss: 0.02871. [ 23 s]\n",
      "Epoch: 32 - 00120/00647 - Loss: 0.01648. [ 28 s]\n",
      "Epoch: 32 - 00140/00647 - Loss: 0.02724. [ 32 s]\n",
      "Epoch: 32 - 00160/00647 - Loss: 0.02467. [ 37 s]\n",
      "Epoch: 32 - 00180/00647 - Loss: 0.01385. [ 41 s]\n",
      "Epoch: 32 - 00200/00647 - Loss: 0.04914. [ 46 s]\n",
      "Epoch: 32 - 00220/00647 - Loss: 0.06256. [ 51 s]\n",
      "Epoch: 32 - 00240/00647 - Loss: 0.05491. [ 55 s]\n",
      "Epoch: 32 - 00260/00647 - Loss: 0.02058. [ 60 s]\n",
      "Epoch: 32 - 00280/00647 - Loss: 0.02690. [ 65 s]\n",
      "Epoch: 32 - 00300/00647 - Loss: 0.02134. [ 69 s]\n",
      "Epoch: 32 - 00320/00647 - Loss: 0.01978. [ 74 s]\n",
      "Epoch: 32 - 00340/00647 - Loss: 0.06941. [ 78 s]\n",
      "Epoch: 32 - 00360/00647 - Loss: 0.05140. [ 83 s]\n",
      "Epoch: 32 - 00380/00647 - Loss: 0.08409. [ 88 s]\n",
      "Epoch: 32 - 00400/00647 - Loss: 0.05683. [ 92 s]\n",
      "Epoch: 32 - 00420/00647 - Loss: 0.01637. [ 97 s]\n",
      "Epoch: 32 - 00440/00647 - Loss: 0.02468. [102 s]\n",
      "Epoch: 32 - 00460/00647 - Loss: 0.01828. [106 s]\n",
      "Epoch: 32 - 00480/00647 - Loss: 0.03640. [111 s]\n",
      "Epoch: 32 - 00500/00647 - Loss: 0.03562. [116 s]\n",
      "Epoch: 32 - 00520/00647 - Loss: 0.03561. [120 s]\n",
      "Epoch: 32 - 00540/00647 - Loss: 0.02770. [125 s]\n",
      "Epoch: 32 - 00560/00647 - Loss: 0.02752. [130 s]\n",
      "Epoch: 32 - 00580/00647 - Loss: 0.03350. [134 s]\n",
      "Epoch: 32 - 00600/00647 - Loss: 0.02385. [139 s]\n",
      "Epoch: 32 - 00620/00647 - Loss: 0.03130. [143 s]\n",
      "Epoch: 32 - 00640/00647 - Loss: 0.06687. [148 s]\n",
      "Epoch: 32 - loss(trn/val):0.02995/0.13288, acc(val):96.04%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 33 - 00020/00647 - Loss: 0.10327. [  4 s]\n",
      "Epoch: 33 - 00040/00647 - Loss: 0.02473. [  9 s]\n",
      "Epoch: 33 - 00060/00647 - Loss: 0.02792. [ 14 s]\n",
      "Epoch: 33 - 00080/00647 - Loss: 0.02038. [ 18 s]\n",
      "Epoch: 33 - 00100/00647 - Loss: 0.40216. [ 23 s]\n",
      "Epoch: 33 - 00120/00647 - Loss: 0.04135. [ 27 s]\n",
      "Epoch: 33 - 00140/00647 - Loss: 0.02132. [ 32 s]\n",
      "Epoch: 33 - 00160/00647 - Loss: 0.02603. [ 37 s]\n",
      "Epoch: 33 - 00180/00647 - Loss: 0.03749. [ 41 s]\n",
      "Epoch: 33 - 00200/00647 - Loss: 0.01496. [ 46 s]\n",
      "Epoch: 33 - 00220/00647 - Loss: 0.26070. [ 51 s]\n",
      "Epoch: 33 - 00240/00647 - Loss: 0.01108. [ 55 s]\n",
      "Epoch: 33 - 00260/00647 - Loss: 0.02106. [ 60 s]\n",
      "Epoch: 33 - 00280/00647 - Loss: 0.03576. [ 65 s]\n",
      "Epoch: 33 - 00300/00647 - Loss: 0.01317. [ 69 s]\n",
      "Epoch: 33 - 00320/00647 - Loss: 0.06847. [ 74 s]\n",
      "Epoch: 33 - 00340/00647 - Loss: 0.02738. [ 78 s]\n",
      "Epoch: 33 - 00360/00647 - Loss: 0.03589. [ 83 s]\n",
      "Epoch: 33 - 00380/00647 - Loss: 0.03183. [ 88 s]\n",
      "Epoch: 33 - 00400/00647 - Loss: 0.03330. [ 92 s]\n",
      "Epoch: 33 - 00420/00647 - Loss: 0.01593. [ 97 s]\n",
      "Epoch: 33 - 00440/00647 - Loss: 0.03633. [102 s]\n",
      "Epoch: 33 - 00460/00647 - Loss: 0.01780. [106 s]\n",
      "Epoch: 33 - 00480/00647 - Loss: 0.02577. [111 s]\n",
      "Epoch: 33 - 00500/00647 - Loss: 0.03474. [116 s]\n",
      "Epoch: 33 - 00520/00647 - Loss: 0.03258. [120 s]\n",
      "Epoch: 33 - 00540/00647 - Loss: 0.04500. [125 s]\n",
      "Epoch: 33 - 00560/00647 - Loss: 0.05872. [130 s]\n",
      "Epoch: 33 - 00580/00647 - Loss: 0.02294. [134 s]\n",
      "Epoch: 33 - 00600/00647 - Loss: 0.05531. [139 s]\n",
      "Epoch: 33 - 00620/00647 - Loss: 0.05833. [144 s]\n",
      "Epoch: 33 - 00640/00647 - Loss: 0.05243. [148 s]\n",
      "Epoch: 33 - loss(trn/val):0.03350/0.12936, acc(val):96.27%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 34 - 00020/00647 - Loss: 0.05527. [  5 s]\n",
      "Epoch: 34 - 00040/00647 - Loss: 0.01967. [  9 s]\n",
      "Epoch: 34 - 00060/00647 - Loss: 0.06323. [ 14 s]\n",
      "Epoch: 34 - 00080/00647 - Loss: 0.02342. [ 18 s]\n",
      "Epoch: 34 - 00100/00647 - Loss: 0.01794. [ 23 s]\n",
      "Epoch: 34 - 00120/00647 - Loss: 0.02166. [ 27 s]\n",
      "Epoch: 34 - 00140/00647 - Loss: 0.02113. [ 32 s]\n",
      "Epoch: 34 - 00160/00647 - Loss: 0.02875. [ 37 s]\n",
      "Epoch: 34 - 00180/00647 - Loss: 0.11122. [ 41 s]\n",
      "Epoch: 34 - 00200/00647 - Loss: 0.03179. [ 46 s]\n",
      "Epoch: 34 - 00220/00647 - Loss: 0.03700. [ 51 s]\n",
      "Epoch: 34 - 00240/00647 - Loss: 0.04282. [ 55 s]\n",
      "Epoch: 34 - 00260/00647 - Loss: 0.02869. [ 60 s]\n",
      "Epoch: 34 - 00280/00647 - Loss: 0.07001. [ 65 s]\n",
      "Epoch: 34 - 00300/00647 - Loss: 0.06086. [ 69 s]\n",
      "Epoch: 34 - 00320/00647 - Loss: 0.03308. [ 74 s]\n",
      "Epoch: 34 - 00340/00647 - Loss: 0.03346. [ 79 s]\n",
      "Epoch: 34 - 00360/00647 - Loss: 0.03407. [ 83 s]\n",
      "Epoch: 34 - 00380/00647 - Loss: 0.03636. [ 88 s]\n",
      "Epoch: 34 - 00400/00647 - Loss: 0.03669. [ 92 s]\n",
      "Epoch: 34 - 00420/00647 - Loss: 0.01660. [ 97 s]\n",
      "Epoch: 34 - 00440/00647 - Loss: 0.03102. [102 s]\n",
      "Epoch: 34 - 00460/00647 - Loss: 0.04107. [106 s]\n",
      "Epoch: 34 - 00480/00647 - Loss: 0.05322. [111 s]\n",
      "Epoch: 34 - 00500/00647 - Loss: 0.02236. [116 s]\n",
      "Epoch: 34 - 00520/00647 - Loss: 0.05104. [120 s]\n",
      "Epoch: 34 - 00540/00647 - Loss: 0.04573. [125 s]\n",
      "Epoch: 34 - 00560/00647 - Loss: 0.04098. [130 s]\n",
      "Epoch: 34 - 00580/00647 - Loss: 0.03180. [134 s]\n",
      "Epoch: 34 - 00600/00647 - Loss: 0.03643. [139 s]\n",
      "Epoch: 34 - 00620/00647 - Loss: 0.02426. [143 s]\n",
      "Epoch: 34 - 00640/00647 - Loss: 0.02213. [148 s]\n",
      "Epoch: 34 - loss(trn/val):0.02724/0.13807, acc(val):95.80%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 35 - 00020/00647 - Loss: 0.02318. [  5 s]\n",
      "Epoch: 35 - 00040/00647 - Loss: 0.00784. [  9 s]\n",
      "Epoch: 35 - 00060/00647 - Loss: 0.03444. [ 14 s]\n",
      "Epoch: 35 - 00080/00647 - Loss: 0.01770. [ 18 s]\n",
      "Epoch: 35 - 00100/00647 - Loss: 0.04381. [ 23 s]\n",
      "Epoch: 35 - 00120/00647 - Loss: 0.04502. [ 27 s]\n",
      "Epoch: 35 - 00140/00647 - Loss: 0.02422. [ 32 s]\n",
      "Epoch: 35 - 00160/00647 - Loss: 0.01657. [ 37 s]\n",
      "Epoch: 35 - 00180/00647 - Loss: 0.03638. [ 41 s]\n",
      "Epoch: 35 - 00200/00647 - Loss: 0.09184. [ 46 s]\n",
      "Epoch: 35 - 00220/00647 - Loss: 0.04333. [ 51 s]\n",
      "Epoch: 35 - 00240/00647 - Loss: 0.01840. [ 55 s]\n",
      "Epoch: 35 - 00260/00647 - Loss: 0.00999. [ 60 s]\n",
      "Epoch: 35 - 00280/00647 - Loss: 0.03415. [ 64 s]\n",
      "Epoch: 35 - 00300/00647 - Loss: 0.03800. [ 69 s]\n",
      "Epoch: 35 - 00320/00647 - Loss: 0.05201. [ 74 s]\n",
      "Epoch: 35 - 00340/00647 - Loss: 0.03834. [ 78 s]\n",
      "Epoch: 35 - 00360/00647 - Loss: 0.10697. [ 83 s]\n",
      "Epoch: 35 - 00380/00647 - Loss: 0.03668. [ 88 s]\n",
      "Epoch: 35 - 00400/00647 - Loss: 0.01878. [ 92 s]\n",
      "Epoch: 35 - 00420/00647 - Loss: 0.03691. [ 97 s]\n",
      "Epoch: 35 - 00440/00647 - Loss: 0.03503. [102 s]\n",
      "Epoch: 35 - 00460/00647 - Loss: 0.02981. [106 s]\n",
      "Epoch: 35 - 00480/00647 - Loss: 0.04189. [111 s]\n",
      "Epoch: 35 - 00500/00647 - Loss: 0.16227. [115 s]\n",
      "Epoch: 35 - 00520/00647 - Loss: 0.02511. [120 s]\n",
      "Epoch: 35 - 00540/00647 - Loss: 0.07260. [125 s]\n",
      "Epoch: 35 - 00560/00647 - Loss: 0.01675. [129 s]\n",
      "Epoch: 35 - 00580/00647 - Loss: 0.02237. [134 s]\n",
      "Epoch: 35 - 00600/00647 - Loss: 0.01622. [139 s]\n",
      "Epoch: 35 - 00620/00647 - Loss: 0.02047. [143 s]\n",
      "Epoch: 35 - 00640/00647 - Loss: 0.02480. [148 s]\n",
      "Epoch: 35 - loss(trn/val):0.02500/0.16252, acc(val):95.58%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 36 - 00020/00647 - Loss: 0.04725. [  5 s]\n",
      "Epoch: 36 - 00040/00647 - Loss: 0.00908. [  9 s]\n",
      "Epoch: 36 - 00060/00647 - Loss: 0.02955. [ 14 s]\n",
      "Epoch: 36 - 00080/00647 - Loss: 0.03658. [ 18 s]\n",
      "Epoch: 36 - 00100/00647 - Loss: 0.17321. [ 23 s]\n",
      "Epoch: 36 - 00120/00647 - Loss: 0.02049. [ 27 s]\n",
      "Epoch: 36 - 00140/00647 - Loss: 0.03350. [ 32 s]\n",
      "Epoch: 36 - 00160/00647 - Loss: 0.04390. [ 37 s]\n",
      "Epoch: 36 - 00180/00647 - Loss: 0.04702. [ 41 s]\n",
      "Epoch: 36 - 00200/00647 - Loss: 0.01459. [ 46 s]\n",
      "Epoch: 36 - 00220/00647 - Loss: 0.02783. [ 51 s]\n",
      "Epoch: 36 - 00240/00647 - Loss: 0.05134. [ 55 s]\n",
      "Epoch: 36 - 00260/00647 - Loss: 0.03106. [ 60 s]\n",
      "Epoch: 36 - 00280/00647 - Loss: 0.01692. [ 64 s]\n",
      "Epoch: 36 - 00300/00647 - Loss: 0.02518. [ 69 s]\n",
      "Epoch: 36 - 00320/00647 - Loss: 0.03148. [ 74 s]\n",
      "Epoch: 36 - 00340/00647 - Loss: 0.02174. [ 78 s]\n",
      "Epoch: 36 - 00360/00647 - Loss: 0.04251. [ 83 s]\n",
      "Epoch: 36 - 00380/00647 - Loss: 0.05397. [ 88 s]\n",
      "Epoch: 36 - 00400/00647 - Loss: 0.05622. [ 92 s]\n",
      "Epoch: 36 - 00420/00647 - Loss: 0.02286. [ 97 s]\n",
      "Epoch: 36 - 00440/00647 - Loss: 0.03330. [102 s]\n",
      "Epoch: 36 - 00460/00647 - Loss: 0.02462. [106 s]\n",
      "Epoch: 36 - 00480/00647 - Loss: 0.03670. [111 s]\n",
      "Epoch: 36 - 00500/00647 - Loss: 0.02132. [115 s]\n",
      "Epoch: 36 - 00520/00647 - Loss: 0.09944. [120 s]\n",
      "Epoch: 36 - 00540/00647 - Loss: 0.03570. [125 s]\n",
      "Epoch: 36 - 00560/00647 - Loss: 0.02193. [129 s]\n",
      "Epoch: 36 - 00580/00647 - Loss: 0.03601. [134 s]\n",
      "Epoch: 36 - 00600/00647 - Loss: 0.02724. [139 s]\n",
      "Epoch: 36 - 00620/00647 - Loss: 0.04805. [143 s]\n",
      "Epoch: 36 - 00640/00647 - Loss: 0.04218. [148 s]\n",
      "Epoch: 36 - loss(trn/val):0.03058/0.16564, acc(val):95.11%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 37 - 00020/00647 - Loss: 0.03200. [  5 s]\n",
      "Epoch: 37 - 00040/00647 - Loss: 0.02320. [  9 s]\n",
      "Epoch: 37 - 00060/00647 - Loss: 0.05680. [ 14 s]\n",
      "Epoch: 37 - 00080/00647 - Loss: 0.02861. [ 18 s]\n",
      "Epoch: 37 - 00100/00647 - Loss: 0.02531. [ 23 s]\n",
      "Epoch: 37 - 00120/00647 - Loss: 0.02111. [ 27 s]\n",
      "Epoch: 37 - 00140/00647 - Loss: 0.01732. [ 32 s]\n",
      "Epoch: 37 - 00160/00647 - Loss: 0.02185. [ 37 s]\n",
      "Epoch: 37 - 00180/00647 - Loss: 0.02567. [ 41 s]\n",
      "Epoch: 37 - 00200/00647 - Loss: 0.02310. [ 46 s]\n",
      "Epoch: 37 - 00220/00647 - Loss: 0.02773. [ 51 s]\n",
      "Epoch: 37 - 00240/00647 - Loss: 0.03088. [ 55 s]\n",
      "Epoch: 37 - 00260/00647 - Loss: 0.03164. [ 60 s]\n",
      "Epoch: 37 - 00280/00647 - Loss: 0.01691. [ 64 s]\n",
      "Epoch: 37 - 00300/00647 - Loss: 0.02409. [ 69 s]\n",
      "Epoch: 37 - 00320/00647 - Loss: 0.02050. [ 74 s]\n",
      "Epoch: 37 - 00340/00647 - Loss: 0.02045. [ 78 s]\n",
      "Epoch: 37 - 00360/00647 - Loss: 0.02542. [ 83 s]\n",
      "Epoch: 37 - 00380/00647 - Loss: 0.02978. [ 88 s]\n",
      "Epoch: 37 - 00400/00647 - Loss: 0.02001. [ 93 s]\n",
      "Epoch: 37 - 00420/00647 - Loss: 0.02859. [ 97 s]\n",
      "Epoch: 37 - 00440/00647 - Loss: 0.04776. [102 s]\n",
      "Epoch: 37 - 00460/00647 - Loss: 0.02792. [107 s]\n",
      "Epoch: 37 - 00480/00647 - Loss: 0.05380. [111 s]\n",
      "Epoch: 37 - 00500/00647 - Loss: 0.11077. [116 s]\n",
      "Epoch: 37 - 00520/00647 - Loss: 0.03938. [121 s]\n",
      "Epoch: 37 - 00540/00647 - Loss: 0.01996. [126 s]\n",
      "Epoch: 37 - 00560/00647 - Loss: 0.02591. [130 s]\n",
      "Epoch: 37 - 00580/00647 - Loss: 0.03936. [135 s]\n",
      "Epoch: 37 - 00600/00647 - Loss: 0.01334. [140 s]\n",
      "Epoch: 37 - 00620/00647 - Loss: 0.01329. [144 s]\n",
      "Epoch: 37 - 00640/00647 - Loss: 0.01668. [149 s]\n",
      "Epoch: 37 - loss(trn/val):0.02883/0.12938, acc(val):96.23%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 38 - 00020/00647 - Loss: 0.01863. [  5 s]\n",
      "Epoch: 38 - 00040/00647 - Loss: 0.01735. [  9 s]\n",
      "Epoch: 38 - 00060/00647 - Loss: 0.01660. [ 14 s]\n",
      "Epoch: 38 - 00080/00647 - Loss: 0.05842. [ 19 s]\n",
      "Epoch: 38 - 00100/00647 - Loss: 0.02455. [ 23 s]\n",
      "Epoch: 38 - 00120/00647 - Loss: 0.02454. [ 28 s]\n",
      "Epoch: 38 - 00140/00647 - Loss: 0.02052. [ 33 s]\n",
      "Epoch: 38 - 00160/00647 - Loss: 0.02980. [ 37 s]\n",
      "Epoch: 38 - 00180/00647 - Loss: 0.02182. [ 42 s]\n",
      "Epoch: 38 - 00200/00647 - Loss: 0.04571. [ 47 s]\n",
      "Epoch: 38 - 00220/00647 - Loss: 0.01560. [ 51 s]\n",
      "Epoch: 38 - 00240/00647 - Loss: 0.02627. [ 56 s]\n",
      "Epoch: 38 - 00260/00647 - Loss: 0.04522. [ 61 s]\n",
      "Epoch: 38 - 00280/00647 - Loss: 0.23510. [ 66 s]\n",
      "Epoch: 38 - 00300/00647 - Loss: 0.02922. [ 70 s]\n",
      "Epoch: 38 - 00320/00647 - Loss: 0.03221. [ 75 s]\n",
      "Epoch: 38 - 00340/00647 - Loss: 0.03649. [ 80 s]\n",
      "Epoch: 38 - 00360/00647 - Loss: 0.03550. [ 84 s]\n",
      "Epoch: 38 - 00380/00647 - Loss: 0.03468. [ 89 s]\n",
      "Epoch: 38 - 00400/00647 - Loss: 0.01823. [ 94 s]\n",
      "Epoch: 38 - 00420/00647 - Loss: 0.02404. [ 99 s]\n",
      "Epoch: 38 - 00440/00647 - Loss: 0.02518. [103 s]\n",
      "Epoch: 38 - 00460/00647 - Loss: 0.03945. [108 s]\n",
      "Epoch: 38 - 00480/00647 - Loss: 0.01570. [113 s]\n",
      "Epoch: 38 - 00500/00647 - Loss: 0.02262. [117 s]\n",
      "Epoch: 38 - 00520/00647 - Loss: 0.00969. [122 s]\n",
      "Epoch: 38 - 00540/00647 - Loss: 0.02536. [127 s]\n",
      "Epoch: 38 - 00560/00647 - Loss: 0.02142. [132 s]\n",
      "Epoch: 38 - 00580/00647 - Loss: 0.01998. [136 s]\n",
      "Epoch: 38 - 00600/00647 - Loss: 0.02191. [141 s]\n",
      "Epoch: 38 - 00620/00647 - Loss: 0.02191. [146 s]\n",
      "Epoch: 38 - 00640/00647 - Loss: 0.00589. [150 s]\n",
      "Epoch: 38 - loss(trn/val):0.03745/0.17129, acc(val):94.81%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 39 - 00020/00647 - Loss: 0.05108. [  5 s]\n",
      "Epoch: 39 - 00040/00647 - Loss: 0.04057. [  9 s]\n",
      "Epoch: 39 - 00060/00647 - Loss: 0.02947. [ 14 s]\n",
      "Epoch: 39 - 00080/00647 - Loss: 0.01825. [ 19 s]\n",
      "Epoch: 39 - 00100/00647 - Loss: 0.02242. [ 23 s]\n",
      "Epoch: 39 - 00120/00647 - Loss: 0.04504. [ 28 s]\n",
      "Epoch: 39 - 00140/00647 - Loss: 0.02064. [ 33 s]\n",
      "Epoch: 39 - 00160/00647 - Loss: 0.03415. [ 37 s]\n",
      "Epoch: 39 - 00180/00647 - Loss: 0.04643. [ 42 s]\n",
      "Epoch: 39 - 00200/00647 - Loss: 0.08143. [ 47 s]\n",
      "Epoch: 39 - 00220/00647 - Loss: 0.03187. [ 52 s]\n",
      "Epoch: 39 - 00240/00647 - Loss: 0.05010. [ 56 s]\n",
      "Epoch: 39 - 00260/00647 - Loss: 0.02073. [ 61 s]\n",
      "Epoch: 39 - 00280/00647 - Loss: 0.02132. [ 66 s]\n",
      "Epoch: 39 - 00300/00647 - Loss: 0.05410. [ 70 s]\n",
      "Epoch: 39 - 00320/00647 - Loss: 0.01241. [ 75 s]\n",
      "Epoch: 39 - 00340/00647 - Loss: 0.02070. [ 80 s]\n",
      "Epoch: 39 - 00360/00647 - Loss: 0.01923. [ 85 s]\n",
      "Epoch: 39 - 00380/00647 - Loss: 0.02167. [ 89 s]\n",
      "Epoch: 39 - 00400/00647 - Loss: 0.03187. [ 94 s]\n",
      "Epoch: 39 - 00420/00647 - Loss: 0.02662. [ 99 s]\n",
      "Epoch: 39 - 00440/00647 - Loss: 0.03397. [103 s]\n",
      "Epoch: 39 - 00460/00647 - Loss: 0.06784. [108 s]\n",
      "Epoch: 39 - 00480/00647 - Loss: 0.06789. [113 s]\n",
      "Epoch: 39 - 00500/00647 - Loss: 0.02333. [117 s]\n",
      "Epoch: 39 - 00520/00647 - Loss: 0.03882. [122 s]\n",
      "Epoch: 39 - 00540/00647 - Loss: 0.01924. [127 s]\n",
      "Epoch: 39 - 00560/00647 - Loss: 0.02005. [132 s]\n",
      "Epoch: 39 - 00580/00647 - Loss: 0.02976. [136 s]\n",
      "Epoch: 39 - 00600/00647 - Loss: 0.01762. [141 s]\n",
      "Epoch: 39 - 00620/00647 - Loss: 0.01613. [146 s]\n",
      "Epoch: 39 - 00640/00647 - Loss: 0.02623. [150 s]\n",
      "Epoch: 39 - loss(trn/val):0.02523/0.21894, acc(val):94.89%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 40 - 00020/00647 - Loss: 0.03748. [  5 s]\n",
      "Epoch: 40 - 00040/00647 - Loss: 0.03935. [  9 s]\n",
      "Epoch: 40 - 00060/00647 - Loss: 0.01150. [ 14 s]\n",
      "Epoch: 40 - 00080/00647 - Loss: 0.06092. [ 18 s]\n",
      "Epoch: 40 - 00100/00647 - Loss: 0.02344. [ 23 s]\n",
      "Epoch: 40 - 00120/00647 - Loss: 0.01022. [ 28 s]\n",
      "Epoch: 40 - 00140/00647 - Loss: 0.02401. [ 32 s]\n",
      "Epoch: 40 - 00160/00647 - Loss: 0.03910. [ 37 s]\n",
      "Epoch: 40 - 00180/00647 - Loss: 0.01330. [ 41 s]\n",
      "Epoch: 40 - 00200/00647 - Loss: 0.01345. [ 46 s]\n",
      "Epoch: 40 - 00220/00647 - Loss: 0.01536. [ 51 s]\n",
      "Epoch: 40 - 00240/00647 - Loss: 0.01463. [ 55 s]\n",
      "Epoch: 40 - 00260/00647 - Loss: 0.02972. [ 60 s]\n",
      "Epoch: 40 - 00280/00647 - Loss: 0.04286. [ 65 s]\n",
      "Epoch: 40 - 00300/00647 - Loss: 0.03684. [ 69 s]\n",
      "Epoch: 40 - 00320/00647 - Loss: 0.07910. [ 74 s]\n",
      "Epoch: 40 - 00340/00647 - Loss: 0.05279. [ 78 s]\n",
      "Epoch: 40 - 00360/00647 - Loss: 0.02881. [ 83 s]\n",
      "Epoch: 40 - 00380/00647 - Loss: 0.01878. [ 88 s]\n",
      "Epoch: 40 - 00400/00647 - Loss: 0.02456. [ 92 s]\n",
      "Epoch: 40 - 00420/00647 - Loss: 0.01652. [ 97 s]\n",
      "Epoch: 40 - 00440/00647 - Loss: 0.01859. [102 s]\n",
      "Epoch: 40 - 00460/00647 - Loss: 0.02400. [106 s]\n",
      "Epoch: 40 - 00480/00647 - Loss: 0.01644. [111 s]\n",
      "Epoch: 40 - 00500/00647 - Loss: 0.02660. [116 s]\n",
      "Epoch: 40 - 00520/00647 - Loss: 0.01801. [120 s]\n",
      "Epoch: 40 - 00540/00647 - Loss: 0.01868. [125 s]\n",
      "Epoch: 40 - 00560/00647 - Loss: 0.01674. [129 s]\n",
      "Epoch: 40 - 00580/00647 - Loss: 0.01946. [134 s]\n",
      "Epoch: 40 - 00600/00647 - Loss: 0.02928. [139 s]\n",
      "Epoch: 40 - 00620/00647 - Loss: 0.04272. [143 s]\n",
      "Epoch: 40 - 00640/00647 - Loss: 0.02077. [148 s]\n",
      "Epoch: 40 - loss(trn/val):0.02250/0.14783, acc(val):95.90%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 41 - 00020/00647 - Loss: 0.01537. [  6 s]\n",
      "Epoch: 41 - 00040/00647 - Loss: 0.03090. [ 10 s]\n",
      "Epoch: 41 - 00060/00647 - Loss: 0.01545. [ 15 s]\n",
      "Epoch: 41 - 00080/00647 - Loss: 0.03292. [ 19 s]\n",
      "Epoch: 41 - 00100/00647 - Loss: 0.04825. [ 24 s]\n",
      "Epoch: 41 - 00120/00647 - Loss: 0.03438. [ 29 s]\n",
      "Epoch: 41 - 00140/00647 - Loss: 0.01318. [ 33 s]\n",
      "Epoch: 41 - 00160/00647 - Loss: 0.02614. [ 38 s]\n",
      "Epoch: 41 - 00180/00647 - Loss: 0.02523. [ 42 s]\n",
      "Epoch: 41 - 00200/00647 - Loss: 0.02469. [ 47 s]\n",
      "Epoch: 41 - 00220/00647 - Loss: 0.02404. [ 52 s]\n",
      "Epoch: 41 - 00240/00647 - Loss: 0.03080. [ 56 s]\n",
      "Epoch: 41 - 00260/00647 - Loss: 0.01985. [ 61 s]\n",
      "Epoch: 41 - 00280/00647 - Loss: 0.02719. [ 66 s]\n",
      "Epoch: 41 - 00300/00647 - Loss: 0.02053. [ 70 s]\n",
      "Epoch: 41 - 00320/00647 - Loss: 0.00989. [ 75 s]\n",
      "Epoch: 41 - 00340/00647 - Loss: 0.02030. [ 79 s]\n",
      "Epoch: 41 - 00360/00647 - Loss: 0.01589. [ 84 s]\n",
      "Epoch: 41 - 00380/00647 - Loss: 0.07713. [ 89 s]\n",
      "Epoch: 41 - 00400/00647 - Loss: 0.02197. [ 93 s]\n",
      "Epoch: 41 - 00420/00647 - Loss: 0.10578. [ 98 s]\n",
      "Epoch: 41 - 00440/00647 - Loss: 0.01793. [103 s]\n",
      "Epoch: 41 - 00460/00647 - Loss: 0.03348. [107 s]\n",
      "Epoch: 41 - 00480/00647 - Loss: 0.01911. [112 s]\n",
      "Epoch: 41 - 00500/00647 - Loss: 0.03198. [117 s]\n",
      "Epoch: 41 - 00520/00647 - Loss: 0.02826. [121 s]\n",
      "Epoch: 41 - 00540/00647 - Loss: 0.03436. [126 s]\n",
      "Epoch: 41 - 00560/00647 - Loss: 0.02328. [130 s]\n",
      "Epoch: 41 - 00580/00647 - Loss: 0.01453. [135 s]\n",
      "Epoch: 41 - 00600/00647 - Loss: 0.02161. [140 s]\n",
      "Epoch: 41 - 00620/00647 - Loss: 0.00998. [144 s]\n",
      "Epoch: 41 - 00640/00647 - Loss: 0.02340. [149 s]\n",
      "Epoch: 41 - loss(trn/val):0.02566/0.14472, acc(val):96.14%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 42 - 00020/00647 - Loss: 0.01477. [  4 s]\n",
      "Epoch: 42 - 00040/00647 - Loss: 0.04949. [  9 s]\n",
      "Epoch: 42 - 00060/00647 - Loss: 0.02287. [ 14 s]\n",
      "Epoch: 42 - 00080/00647 - Loss: 0.02552. [ 18 s]\n",
      "Epoch: 42 - 00100/00647 - Loss: 0.06564. [ 23 s]\n",
      "Epoch: 42 - 00120/00647 - Loss: 0.02067. [ 27 s]\n",
      "Epoch: 42 - 00140/00647 - Loss: 0.02593. [ 32 s]\n",
      "Epoch: 42 - 00160/00647 - Loss: 0.02496. [ 37 s]\n",
      "Epoch: 42 - 00180/00647 - Loss: 0.04098. [ 41 s]\n",
      "Epoch: 42 - 00200/00647 - Loss: 0.01544. [ 46 s]\n",
      "Epoch: 42 - 00220/00647 - Loss: 0.02915. [ 51 s]\n",
      "Epoch: 42 - 00240/00647 - Loss: 0.01705. [ 55 s]\n",
      "Epoch: 42 - 00260/00647 - Loss: 0.04001. [ 60 s]\n",
      "Epoch: 42 - 00280/00647 - Loss: 0.03004. [ 65 s]\n",
      "Epoch: 42 - 00300/00647 - Loss: 0.03867. [ 69 s]\n",
      "Epoch: 42 - 00320/00647 - Loss: 0.03654. [ 74 s]\n",
      "Epoch: 42 - 00340/00647 - Loss: 0.04079. [ 78 s]\n",
      "Epoch: 42 - 00360/00647 - Loss: 0.04066. [ 83 s]\n",
      "Epoch: 42 - 00380/00647 - Loss: 0.01957. [ 88 s]\n",
      "Epoch: 42 - 00400/00647 - Loss: 0.02613. [ 92 s]\n",
      "Epoch: 42 - 00420/00647 - Loss: 0.04219. [ 97 s]\n",
      "Epoch: 42 - 00440/00647 - Loss: 0.02354. [102 s]\n",
      "Epoch: 42 - 00460/00647 - Loss: 0.03429. [106 s]\n",
      "Epoch: 42 - 00480/00647 - Loss: 0.04884. [111 s]\n",
      "Epoch: 42 - 00500/00647 - Loss: 0.08152. [116 s]\n",
      "Epoch: 42 - 00520/00647 - Loss: 0.04358. [120 s]\n",
      "Epoch: 42 - 00540/00647 - Loss: 0.02576. [125 s]\n",
      "Epoch: 42 - 00560/00647 - Loss: 0.02938. [130 s]\n",
      "Epoch: 42 - 00580/00647 - Loss: 0.02098. [134 s]\n",
      "Epoch: 42 - 00600/00647 - Loss: 0.02844. [139 s]\n",
      "Epoch: 42 - 00620/00647 - Loss: 0.04020. [144 s]\n",
      "Epoch: 42 - 00640/00647 - Loss: 0.03190. [148 s]\n",
      "Epoch: 42 - loss(trn/val):0.02940/0.24806, acc(val):93.54%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 43 - 00020/00647 - Loss: 0.04653. [  4 s]\n",
      "Epoch: 43 - 00040/00647 - Loss: 0.04003. [  9 s]\n",
      "Epoch: 43 - 00060/00647 - Loss: 0.02875. [ 14 s]\n",
      "Epoch: 43 - 00080/00647 - Loss: 0.04729. [ 18 s]\n",
      "Epoch: 43 - 00100/00647 - Loss: 0.02152. [ 23 s]\n",
      "Epoch: 43 - 00120/00647 - Loss: 0.04522. [ 27 s]\n",
      "Epoch: 43 - 00140/00647 - Loss: 0.04079. [ 32 s]\n",
      "Epoch: 43 - 00160/00647 - Loss: 0.01717. [ 37 s]\n",
      "Epoch: 43 - 00180/00647 - Loss: 0.01835. [ 41 s]\n",
      "Epoch: 43 - 00200/00647 - Loss: 0.03285. [ 46 s]\n",
      "Epoch: 43 - 00220/00647 - Loss: 0.02467. [ 50 s]\n",
      "Epoch: 43 - 00240/00647 - Loss: 0.02426. [ 55 s]\n",
      "Epoch: 43 - 00260/00647 - Loss: 0.02047. [ 60 s]\n",
      "Epoch: 43 - 00280/00647 - Loss: 0.01341. [ 64 s]\n",
      "Epoch: 43 - 00300/00647 - Loss: 0.01399. [ 69 s]\n",
      "Epoch: 43 - 00320/00647 - Loss: 0.03052. [ 74 s]\n",
      "Epoch: 43 - 00340/00647 - Loss: 0.01405. [ 78 s]\n",
      "Epoch: 43 - 00360/00647 - Loss: 0.01401. [ 83 s]\n",
      "Epoch: 43 - 00380/00647 - Loss: 0.02494. [ 87 s]\n",
      "Epoch: 43 - 00400/00647 - Loss: 0.02639. [ 92 s]\n",
      "Epoch: 43 - 00420/00647 - Loss: 0.04066. [ 97 s]\n",
      "Epoch: 43 - 00440/00647 - Loss: 0.00770. [101 s]\n",
      "Epoch: 43 - 00460/00647 - Loss: 0.02124. [106 s]\n",
      "Epoch: 43 - 00480/00647 - Loss: 0.01662. [111 s]\n",
      "Epoch: 43 - 00500/00647 - Loss: 0.04250. [115 s]\n",
      "Epoch: 43 - 00520/00647 - Loss: 0.03349. [120 s]\n",
      "Epoch: 43 - 00540/00647 - Loss: 0.03530. [125 s]\n",
      "Epoch: 43 - 00560/00647 - Loss: 0.02842. [129 s]\n",
      "Epoch: 43 - 00580/00647 - Loss: 0.02056. [134 s]\n",
      "Epoch: 43 - 00600/00647 - Loss: 0.02062. [138 s]\n",
      "Epoch: 43 - 00620/00647 - Loss: 0.05372. [143 s]\n",
      "Epoch: 43 - 00640/00647 - Loss: 0.03114. [148 s]\n",
      "Epoch: 43 - loss(trn/val):0.02311/0.20893, acc(val):94.79%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 44 - 00020/00647 - Loss: 0.01038. [  4 s]\n",
      "Epoch: 44 - 00040/00647 - Loss: 0.02322. [  9 s]\n",
      "Epoch: 44 - 00060/00647 - Loss: 0.10639. [ 14 s]\n",
      "Epoch: 44 - 00080/00647 - Loss: 0.04705. [ 18 s]\n",
      "Epoch: 44 - 00100/00647 - Loss: 0.09851. [ 23 s]\n",
      "Epoch: 44 - 00120/00647 - Loss: 0.01671. [ 27 s]\n",
      "Epoch: 44 - 00140/00647 - Loss: 0.04513. [ 32 s]\n",
      "Epoch: 44 - 00160/00647 - Loss: 0.01477. [ 37 s]\n",
      "Epoch: 44 - 00180/00647 - Loss: 0.01360. [ 41 s]\n",
      "Epoch: 44 - 00200/00647 - Loss: 0.01400. [ 46 s]\n",
      "Epoch: 44 - 00220/00647 - Loss: 0.02000. [ 51 s]\n",
      "Epoch: 44 - 00240/00647 - Loss: 0.03090. [ 55 s]\n",
      "Epoch: 44 - 00260/00647 - Loss: 0.02695. [ 60 s]\n",
      "Epoch: 44 - 00280/00647 - Loss: 0.01321. [ 64 s]\n",
      "Epoch: 44 - 00300/00647 - Loss: 0.01478. [ 69 s]\n",
      "Epoch: 44 - 00320/00647 - Loss: 0.02243. [ 74 s]\n",
      "Epoch: 44 - 00340/00647 - Loss: 0.02792. [ 78 s]\n",
      "Epoch: 44 - 00360/00647 - Loss: 0.01943. [ 83 s]\n",
      "Epoch: 44 - 00380/00647 - Loss: 0.04294. [ 88 s]\n",
      "Epoch: 44 - 00400/00647 - Loss: 0.09835. [ 92 s]\n",
      "Epoch: 44 - 00420/00647 - Loss: 0.02675. [ 97 s]\n",
      "Epoch: 44 - 00440/00647 - Loss: 0.01880. [102 s]\n",
      "Epoch: 44 - 00460/00647 - Loss: 0.03467. [106 s]\n",
      "Epoch: 44 - 00480/00647 - Loss: 0.01955. [111 s]\n",
      "Epoch: 44 - 00500/00647 - Loss: 0.03805. [115 s]\n",
      "Epoch: 44 - 00520/00647 - Loss: 0.01594. [120 s]\n",
      "Epoch: 44 - 00540/00647 - Loss: 0.02971. [125 s]\n",
      "Epoch: 44 - 00560/00647 - Loss: 0.01099. [129 s]\n",
      "Epoch: 44 - 00580/00647 - Loss: 0.04375. [134 s]\n",
      "Epoch: 44 - 00600/00647 - Loss: 0.03081. [139 s]\n",
      "Epoch: 44 - 00620/00647 - Loss: 0.03985. [143 s]\n",
      "Epoch: 44 - 00640/00647 - Loss: 0.00758. [148 s]\n",
      "Epoch: 44 - loss(trn/val):0.02209/0.19727, acc(val):94.94%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 45 - 00020/00647 - Loss: 0.01669. [  4 s]\n",
      "Epoch: 45 - 00040/00647 - Loss: 0.01545. [  9 s]\n",
      "Epoch: 45 - 00060/00647 - Loss: 0.02373. [ 14 s]\n",
      "Epoch: 45 - 00080/00647 - Loss: 0.01763. [ 18 s]\n",
      "Epoch: 45 - 00100/00647 - Loss: 0.02301. [ 23 s]\n",
      "Epoch: 45 - 00120/00647 - Loss: 0.02627. [ 27 s]\n",
      "Epoch: 45 - 00140/00647 - Loss: 0.02199. [ 32 s]\n",
      "Epoch: 45 - 00160/00647 - Loss: 0.02723. [ 37 s]\n",
      "Epoch: 45 - 00180/00647 - Loss: 0.02334. [ 41 s]\n",
      "Epoch: 45 - 00200/00647 - Loss: 0.01898. [ 46 s]\n",
      "Epoch: 45 - 00220/00647 - Loss: 0.02088. [ 51 s]\n",
      "Epoch: 45 - 00240/00647 - Loss: 0.03803. [ 55 s]\n",
      "Epoch: 45 - 00260/00647 - Loss: 0.01342. [ 60 s]\n",
      "Epoch: 45 - 00280/00647 - Loss: 0.02776. [ 64 s]\n",
      "Epoch: 45 - 00300/00647 - Loss: 0.02108. [ 69 s]\n",
      "Epoch: 45 - 00320/00647 - Loss: 0.01790. [ 74 s]\n",
      "Epoch: 45 - 00340/00647 - Loss: 0.03681. [ 78 s]\n",
      "Epoch: 45 - 00360/00647 - Loss: 0.02446. [ 83 s]\n",
      "Epoch: 45 - 00380/00647 - Loss: 0.01753. [ 88 s]\n",
      "Epoch: 45 - 00400/00647 - Loss: 0.02443. [ 92 s]\n",
      "Epoch: 45 - 00420/00647 - Loss: 0.01717. [ 97 s]\n",
      "Epoch: 45 - 00440/00647 - Loss: 0.03295. [102 s]\n",
      "Epoch: 45 - 00460/00647 - Loss: 0.02304. [106 s]\n",
      "Epoch: 45 - 00480/00647 - Loss: 0.01438. [111 s]\n",
      "Epoch: 45 - 00500/00647 - Loss: 0.01705. [115 s]\n",
      "Epoch: 45 - 00520/00647 - Loss: 0.02036. [120 s]\n",
      "Epoch: 45 - 00540/00647 - Loss: 0.02329. [125 s]\n",
      "Epoch: 45 - 00560/00647 - Loss: 0.01712. [129 s]\n",
      "Epoch: 45 - 00580/00647 - Loss: 0.02697. [134 s]\n",
      "Epoch: 45 - 00600/00647 - Loss: 0.01612. [139 s]\n",
      "Epoch: 45 - 00620/00647 - Loss: 0.04732. [143 s]\n",
      "Epoch: 45 - 00640/00647 - Loss: 0.05593. [148 s]\n",
      "Epoch: 45 - loss(trn/val):0.02958/0.23729, acc(val):93.58%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 46 - 00020/00647 - Loss: 0.03616. [  4 s]\n",
      "Epoch: 46 - 00040/00647 - Loss: 0.05749. [  9 s]\n",
      "Epoch: 46 - 00060/00647 - Loss: 0.02254. [ 14 s]\n",
      "Epoch: 46 - 00080/00647 - Loss: 0.01345. [ 18 s]\n",
      "Epoch: 46 - 00100/00647 - Loss: 0.01501. [ 23 s]\n",
      "Epoch: 46 - 00120/00647 - Loss: 0.01506. [ 27 s]\n",
      "Epoch: 46 - 00140/00647 - Loss: 0.02284. [ 32 s]\n",
      "Epoch: 46 - 00160/00647 - Loss: 0.01935. [ 37 s]\n",
      "Epoch: 46 - 00180/00647 - Loss: 0.01749. [ 41 s]\n",
      "Epoch: 46 - 00200/00647 - Loss: 0.03274. [ 46 s]\n",
      "Epoch: 46 - 00220/00647 - Loss: 0.01834. [ 51 s]\n",
      "Epoch: 46 - 00240/00647 - Loss: 0.01821. [ 55 s]\n",
      "Epoch: 46 - 00260/00647 - Loss: 0.02004. [ 60 s]\n",
      "Epoch: 46 - 00280/00647 - Loss: 0.03010. [ 65 s]\n",
      "Epoch: 46 - 00300/00647 - Loss: 0.02124. [ 69 s]\n",
      "Epoch: 46 - 00320/00647 - Loss: 0.03277. [ 74 s]\n",
      "Epoch: 46 - 00340/00647 - Loss: 0.02729. [ 78 s]\n",
      "Epoch: 46 - 00360/00647 - Loss: 0.02729. [ 83 s]\n",
      "Epoch: 46 - 00380/00647 - Loss: 0.01554. [ 88 s]\n",
      "Epoch: 46 - 00400/00647 - Loss: 0.00982. [ 92 s]\n",
      "Epoch: 46 - 00420/00647 - Loss: 0.03269. [ 97 s]\n",
      "Epoch: 46 - 00440/00647 - Loss: 0.01247. [102 s]\n",
      "Epoch: 46 - 00460/00647 - Loss: 0.02516. [106 s]\n",
      "Epoch: 46 - 00480/00647 - Loss: 0.02651. [111 s]\n",
      "Epoch: 46 - 00500/00647 - Loss: 0.33220. [116 s]\n",
      "Epoch: 46 - 00520/00647 - Loss: 0.04303. [120 s]\n",
      "Epoch: 46 - 00540/00647 - Loss: 0.02429. [125 s]\n",
      "Epoch: 46 - 00560/00647 - Loss: 0.02889. [130 s]\n",
      "Epoch: 46 - 00580/00647 - Loss: 0.02084. [134 s]\n",
      "Epoch: 46 - 00600/00647 - Loss: 0.02361. [139 s]\n",
      "Epoch: 46 - 00620/00647 - Loss: 0.03349. [143 s]\n",
      "Epoch: 46 - 00640/00647 - Loss: 0.03635. [148 s]\n",
      "Epoch: 46 - loss(trn/val):0.02292/0.18088, acc(val):95.40%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 47 - 00020/00647 - Loss: 0.02232. [  4 s]\n",
      "Epoch: 47 - 00040/00647 - Loss: 0.01308. [  9 s]\n",
      "Epoch: 47 - 00060/00647 - Loss: 0.02977. [ 14 s]\n",
      "Epoch: 47 - 00080/00647 - Loss: 0.02293. [ 18 s]\n",
      "Epoch: 47 - 00100/00647 - Loss: 0.01019. [ 23 s]\n",
      "Epoch: 47 - 00120/00647 - Loss: 0.03330. [ 27 s]\n",
      "Epoch: 47 - 00140/00647 - Loss: 0.02459. [ 32 s]\n",
      "Epoch: 47 - 00160/00647 - Loss: 0.00811. [ 37 s]\n",
      "Epoch: 47 - 00180/00647 - Loss: 0.02312. [ 41 s]\n",
      "Epoch: 47 - 00200/00647 - Loss: 0.03297. [ 46 s]\n",
      "Epoch: 47 - 00220/00647 - Loss: 0.03106. [ 51 s]\n",
      "Epoch: 47 - 00240/00647 - Loss: 0.02933. [ 55 s]\n",
      "Epoch: 47 - 00260/00647 - Loss: 0.03232. [ 60 s]\n",
      "Epoch: 47 - 00280/00647 - Loss: 0.02937. [ 64 s]\n",
      "Epoch: 47 - 00300/00647 - Loss: 0.01326. [ 69 s]\n",
      "Epoch: 47 - 00320/00647 - Loss: 0.01454. [ 74 s]\n",
      "Epoch: 47 - 00340/00647 - Loss: 0.12000. [ 78 s]\n",
      "Epoch: 47 - 00360/00647 - Loss: 0.03245. [ 83 s]\n",
      "Epoch: 47 - 00380/00647 - Loss: 0.02724. [ 88 s]\n",
      "Epoch: 47 - 00400/00647 - Loss: 0.02542. [ 92 s]\n",
      "Epoch: 47 - 00420/00647 - Loss: 0.05704. [ 97 s]\n",
      "Epoch: 47 - 00440/00647 - Loss: 0.01104. [102 s]\n",
      "Epoch: 47 - 00460/00647 - Loss: 0.02056. [106 s]\n",
      "Epoch: 47 - 00480/00647 - Loss: 0.01444. [111 s]\n",
      "Epoch: 47 - 00500/00647 - Loss: 0.02285. [116 s]\n",
      "Epoch: 47 - 00520/00647 - Loss: 0.02349. [120 s]\n",
      "Epoch: 47 - 00540/00647 - Loss: 0.04300. [125 s]\n",
      "Epoch: 47 - 00560/00647 - Loss: 0.01393. [129 s]\n",
      "Epoch: 47 - 00580/00647 - Loss: 0.03406. [134 s]\n",
      "Epoch: 47 - 00600/00647 - Loss: 0.03019. [139 s]\n",
      "Epoch: 47 - 00620/00647 - Loss: 0.01756. [143 s]\n",
      "Epoch: 47 - 00640/00647 - Loss: 0.02237. [148 s]\n",
      "Epoch: 47 - loss(trn/val):0.02094/0.17249, acc(val):95.54%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 48 - 00020/00647 - Loss: 0.01871. [  5 s]\n",
      "Epoch: 48 - 00040/00647 - Loss: 0.03709. [  9 s]\n",
      "Epoch: 48 - 00060/00647 - Loss: 0.02052. [ 14 s]\n",
      "Epoch: 48 - 00080/00647 - Loss: 0.04007. [ 18 s]\n",
      "Epoch: 48 - 00100/00647 - Loss: 0.02628. [ 23 s]\n",
      "Epoch: 48 - 00120/00647 - Loss: 0.02472. [ 27 s]\n",
      "Epoch: 48 - 00140/00647 - Loss: 0.02659. [ 32 s]\n",
      "Epoch: 48 - 00160/00647 - Loss: 0.03731. [ 37 s]\n",
      "Epoch: 48 - 00180/00647 - Loss: 0.03017. [ 41 s]\n",
      "Epoch: 48 - 00200/00647 - Loss: 0.01493. [ 46 s]\n",
      "Epoch: 48 - 00220/00647 - Loss: 0.02080. [ 50 s]\n",
      "Epoch: 48 - 00240/00647 - Loss: 0.01825. [ 55 s]\n",
      "Epoch: 48 - 00260/00647 - Loss: 0.02349. [ 60 s]\n",
      "Epoch: 48 - 00280/00647 - Loss: 0.01563. [ 64 s]\n",
      "Epoch: 48 - 00300/00647 - Loss: 0.01489. [ 69 s]\n",
      "Epoch: 48 - 00320/00647 - Loss: 0.02091. [ 74 s]\n",
      "Epoch: 48 - 00340/00647 - Loss: 0.01674. [ 78 s]\n",
      "Epoch: 48 - 00360/00647 - Loss: 0.03579. [ 83 s]\n",
      "Epoch: 48 - 00380/00647 - Loss: 0.02555. [ 88 s]\n",
      "Epoch: 48 - 00400/00647 - Loss: 0.01495. [ 92 s]\n",
      "Epoch: 48 - 00420/00647 - Loss: 0.02446. [ 97 s]\n",
      "Epoch: 48 - 00440/00647 - Loss: 0.02175. [101 s]\n",
      "Epoch: 48 - 00460/00647 - Loss: 0.01072. [106 s]\n",
      "Epoch: 48 - 00480/00647 - Loss: 0.02588. [111 s]\n",
      "Epoch: 48 - 00500/00647 - Loss: 0.02172. [115 s]\n",
      "Epoch: 48 - 00520/00647 - Loss: 0.02533. [120 s]\n",
      "Epoch: 48 - 00540/00647 - Loss: 0.02306. [125 s]\n",
      "Epoch: 48 - 00560/00647 - Loss: 0.01681. [129 s]\n",
      "Epoch: 48 - 00580/00647 - Loss: 0.01679. [134 s]\n",
      "Epoch: 48 - 00600/00647 - Loss: 0.02114. [139 s]\n",
      "Epoch: 48 - 00620/00647 - Loss: 0.04163. [143 s]\n",
      "Epoch: 48 - 00640/00647 - Loss: 0.06226. [148 s]\n",
      "Epoch: 48 - loss(trn/val):0.02961/0.16547, acc(val):95.06%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 49 - 00020/00647 - Loss: 0.01468. [  5 s]\n",
      "Epoch: 49 - 00040/00647 - Loss: 0.02668. [  9 s]\n",
      "Epoch: 49 - 00060/00647 - Loss: 0.01872. [ 14 s]\n",
      "Epoch: 49 - 00080/00647 - Loss: 0.03825. [ 19 s]\n",
      "Epoch: 49 - 00100/00647 - Loss: 0.11411. [ 23 s]\n",
      "Epoch: 49 - 00120/00647 - Loss: 0.03489. [ 28 s]\n",
      "Epoch: 49 - 00140/00647 - Loss: 0.02219. [ 33 s]\n",
      "Epoch: 49 - 00160/00647 - Loss: 0.02934. [ 37 s]\n",
      "Epoch: 49 - 00180/00647 - Loss: 0.01769. [ 42 s]\n",
      "Epoch: 49 - 00200/00647 - Loss: 0.03649. [ 47 s]\n",
      "Epoch: 49 - 00220/00647 - Loss: 0.02885. [ 51 s]\n",
      "Epoch: 49 - 00240/00647 - Loss: 0.02004. [ 56 s]\n",
      "Epoch: 49 - 00260/00647 - Loss: 0.02290. [ 61 s]\n",
      "Epoch: 49 - 00280/00647 - Loss: 0.02978. [ 66 s]\n",
      "Epoch: 49 - 00300/00647 - Loss: 0.02351. [ 70 s]\n",
      "Epoch: 49 - 00320/00647 - Loss: 0.01276. [ 75 s]\n",
      "Epoch: 49 - 00340/00647 - Loss: 0.01473. [ 80 s]\n",
      "Epoch: 49 - 00360/00647 - Loss: 0.02054. [ 84 s]\n",
      "Epoch: 49 - 00380/00647 - Loss: 0.01597. [ 89 s]\n",
      "Epoch: 49 - 00400/00647 - Loss: 0.01561. [ 94 s]\n",
      "Epoch: 49 - 00420/00647 - Loss: 0.01532. [ 99 s]\n",
      "Epoch: 49 - 00440/00647 - Loss: 0.04033. [103 s]\n",
      "Epoch: 49 - 00460/00647 - Loss: 0.05194. [108 s]\n",
      "Epoch: 49 - 00480/00647 - Loss: 0.52040. [113 s]\n",
      "Epoch: 49 - 00500/00647 - Loss: 0.02074. [117 s]\n",
      "Epoch: 49 - 00520/00647 - Loss: 0.01212. [122 s]\n",
      "Epoch: 49 - 00540/00647 - Loss: 0.02794. [127 s]\n",
      "Epoch: 49 - 00560/00647 - Loss: 0.04310. [132 s]\n",
      "Epoch: 49 - 00580/00647 - Loss: 0.01675. [136 s]\n",
      "Epoch: 49 - 00600/00647 - Loss: 0.03511. [141 s]\n",
      "Epoch: 49 - 00620/00647 - Loss: 0.03543. [146 s]\n",
      "Epoch: 49 - 00640/00647 - Loss: 0.02402. [150 s]\n",
      "Epoch: 49 - loss(trn/val):0.02244/0.15134, acc(val):95.83%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 50 - 00020/00647 - Loss: 0.02132. [  5 s]\n",
      "Epoch: 50 - 00040/00647 - Loss: 0.00953. [ 10 s]\n",
      "Epoch: 50 - 00060/00647 - Loss: 0.00853. [ 14 s]\n",
      "Epoch: 50 - 00080/00647 - Loss: 0.02199. [ 19 s]\n",
      "Epoch: 50 - 00100/00647 - Loss: 0.01615. [ 24 s]\n",
      "Epoch: 50 - 00120/00647 - Loss: 0.00927. [ 28 s]\n",
      "Epoch: 50 - 00140/00647 - Loss: 0.01378. [ 33 s]\n",
      "Epoch: 50 - 00160/00647 - Loss: 0.03503. [ 37 s]\n",
      "Epoch: 50 - 00180/00647 - Loss: 0.04425. [ 42 s]\n",
      "Epoch: 50 - 00200/00647 - Loss: 0.02534. [ 47 s]\n",
      "Epoch: 50 - 00220/00647 - Loss: 0.01701. [ 52 s]\n",
      "Epoch: 50 - 00240/00647 - Loss: 0.02196. [ 57 s]\n",
      "Epoch: 50 - 00260/00647 - Loss: 0.01701. [ 61 s]\n",
      "Epoch: 50 - 00280/00647 - Loss: 0.02249. [ 66 s]\n",
      "Epoch: 50 - 00300/00647 - Loss: 0.02878. [ 71 s]\n",
      "Epoch: 50 - 00320/00647 - Loss: 0.04170. [ 75 s]\n",
      "Epoch: 50 - 00340/00647 - Loss: 0.03973. [ 80 s]\n",
      "Epoch: 50 - 00360/00647 - Loss: 0.02483. [ 85 s]\n",
      "Epoch: 50 - 00380/00647 - Loss: 0.02004. [ 89 s]\n",
      "Epoch: 50 - 00400/00647 - Loss: 0.01734. [ 94 s]\n",
      "Epoch: 50 - 00420/00647 - Loss: 0.01127. [ 99 s]\n",
      "Epoch: 50 - 00440/00647 - Loss: 0.01742. [104 s]\n",
      "Epoch: 50 - 00460/00647 - Loss: 0.02496. [108 s]\n",
      "Epoch: 50 - 00480/00647 - Loss: 0.03576. [113 s]\n",
      "Epoch: 50 - 00500/00647 - Loss: 0.00900. [118 s]\n",
      "Epoch: 50 - 00520/00647 - Loss: 0.02914. [123 s]\n",
      "Epoch: 50 - 00540/00647 - Loss: 0.02362. [127 s]\n",
      "Epoch: 50 - 00560/00647 - Loss: 0.01602. [132 s]\n",
      "Epoch: 50 - 00580/00647 - Loss: 0.06178. [137 s]\n",
      "Epoch: 50 - 00600/00647 - Loss: 0.01697. [141 s]\n",
      "Epoch: 50 - 00620/00647 - Loss: 0.01631. [146 s]\n",
      "Epoch: 50 - 00640/00647 - Loss: 0.01350. [151 s]\n",
      "Epoch: 50 - loss(trn/val):0.02044/0.20311, acc(val):95.49%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 51 - 00020/00647 - Loss: 0.02195. [  5 s]\n",
      "Epoch: 51 - 00040/00647 - Loss: 0.01739. [  9 s]\n",
      "Epoch: 51 - 00060/00647 - Loss: 0.02476. [ 14 s]\n",
      "Epoch: 51 - 00080/00647 - Loss: 0.02170. [ 19 s]\n",
      "Epoch: 51 - 00100/00647 - Loss: 0.02072. [ 23 s]\n",
      "Epoch: 51 - 00120/00647 - Loss: 0.02703. [ 28 s]\n",
      "Epoch: 51 - 00140/00647 - Loss: 0.01703. [ 33 s]\n",
      "Epoch: 51 - 00160/00647 - Loss: 0.00840. [ 37 s]\n",
      "Epoch: 51 - 00180/00647 - Loss: 0.01094. [ 42 s]\n",
      "Epoch: 51 - 00200/00647 - Loss: 0.01326. [ 47 s]\n",
      "Epoch: 51 - 00220/00647 - Loss: 0.11301. [ 51 s]\n",
      "Epoch: 51 - 00240/00647 - Loss: 0.01390. [ 56 s]\n",
      "Epoch: 51 - 00260/00647 - Loss: 0.01870. [ 61 s]\n",
      "Epoch: 51 - 00280/00647 - Loss: 0.01339. [ 66 s]\n",
      "Epoch: 51 - 00300/00647 - Loss: 0.01665. [ 70 s]\n",
      "Epoch: 51 - 00320/00647 - Loss: 0.63382. [ 75 s]\n",
      "Epoch: 51 - 00340/00647 - Loss: 0.05246. [ 80 s]\n",
      "Epoch: 51 - 00360/00647 - Loss: 0.05763. [ 84 s]\n",
      "Epoch: 51 - 00380/00647 - Loss: 0.06510. [ 89 s]\n",
      "Epoch: 51 - 00400/00647 - Loss: 0.03316. [ 94 s]\n",
      "Epoch: 51 - 00420/00647 - Loss: 0.03704. [ 98 s]\n",
      "Epoch: 51 - 00440/00647 - Loss: 0.02133. [103 s]\n",
      "Epoch: 51 - 00460/00647 - Loss: 0.03347. [108 s]\n",
      "Epoch: 51 - 00480/00647 - Loss: 0.02969. [112 s]\n",
      "Epoch: 51 - 00500/00647 - Loss: 0.01941. [117 s]\n",
      "Epoch: 51 - 00520/00647 - Loss: 0.01432. [122 s]\n",
      "Epoch: 51 - 00540/00647 - Loss: 0.02945. [126 s]\n",
      "Epoch: 51 - 00560/00647 - Loss: 0.01580. [131 s]\n",
      "Epoch: 51 - 00580/00647 - Loss: 0.02502. [136 s]\n",
      "Epoch: 51 - 00600/00647 - Loss: 0.05138. [140 s]\n",
      "Epoch: 51 - 00620/00647 - Loss: 0.01563. [145 s]\n",
      "Epoch: 51 - 00640/00647 - Loss: 0.02725. [150 s]\n",
      "Epoch: 51 - loss(trn/val):0.02131/0.19207, acc(val):94.73%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 52 - 00020/00647 - Loss: 0.02123. [  5 s]\n",
      "Epoch: 52 - 00040/00647 - Loss: 0.02190. [  9 s]\n",
      "Epoch: 52 - 00060/00647 - Loss: 0.03986. [ 14 s]\n",
      "Epoch: 52 - 00080/00647 - Loss: 0.01624. [ 18 s]\n",
      "Epoch: 52 - 00100/00647 - Loss: 0.01794. [ 23 s]\n",
      "Epoch: 52 - 00120/00647 - Loss: 0.02776. [ 27 s]\n",
      "Epoch: 52 - 00140/00647 - Loss: 0.02911. [ 32 s]\n",
      "Epoch: 52 - 00160/00647 - Loss: 0.02734. [ 37 s]\n",
      "Epoch: 52 - 00180/00647 - Loss: 0.02442. [ 41 s]\n",
      "Epoch: 52 - 00200/00647 - Loss: 0.02380. [ 46 s]\n",
      "Epoch: 52 - 00220/00647 - Loss: 0.03095. [ 51 s]\n",
      "Epoch: 52 - 00240/00647 - Loss: 0.01969. [ 55 s]\n",
      "Epoch: 52 - 00260/00647 - Loss: 0.01935. [ 60 s]\n",
      "Epoch: 52 - 00280/00647 - Loss: 0.01346. [ 65 s]\n",
      "Epoch: 52 - 00300/00647 - Loss: 0.02882. [ 69 s]\n",
      "Epoch: 52 - 00320/00647 - Loss: 0.02629. [ 74 s]\n",
      "Epoch: 52 - 00340/00647 - Loss: 0.02259. [ 78 s]\n",
      "Epoch: 52 - 00360/00647 - Loss: 0.00754. [ 83 s]\n",
      "Epoch: 52 - 00380/00647 - Loss: 0.00664. [ 88 s]\n",
      "Epoch: 52 - 00400/00647 - Loss: 0.01550. [ 92 s]\n",
      "Epoch: 52 - 00420/00647 - Loss: 0.03357. [ 97 s]\n",
      "Epoch: 52 - 00440/00647 - Loss: 0.15428. [102 s]\n",
      "Epoch: 52 - 00460/00647 - Loss: 0.02451. [106 s]\n",
      "Epoch: 52 - 00480/00647 - Loss: 0.03045. [111 s]\n",
      "Epoch: 52 - 00500/00647 - Loss: 0.01392. [116 s]\n",
      "Epoch: 52 - 00520/00647 - Loss: 0.02278. [120 s]\n",
      "Epoch: 52 - 00540/00647 - Loss: 0.02447. [125 s]\n",
      "Epoch: 52 - 00560/00647 - Loss: 0.01746. [130 s]\n",
      "Epoch: 52 - 00580/00647 - Loss: 0.02624. [134 s]\n",
      "Epoch: 52 - 00600/00647 - Loss: 0.02483. [139 s]\n",
      "Epoch: 52 - 00620/00647 - Loss: 0.01802. [144 s]\n",
      "Epoch: 52 - 00640/00647 - Loss: 0.01512. [148 s]\n",
      "Epoch: 52 - loss(trn/val):0.01993/0.25304, acc(val):94.00%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 53 - 00020/00647 - Loss: 0.01477. [  5 s]\n",
      "Epoch: 53 - 00040/00647 - Loss: 0.02781. [  9 s]\n",
      "Epoch: 53 - 00060/00647 - Loss: 0.01563. [ 14 s]\n",
      "Epoch: 53 - 00080/00647 - Loss: 0.02929. [ 18 s]\n",
      "Epoch: 53 - 00100/00647 - Loss: 0.04814. [ 23 s]\n",
      "Epoch: 53 - 00120/00647 - Loss: 0.01786. [ 27 s]\n",
      "Epoch: 53 - 00140/00647 - Loss: 0.02296. [ 32 s]\n",
      "Epoch: 53 - 00160/00647 - Loss: 0.03026. [ 37 s]\n",
      "Epoch: 53 - 00180/00647 - Loss: 0.03427. [ 41 s]\n",
      "Epoch: 53 - 00200/00647 - Loss: 0.01908. [ 46 s]\n",
      "Epoch: 53 - 00220/00647 - Loss: 0.02781. [ 51 s]\n",
      "Epoch: 53 - 00240/00647 - Loss: 0.02739. [ 55 s]\n",
      "Epoch: 53 - 00260/00647 - Loss: 0.03349. [ 60 s]\n",
      "Epoch: 53 - 00280/00647 - Loss: 0.02080. [ 65 s]\n",
      "Epoch: 53 - 00300/00647 - Loss: 0.00952. [ 69 s]\n",
      "Epoch: 53 - 00320/00647 - Loss: 0.01285. [ 74 s]\n",
      "Epoch: 53 - 00340/00647 - Loss: 0.02741. [ 78 s]\n",
      "Epoch: 53 - 00360/00647 - Loss: 0.03923. [ 83 s]\n",
      "Epoch: 53 - 00380/00647 - Loss: 0.01472. [ 88 s]\n",
      "Epoch: 53 - 00400/00647 - Loss: 0.01588. [ 92 s]\n",
      "Epoch: 53 - 00420/00647 - Loss: 0.01739. [ 97 s]\n",
      "Epoch: 53 - 00440/00647 - Loss: 0.03001. [102 s]\n",
      "Epoch: 53 - 00460/00647 - Loss: 0.01391. [106 s]\n",
      "Epoch: 53 - 00480/00647 - Loss: 0.02276. [111 s]\n",
      "Epoch: 53 - 00500/00647 - Loss: 0.02114. [116 s]\n",
      "Epoch: 53 - 00520/00647 - Loss: 0.01186. [120 s]\n",
      "Epoch: 53 - 00540/00647 - Loss: 0.02517. [125 s]\n",
      "Epoch: 53 - 00560/00647 - Loss: 0.02752. [130 s]\n",
      "Epoch: 53 - 00580/00647 - Loss: 0.02656. [134 s]\n",
      "Epoch: 53 - 00600/00647 - Loss: 0.01572. [139 s]\n",
      "Epoch: 53 - 00620/00647 - Loss: 0.02489. [144 s]\n",
      "Epoch: 53 - 00640/00647 - Loss: 0.01635. [148 s]\n",
      "Epoch: 53 - loss(trn/val):0.01950/0.21929, acc(val):95.07%, lr=0.00010. [150s] @17 samples/s \n",
      "Epoch: 54 - 00020/00647 - Loss: 0.03189. [  5 s]\n",
      "Epoch: 54 - 00040/00647 - Loss: 0.01942. [  9 s]\n",
      "Epoch: 54 - 00060/00647 - Loss: 0.01449. [ 14 s]\n",
      "Epoch: 54 - 00080/00647 - Loss: 0.00849. [ 18 s]\n",
      "Epoch: 54 - 00100/00647 - Loss: 0.01543. [ 23 s]\n",
      "Epoch: 54 - 00120/00647 - Loss: 0.01459. [ 27 s]\n",
      "Epoch: 54 - 00140/00647 - Loss: 0.01371. [ 32 s]\n",
      "Epoch: 54 - 00160/00647 - Loss: 0.01989. [ 37 s]\n",
      "Epoch: 54 - 00180/00647 - Loss: 0.02340. [ 41 s]\n",
      "Epoch: 54 - 00200/00647 - Loss: 0.01735. [ 46 s]\n",
      "Epoch: 54 - 00220/00647 - Loss: 0.01536. [ 51 s]\n",
      "Epoch: 54 - 00240/00647 - Loss: 0.00814. [ 55 s]\n",
      "Epoch: 54 - 00260/00647 - Loss: 0.03293. [ 60 s]\n",
      "Epoch: 54 - 00280/00647 - Loss: 0.04932. [ 65 s]\n",
      "Epoch: 54 - 00300/00647 - Loss: 0.03166. [ 70 s]\n",
      "Epoch: 54 - 00320/00647 - Loss: 0.02802. [ 74 s]\n",
      "Epoch: 54 - 00340/00647 - Loss: 0.02905. [ 79 s]\n",
      "Epoch: 54 - 00360/00647 - Loss: 0.01946. [ 84 s]\n",
      "Epoch: 54 - 00380/00647 - Loss: 0.02700. [ 88 s]\n",
      "Epoch: 54 - 00400/00647 - Loss: 0.01243. [ 93 s]\n",
      "Epoch: 54 - 00420/00647 - Loss: 0.03285. [ 98 s]\n",
      "Epoch: 54 - 00440/00647 - Loss: 0.00692. [103 s]\n",
      "Epoch: 54 - 00460/00647 - Loss: 0.01851. [107 s]\n",
      "Epoch: 54 - 00480/00647 - Loss: 0.04112. [112 s]\n",
      "Epoch: 54 - 00500/00647 - Loss: 0.02080. [117 s]\n",
      "Epoch: 54 - 00520/00647 - Loss: 0.02062. [121 s]\n",
      "Epoch: 54 - 00540/00647 - Loss: 0.01497. [126 s]\n",
      "Epoch: 54 - 00560/00647 - Loss: 0.02049. [131 s]\n",
      "Epoch: 54 - 00580/00647 - Loss: 0.02637. [136 s]\n",
      "Epoch: 54 - 00600/00647 - Loss: 0.02894. [140 s]\n",
      "Epoch: 54 - 00620/00647 - Loss: 0.02206. [145 s]\n",
      "Epoch: 54 - 00640/00647 - Loss: 0.01212. [150 s]\n",
      "Epoch: 54 - loss(trn/val):0.01908/0.20394, acc(val):95.13%, lr=0.00010. [151s] @17 samples/s \n",
      "Epoch: 55 - 00020/00647 - Loss: 0.01992. [  5 s]\n",
      "Epoch: 55 - 00040/00647 - Loss: 0.02804. [  9 s]\n",
      "Epoch: 55 - 00060/00647 - Loss: 0.03701. [ 14 s]\n",
      "Epoch: 55 - 00080/00647 - Loss: 0.01483. [ 18 s]\n",
      "Epoch: 55 - 00100/00647 - Loss: 0.02651. [ 23 s]\n",
      "Epoch: 55 - 00120/00647 - Loss: 0.02447. [ 28 s]\n",
      "Epoch: 55 - 00140/00647 - Loss: 0.03870. [ 32 s]\n",
      "Epoch: 55 - 00160/00647 - Loss: 0.02158. [ 37 s]\n",
      "Epoch: 55 - 00180/00647 - Loss: 0.02784. [ 42 s]\n",
      "Epoch: 55 - 00200/00647 - Loss: 0.02076. [ 47 s]\n",
      "Epoch: 55 - 00220/00647 - Loss: 0.01886. [ 51 s]\n",
      "Epoch: 55 - 00240/00647 - Loss: 0.01145. [ 56 s]\n",
      "Epoch: 55 - 00260/00647 - Loss: 0.03061. [ 61 s]\n",
      "Epoch: 55 - 00280/00647 - Loss: 0.01869. [ 65 s]\n",
      "Epoch: 55 - 00300/00647 - Loss: 0.01348. [ 70 s]\n",
      "Epoch: 55 - 00320/00647 - Loss: 0.02522. [ 75 s]\n",
      "Epoch: 55 - 00340/00647 - Loss: 0.00855. [ 80 s]\n",
      "Epoch: 55 - 00360/00647 - Loss: 0.02668. [ 84 s]\n",
      "Epoch: 55 - 00380/00647 - Loss: 0.02730. [ 89 s]\n",
      "Epoch: 55 - 00400/00647 - Loss: 0.01548. [ 94 s]\n",
      "Epoch: 55 - 00420/00647 - Loss: 0.02795. [ 98 s]\n",
      "Epoch: 55 - 00440/00647 - Loss: 0.01479. [103 s]\n",
      "Epoch: 55 - 00460/00647 - Loss: 0.01971. [108 s]\n",
      "Epoch: 55 - 00480/00647 - Loss: 0.04328. [112 s]\n",
      "Epoch: 55 - 00500/00647 - Loss: 0.05051. [117 s]\n",
      "Epoch: 55 - 00520/00647 - Loss: 0.02643. [122 s]\n",
      "Epoch: 55 - 00540/00647 - Loss: 0.01419. [127 s]\n",
      "Epoch: 55 - 00560/00647 - Loss: 0.02037. [131 s]\n",
      "Epoch: 55 - 00580/00647 - Loss: 0.01785. [136 s]\n",
      "Epoch: 55 - 00600/00647 - Loss: 0.01912. [141 s]\n",
      "Epoch: 55 - 00620/00647 - Loss: 0.02665. [145 s]\n",
      "Epoch: 55 - 00640/00647 - Loss: 0.03797. [150 s]\n",
      "Epoch: 55 - loss(trn/val):0.02399/0.21389, acc(val):94.10%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 56 - 00020/00647 - Loss: 0.01040. [  5 s]\n",
      "Epoch: 56 - 00040/00647 - Loss: 0.07551. [  9 s]\n",
      "Epoch: 56 - 00060/00647 - Loss: 0.01426. [ 14 s]\n",
      "Epoch: 56 - 00080/00647 - Loss: 0.03084. [ 19 s]\n",
      "Epoch: 56 - 00100/00647 - Loss: 0.01838. [ 23 s]\n",
      "Epoch: 56 - 00120/00647 - Loss: 0.02241. [ 28 s]\n",
      "Epoch: 56 - 00140/00647 - Loss: 0.01306. [ 33 s]\n",
      "Epoch: 56 - 00160/00647 - Loss: 0.02528. [ 37 s]\n",
      "Epoch: 56 - 00180/00647 - Loss: 0.01623. [ 42 s]\n",
      "Epoch: 56 - 00200/00647 - Loss: 0.02067. [ 47 s]\n",
      "Epoch: 56 - 00220/00647 - Loss: 0.02084. [ 51 s]\n",
      "Epoch: 56 - 00240/00647 - Loss: 0.01703. [ 56 s]\n",
      "Epoch: 56 - 00260/00647 - Loss: 0.01859. [ 61 s]\n",
      "Epoch: 56 - 00280/00647 - Loss: 0.02095. [ 66 s]\n",
      "Epoch: 56 - 00300/00647 - Loss: 0.01954. [ 70 s]\n",
      "Epoch: 56 - 00320/00647 - Loss: 0.03643. [ 75 s]\n",
      "Epoch: 56 - 00340/00647 - Loss: 0.02423. [ 80 s]\n",
      "Epoch: 56 - 00360/00647 - Loss: 0.01241. [ 84 s]\n",
      "Epoch: 56 - 00380/00647 - Loss: 0.01765. [ 89 s]\n",
      "Epoch: 56 - 00400/00647 - Loss: 0.03756. [ 94 s]\n",
      "Epoch: 56 - 00420/00647 - Loss: 0.01473. [ 99 s]\n",
      "Epoch: 56 - 00440/00647 - Loss: 0.02842. [103 s]\n",
      "Epoch: 56 - 00460/00647 - Loss: 0.02344. [108 s]\n",
      "Epoch: 56 - 00480/00647 - Loss: 0.01612. [113 s]\n",
      "Epoch: 56 - 00500/00647 - Loss: 0.02359. [117 s]\n",
      "Epoch: 56 - 00520/00647 - Loss: 0.01615. [122 s]\n",
      "Epoch: 56 - 00540/00647 - Loss: 0.00899. [127 s]\n",
      "Epoch: 56 - 00560/00647 - Loss: 0.01930. [132 s]\n",
      "Epoch: 56 - 00580/00647 - Loss: 0.03025. [136 s]\n",
      "Epoch: 56 - 00600/00647 - Loss: 0.02055. [141 s]\n",
      "Epoch: 56 - 00620/00647 - Loss: 0.02365. [146 s]\n",
      "Epoch: 56 - 00640/00647 - Loss: 0.10509. [150 s]\n",
      "Epoch: 56 - loss(trn/val):0.02802/0.25050, acc(val):92.84%, lr=0.00010. [152s] @16 samples/s \n",
      "Epoch: 57 - 00020/00647 - Loss: 0.03072. [  4 s]\n",
      "Epoch: 57 - 00040/00647 - Loss: 0.01385. [  9 s]\n",
      "Epoch: 57 - 00060/00647 - Loss: 0.01214. [ 14 s]\n",
      "Epoch: 57 - 00080/00647 - Loss: 0.01777. [ 18 s]\n",
      "Epoch: 57 - 00100/00647 - Loss: 0.03795. [ 23 s]\n",
      "Epoch: 57 - 00120/00647 - Loss: 0.01874. [ 27 s]\n",
      "Epoch: 57 - 00140/00647 - Loss: 0.01177. [ 32 s]\n",
      "Epoch: 57 - 00160/00647 - Loss: 0.01806. [ 36 s]\n",
      "Epoch: 57 - 00180/00647 - Loss: 0.01904. [ 41 s]\n",
      "Epoch: 57 - 00200/00647 - Loss: 0.01495. [ 46 s]\n",
      "Epoch: 57 - 00220/00647 - Loss: 0.01120. [ 50 s]\n",
      "Epoch: 57 - 00240/00647 - Loss: 0.02502. [ 55 s]\n",
      "Epoch: 57 - 00260/00647 - Loss: 0.03053. [ 60 s]\n",
      "Epoch: 57 - 00280/00647 - Loss: 0.03299. [ 64 s]\n",
      "Epoch: 57 - 00300/00647 - Loss: 0.02430. [ 69 s]\n",
      "Epoch: 57 - 00320/00647 - Loss: 0.01856. [ 74 s]\n",
      "Epoch: 57 - 00340/00647 - Loss: 0.03367. [ 78 s]\n",
      "Epoch: 57 - 00360/00647 - Loss: 0.02913. [ 83 s]\n",
      "Epoch: 57 - 00380/00647 - Loss: 0.02105. [ 87 s]\n",
      "Epoch: 57 - 00400/00647 - Loss: 0.01979. [ 92 s]\n",
      "Epoch: 57 - 00420/00647 - Loss: 0.01675. [ 97 s]\n",
      "Epoch: 57 - 00440/00647 - Loss: 0.02310. [101 s]\n",
      "Epoch: 57 - 00460/00647 - Loss: 0.01815. [106 s]\n",
      "Epoch: 57 - 00480/00647 - Loss: 0.02037. [111 s]\n",
      "Epoch: 57 - 00500/00647 - Loss: 0.01252. [115 s]\n",
      "Epoch: 57 - 00520/00647 - Loss: 0.02089. [120 s]\n",
      "Epoch: 57 - 00540/00647 - Loss: 0.03668. [125 s]\n",
      "Epoch: 57 - 00560/00647 - Loss: 0.04282. [129 s]\n",
      "Epoch: 57 - 00580/00647 - Loss: 0.02372. [134 s]\n",
      "Epoch: 57 - 00600/00647 - Loss: 0.01185. [138 s]\n",
      "Epoch: 57 - 00620/00647 - Loss: 0.01216. [143 s]\n",
      "Epoch: 57 - 00640/00647 - Loss: 0.02224. [148 s]\n",
      "Epoch: 57 - loss(trn/val):0.02076/0.16452, acc(val):95.74%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 58 - 00020/00647 - Loss: 0.02747. [  4 s]\n",
      "Epoch: 58 - 00040/00647 - Loss: 0.01496. [  9 s]\n",
      "Epoch: 58 - 00060/00647 - Loss: 0.01186. [ 14 s]\n",
      "Epoch: 58 - 00080/00647 - Loss: 0.02742. [ 18 s]\n",
      "Epoch: 58 - 00100/00647 - Loss: 0.02461. [ 23 s]\n",
      "Epoch: 58 - 00120/00647 - Loss: 0.02180. [ 27 s]\n",
      "Epoch: 58 - 00140/00647 - Loss: 0.02688. [ 32 s]\n",
      "Epoch: 58 - 00160/00647 - Loss: 0.02872. [ 37 s]\n",
      "Epoch: 58 - 00180/00647 - Loss: 0.01701. [ 41 s]\n",
      "Epoch: 58 - 00200/00647 - Loss: 0.01860. [ 46 s]\n",
      "Epoch: 58 - 00220/00647 - Loss: 0.02422. [ 50 s]\n",
      "Epoch: 58 - 00240/00647 - Loss: 0.01607. [ 55 s]\n",
      "Epoch: 58 - 00260/00647 - Loss: 0.01416. [ 60 s]\n",
      "Epoch: 58 - 00280/00647 - Loss: 0.02484. [ 64 s]\n",
      "Epoch: 58 - 00300/00647 - Loss: 0.01835. [ 69 s]\n",
      "Epoch: 58 - 00320/00647 - Loss: 0.02348. [ 74 s]\n",
      "Epoch: 58 - 00340/00647 - Loss: 0.02199. [ 78 s]\n",
      "Epoch: 58 - 00360/00647 - Loss: 0.02005. [ 83 s]\n",
      "Epoch: 58 - 00380/00647 - Loss: 0.01327. [ 88 s]\n",
      "Epoch: 58 - 00400/00647 - Loss: 0.03063. [ 92 s]\n",
      "Epoch: 58 - 00420/00647 - Loss: 0.02026. [ 97 s]\n",
      "Epoch: 58 - 00440/00647 - Loss: 0.01390. [101 s]\n",
      "Epoch: 58 - 00460/00647 - Loss: 0.01521. [106 s]\n",
      "Epoch: 58 - 00480/00647 - Loss: 0.01444. [111 s]\n",
      "Epoch: 58 - 00500/00647 - Loss: 0.01708. [115 s]\n",
      "Epoch: 58 - 00520/00647 - Loss: 0.01188. [120 s]\n",
      "Epoch: 58 - 00540/00647 - Loss: 0.01445. [125 s]\n",
      "Epoch: 58 - 00560/00647 - Loss: 0.01797. [129 s]\n",
      "Epoch: 58 - 00580/00647 - Loss: 0.02793. [134 s]\n",
      "Epoch: 58 - 00600/00647 - Loss: 0.01789. [139 s]\n",
      "Epoch: 58 - 00620/00647 - Loss: 0.03059. [143 s]\n",
      "Epoch: 58 - 00640/00647 - Loss: 0.02125. [148 s]\n",
      "Epoch: 58 - loss(trn/val):0.01886/0.20823, acc(val):94.67%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 59 - 00020/00647 - Loss: 0.01717. [  4 s]\n",
      "Epoch: 59 - 00040/00647 - Loss: 0.06384. [  9 s]\n",
      "Epoch: 59 - 00060/00647 - Loss: 0.02098. [ 14 s]\n",
      "Epoch: 59 - 00080/00647 - Loss: 0.01955. [ 18 s]\n",
      "Epoch: 59 - 00100/00647 - Loss: 0.02183. [ 23 s]\n",
      "Epoch: 59 - 00120/00647 - Loss: 0.00881. [ 27 s]\n",
      "Epoch: 59 - 00140/00647 - Loss: 0.01371. [ 32 s]\n",
      "Epoch: 59 - 00160/00647 - Loss: 0.01894. [ 37 s]\n",
      "Epoch: 59 - 00180/00647 - Loss: 0.01708. [ 41 s]\n",
      "Epoch: 59 - 00200/00647 - Loss: 0.01519. [ 46 s]\n",
      "Epoch: 59 - 00220/00647 - Loss: 0.01938. [ 50 s]\n",
      "Epoch: 59 - 00240/00647 - Loss: 0.02894. [ 55 s]\n",
      "Epoch: 59 - 00260/00647 - Loss: 0.01364. [ 60 s]\n",
      "Epoch: 59 - 00280/00647 - Loss: 0.02237. [ 64 s]\n",
      "Epoch: 59 - 00300/00647 - Loss: 0.01100. [ 69 s]\n",
      "Epoch: 59 - 00320/00647 - Loss: 0.01723. [ 74 s]\n",
      "Epoch: 59 - 00340/00647 - Loss: 0.01203. [ 78 s]\n",
      "Epoch: 59 - 00360/00647 - Loss: 0.01152. [ 83 s]\n",
      "Epoch: 59 - 00380/00647 - Loss: 0.01387. [ 87 s]\n",
      "Epoch: 59 - 00400/00647 - Loss: 0.02850. [ 92 s]\n",
      "Epoch: 59 - 00420/00647 - Loss: 0.01997. [ 97 s]\n",
      "Epoch: 59 - 00440/00647 - Loss: 0.01706. [101 s]\n",
      "Epoch: 59 - 00460/00647 - Loss: 0.03261. [106 s]\n",
      "Epoch: 59 - 00480/00647 - Loss: 0.01529. [111 s]\n",
      "Epoch: 59 - 00500/00647 - Loss: 0.01717. [115 s]\n",
      "Epoch: 59 - 00520/00647 - Loss: 0.02681. [120 s]\n",
      "Epoch: 59 - 00540/00647 - Loss: 0.01989. [124 s]\n",
      "Epoch: 59 - 00560/00647 - Loss: 0.01330. [129 s]\n",
      "Epoch: 59 - 00580/00647 - Loss: 0.02112. [134 s]\n",
      "Epoch: 59 - 00600/00647 - Loss: 0.01365. [138 s]\n",
      "Epoch: 59 - 00620/00647 - Loss: 0.02486. [143 s]\n",
      "Epoch: 59 - 00640/00647 - Loss: 0.01466. [148 s]\n",
      "Epoch: 59 - loss(trn/val):0.01684/0.21936, acc(val):94.87%, lr=0.00010. [149s] @17 samples/s \n",
      "Epoch: 60 - 00020/00647 - Loss: 0.01573. [  5 s]\n",
      "Epoch: 60 - 00040/00647 - Loss: 0.02976. [  9 s]\n",
      "Epoch: 60 - 00060/00647 - Loss: 0.01800. [ 14 s]\n",
      "Epoch: 60 - 00080/00647 - Loss: 0.03103. [ 18 s]\n",
      "Epoch: 60 - 00100/00647 - Loss: 0.00945. [ 23 s]\n",
      "Epoch: 60 - 00120/00647 - Loss: 0.01522. [ 27 s]\n",
      "Epoch: 60 - 00140/00647 - Loss: 0.02408. [ 32 s]\n",
      "Epoch: 60 - 00160/00647 - Loss: 0.01160. [ 37 s]\n",
      "Epoch: 60 - 00180/00647 - Loss: 0.02980. [ 41 s]\n",
      "Epoch: 60 - 00200/00647 - Loss: 0.01448. [ 46 s]\n",
      "Epoch: 60 - 00220/00647 - Loss: 0.01727. [ 50 s]\n",
      "Epoch: 60 - 00240/00647 - Loss: 0.01643. [ 55 s]\n",
      "Epoch: 60 - 00260/00647 - Loss: 0.01174. [ 60 s]\n",
      "Epoch: 60 - 00280/00647 - Loss: 0.02983. [ 64 s]\n",
      "Epoch: 60 - 00300/00647 - Loss: 0.01921. [ 69 s]\n",
      "Epoch: 60 - 00320/00647 - Loss: 0.01401. [ 74 s]\n",
      "Epoch: 60 - 00340/00647 - Loss: 0.01673. [ 78 s]\n",
      "Epoch: 60 - 00360/00647 - Loss: 0.03207. [ 83 s]\n",
      "Epoch: 60 - 00380/00647 - Loss: 0.01477. [ 88 s]\n",
      "Epoch: 60 - 00400/00647 - Loss: 0.02822. [ 92 s]\n",
      "Epoch: 60 - 00420/00647 - Loss: 0.01233. [ 97 s]\n",
      "Epoch: 60 - 00440/00647 - Loss: 0.01805. [101 s]\n",
      "Epoch: 60 - 00460/00647 - Loss: 0.02900. [106 s]\n",
      "Epoch: 60 - 00480/00647 - Loss: 0.03618. [111 s]\n",
      "Epoch: 60 - 00500/00647 - Loss: 0.01570. [115 s]\n",
      "Epoch: 60 - 00520/00647 - Loss: 0.03225. [120 s]\n",
      "Epoch: 60 - 00540/00647 - Loss: 0.02326. [125 s]\n",
      "Epoch: 60 - 00560/00647 - Loss: 0.02106. [129 s]\n",
      "Epoch: 60 - 00580/00647 - Loss: 0.01852. [134 s]\n",
      "Epoch: 60 - 00600/00647 - Loss: 0.03872. [139 s]\n",
      "Epoch: 60 - 00620/00647 - Loss: 0.01259. [143 s]\n",
      "Epoch: 60 - 00640/00647 - Loss: 0.01093. [148 s]\n",
      "Epoch: 60 - loss(trn/val):0.01989/0.15303, acc(val):95.85%, lr=0.00010. [150s] @17 samples/s \n",
      "Performance on validation set: \n",
      "loss=0.0955 \n",
      "iou=0.8900 \n",
      "acc=0.9625 \n",
      "sensitivity=0.9258 \n",
      "specificity=0.9757 \n",
      "precision=0.9491 \n",
      "f1=0.9333\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, data_loader_val, optimizer, model, epochs, path, logger)\n",
    "logger.save_results(path + \"_learning_log.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d9c81",
   "metadata": {},
   "source": [
    "#### Result: RMS | lr: 0.0001 | wd: 0.00001 | momentum: 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7bcb6c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum validation loss: 0.095489 at epoch 9\n",
      "Minimum training loss: 0.016843 at epoch 59\n",
      "Maximum validation IoU: 0.890310 at epoch 22\n",
      "Maximum training IoU: 0.971577 at epoch 59\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(directory +\"/ResNet34_Enc_Dec_Combined_RMS_lr_e-4_nopretrain\" + \"_learning_log.json\"))\n",
    "visualize_training(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b217e992df2403db0b9de090ee314288d6f0bee849e7ba4278c2249a611da357"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
